{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW, Optimizer, SGD\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from scratchers.vanilla_transformer.attn import Attention, MultiheadAttention\n",
    "from scratchers.vanilla_transformer.transformer import TransformerBlock, TransformerDecoder\n",
    "from scratchers.transformer_config import TransformerConfig\n",
    "from scratchers.transformer.attn import Attention as CachedAttention\n",
    "from scratchers.transformer.transformer import TransformerDecoder as CachedTransformerDecoder\n",
    "\n",
    "from scratchers.encoder_decoder_transformer import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'scratchers' (namespace) from ['/Users/s-a-bakulin/mosquitto/ml/scratchers']>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scratchers\n",
    "importlib.reload(scratchers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_SEQ_LEN = 12\n",
    "TARGET_SEQ_LEN = 12\n",
    "\n",
    "DATA_SIZE = 1500\n",
    "TRAIN_SIZE = int(DATA_SIZE * 0.8)\n",
    "TEST_SIZE = DATA_SIZE - TRAIN_SIZE\n",
    "\n",
    "# radii = np.random.ranf((DATA_SIZE)) # * 9 + 1\n",
    "radii = np.ones((DATA_SIZE))\n",
    "starting_radian = np.random.ranf((DATA_SIZE)) * 2 * np.pi\n",
    "directions = np.random.randint(2, size=DATA_SIZE) * 2 - 1\n",
    "\n",
    "def to_cartesian(\n",
    "    radius: float,\n",
    "    start_radian: float, \n",
    "    direction: int\n",
    "):\n",
    "    delta = 2 * np.pi / (SOURCE_SEQ_LEN + TARGET_SEQ_LEN)\n",
    "    seq = np.array([\n",
    "        np.array([\n",
    "            radius * np.cos(start_radian + (i * direction * delta)),\n",
    "            radius * np.sin(start_radian + (i * direction * delta))\n",
    "        ])\n",
    "        for i in range(SOURCE_SEQ_LEN + TARGET_SEQ_LEN)\n",
    "    ])\n",
    "    source_seq = seq[:-1]\n",
    "    target_seq = seq[1:]\n",
    "    return source_seq, target_seq\n",
    "\n",
    "def make_circles_data():\n",
    "    X = np.empty((DATA_SIZE, (SOURCE_SEQ_LEN + TARGET_SEQ_LEN) - 1, 2))\n",
    "    Y = np.empty((DATA_SIZE, (SOURCE_SEQ_LEN + TARGET_SEQ_LEN) - 1, 2))\n",
    "    for idx, (radius, start_radian, direction) in enumerate(zip(radii, starting_radian, directions)):\n",
    "        x, y = to_cartesian(radius, start_radian, direction)\n",
    "        X[idx, :, :] = x\n",
    "        Y[idx, :, :] = y\n",
    "\n",
    "    return (\n",
    "        torch.from_numpy(X[:TRAIN_SIZE]).float(), \n",
    "        torch.from_numpy(Y[:TRAIN_SIZE]).float(),\n",
    "        torch.from_numpy(X[TRAIN_SIZE:]).float(), \n",
    "        torch.from_numpy(Y[TRAIN_SIZE:]).float()\n",
    "    )\n",
    "\n",
    "def make_squares_data():\n",
    "    X = np.empty((DATA_SIZE, (2 + 2) - 1, 2))\n",
    "    Y = np.empty((DATA_SIZE, (2 + 2) - 1, 2))\n",
    "\n",
    "    def map(elements):\n",
    "        res = []\n",
    "        for element in elements:\n",
    "            if element == 0:\n",
    "                res.append(np.array([-1, -1]))\n",
    "            elif element == 1:\n",
    "                res.append(np.array([1, -1]))\n",
    "            elif element == 2:\n",
    "                res.append(np.array([1, 1]))\n",
    "            elif element == 3:\n",
    "                res.append(np.array([-1, 1]))\n",
    "\n",
    "        return np.array(res)\n",
    "\n",
    "    first = np.random.randint(0, 4, size=(DATA_SIZE))\n",
    "    second = (first + 1) % 4\n",
    "    third = (second + 1) % 4\n",
    "    fourth = (third + 1) % 4\n",
    "    first = map(first)\n",
    "    second = map(second)\n",
    "    third = map(third)\n",
    "    fourth = map(fourth)\n",
    "\n",
    "    X[:, 0, :] = first\n",
    "    X[:, 1, :] = second\n",
    "    X[:, 2, :] = third\n",
    "    Y[:, 0, :] = second\n",
    "    Y[:, 1, :] = third\n",
    "    Y[:, 2, :] = fourth\n",
    "\n",
    "    return (\n",
    "        torch.from_numpy(X[:TRAIN_SIZE]).float(), \n",
    "        torch.from_numpy(Y[:TRAIN_SIZE]).float(),\n",
    "        torch.from_numpy(X[TRAIN_SIZE:]).float(), \n",
    "        torch.from_numpy(Y[TRAIN_SIZE:]).float()\n",
    "    )\n",
    "\n",
    "\n",
    "X_train, y_train, X_test, y_test = make_circles_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_circle(input, prediction, figsize=(6, 6)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    colors = cm.rainbow(np.linspace(0, 1, len(input)))\n",
    "    plt.scatter(\n",
    "        input[:, 0], input[:, 1], label=\"input\", marker=\"*\", s=250, alpha=0.5, color=colors\n",
    "    )\n",
    "    plt.scatter(prediction[:, 0], prediction[:, 1], label=\"prediction\", color=colors)\n",
    "    plt.legend()\n",
    "\n",
    "plot_circle(X_train[4], y_train[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = TransformerConfig(\n",
    "    input_size=2,\n",
    "    attn_d_k=64,\n",
    "    transformer_proj_dim=128,\n",
    "    dropout=0.2,\n",
    "    nlayers=3,\n",
    "    is_self_attn=False,\n",
    "    max_seq_len=TARGET_SEQ_LEN + SOURCE_SEQ_LEN,\n",
    "    nheads=2,\n",
    "    pre_layer_norm=True,\n",
    "    use_cache=True\n",
    ")\n",
    "\n",
    "def attn_factory(config: TransformerConfig):\n",
    "    return Attention(\n",
    "        d_k=config.attn_d_k,\n",
    "        # nheads=config.nheads,\n",
    "        is_self_attn=config.is_self_attn,\n",
    "        dropout=config.dropout\n",
    "    )\n",
    "\n",
    "def caching_attn_factory(config: TransformerConfig):\n",
    "    return CachedAttention(\n",
    "        d_k=config.attn_d_k,\n",
    "        dropout=config.dropout\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_data_loader: DataLoader,\n",
    "        test_data_loader: DataLoader,\n",
    "        optimizer: Optimizer,\n",
    "        model: nn.Module,\n",
    "        lr_scheduler = None,\n",
    "        epochs: int = 10,\n",
    "    ):\n",
    "        self.train_data_loader = train_data_loader\n",
    "        self.test_data_loader = test_data_loader\n",
    "        self.optimizer = optimizer\n",
    "        self.model = model\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def train(self, loss_fn: callable):\n",
    "        for epoch in range(self.epochs):\n",
    "            losses = []\n",
    "            self.model.train()\n",
    "            for x_batch, y_batch in self.train_data_loader:\n",
    "                out = self.model(x_batch)\n",
    "                loss = loss_fn(out, y_batch)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                losses.append(loss.detach().cpu().numpy())\n",
    "\n",
    "            if self.lr_scheduler is not None:\n",
    "                self.lr_scheduler.step()\n",
    "\n",
    "            print(f\"Train loss at epoch ({epoch}): \", np.array(losses).mean())\n",
    "\n",
    "            with torch.no_grad():\n",
    "                self.model.eval()\n",
    "                test_losses = []\n",
    "                for x_batch, y_batch in self.test_data_loader:\n",
    "                    out = self.model(x_batch)\n",
    "                    loss = loss_fn(out, y_batch)\n",
    "                    test_losses.append(loss.detach().cpu().numpy())\n",
    "\n",
    "                print(f\"Test loss at epoch ({epoch}): \", np.array(test_losses).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "EPOCHS = 1500\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "decoder = TransformerDecoder(cfg, attn_factory)\n",
    "optimizer = SGD(decoder.parameters(), lr=1e-4, momentum=0.9)\n",
    "lr_scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "trainer = Trainer(\n",
    "    train_data_loader=train_data_loader,\n",
    "    test_data_loader=test_data_loader,\n",
    "    # optimizer=AdamW(decoder.parameters(), lr=1e-5),\n",
    "    optimizer=optimizer,\n",
    "    model=decoder,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    epochs=EPOCHS\n",
    ")\n",
    "\n",
    "# trainer.train(loss_fn=nn.L1Loss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss at epoch (0):  0.72251946\n",
      "Test loss at epoch (0):  0.61641324\n",
      "Train loss at epoch (1):  0.5616314\n",
      "Test loss at epoch (1):  0.48370397\n",
      "Train loss at epoch (2):  0.451972\n",
      "Test loss at epoch (2):  0.38260365\n",
      "Train loss at epoch (3):  0.37542135\n",
      "Test loss at epoch (3):  0.31361064\n",
      "Train loss at epoch (4):  0.32571858\n",
      "Test loss at epoch (4):  0.27149117\n",
      "Train loss at epoch (5):  0.29638687\n",
      "Test loss at epoch (5):  0.24568234\n",
      "Train loss at epoch (6):  0.2776189\n",
      "Test loss at epoch (6):  0.22882755\n",
      "Train loss at epoch (7):  0.26447824\n",
      "Test loss at epoch (7):  0.21642952\n",
      "Train loss at epoch (8):  0.2542866\n",
      "Test loss at epoch (8):  0.20664339\n",
      "Train loss at epoch (9):  0.24454743\n",
      "Test loss at epoch (9):  0.19823301\n",
      "Train loss at epoch (10):  0.23535153\n",
      "Test loss at epoch (10):  0.19106942\n",
      "Train loss at epoch (11):  0.22842552\n",
      "Test loss at epoch (11):  0.18448481\n",
      "Train loss at epoch (12):  0.22228202\n",
      "Test loss at epoch (12):  0.17865519\n",
      "Train loss at epoch (13):  0.21649452\n",
      "Test loss at epoch (13):  0.17377357\n",
      "Train loss at epoch (14):  0.21023366\n",
      "Test loss at epoch (14):  0.16915974\n",
      "Train loss at epoch (15):  0.20488763\n",
      "Test loss at epoch (15):  0.16521287\n",
      "Train loss at epoch (16):  0.20067333\n",
      "Test loss at epoch (16):  0.16169634\n",
      "Train loss at epoch (17):  0.19719936\n",
      "Test loss at epoch (17):  0.15854657\n",
      "Train loss at epoch (18):  0.19322306\n",
      "Test loss at epoch (18):  0.15570898\n",
      "Train loss at epoch (19):  0.18967815\n",
      "Test loss at epoch (19):  0.1529795\n",
      "Train loss at epoch (20):  0.18768555\n",
      "Test loss at epoch (20):  0.15058242\n",
      "Train loss at epoch (21):  0.18532108\n",
      "Test loss at epoch (21):  0.14860648\n",
      "Train loss at epoch (22):  0.18157561\n",
      "Test loss at epoch (22):  0.14685021\n",
      "Train loss at epoch (23):  0.17905104\n",
      "Test loss at epoch (23):  0.14504658\n",
      "Train loss at epoch (24):  0.17668562\n",
      "Test loss at epoch (24):  0.14332657\n",
      "Train loss at epoch (25):  0.1750816\n",
      "Test loss at epoch (25):  0.14186758\n",
      "Train loss at epoch (26):  0.17306757\n",
      "Test loss at epoch (26):  0.140411\n",
      "Train loss at epoch (27):  0.17129989\n",
      "Test loss at epoch (27):  0.13913293\n",
      "Train loss at epoch (28):  0.17047565\n",
      "Test loss at epoch (28):  0.13805358\n",
      "Train loss at epoch (29):  0.16877413\n",
      "Test loss at epoch (29):  0.13689756\n",
      "Train loss at epoch (30):  0.16624048\n",
      "Test loss at epoch (30):  0.13598591\n",
      "Train loss at epoch (31):  0.16542026\n",
      "Test loss at epoch (31):  0.13517253\n",
      "Train loss at epoch (32):  0.16446361\n",
      "Test loss at epoch (32):  0.13414747\n",
      "Train loss at epoch (33):  0.16318789\n",
      "Test loss at epoch (33):  0.13325788\n",
      "Train loss at epoch (34):  0.16174957\n",
      "Test loss at epoch (34):  0.13255472\n",
      "Train loss at epoch (35):  0.16160613\n",
      "Test loss at epoch (35):  0.13172255\n",
      "Train loss at epoch (36):  0.15879458\n",
      "Test loss at epoch (36):  0.13086385\n",
      "Train loss at epoch (37):  0.15845989\n",
      "Test loss at epoch (37):  0.13024843\n",
      "Train loss at epoch (38):  0.1576427\n",
      "Test loss at epoch (38):  0.12937824\n",
      "Train loss at epoch (39):  0.15700339\n",
      "Test loss at epoch (39):  0.12896673\n",
      "Train loss at epoch (40):  0.15554032\n",
      "Test loss at epoch (40):  0.12829368\n",
      "Train loss at epoch (41):  0.15437719\n",
      "Test loss at epoch (41):  0.12747079\n",
      "Train loss at epoch (42):  0.1536733\n",
      "Test loss at epoch (42):  0.12702645\n",
      "Train loss at epoch (43):  0.15310222\n",
      "Test loss at epoch (43):  0.12620072\n",
      "Train loss at epoch (44):  0.1519005\n",
      "Test loss at epoch (44):  0.12567095\n",
      "Train loss at epoch (45):  0.15110108\n",
      "Test loss at epoch (45):  0.12508914\n",
      "Train loss at epoch (46):  0.1503424\n",
      "Test loss at epoch (46):  0.12458277\n",
      "Train loss at epoch (47):  0.14940222\n",
      "Test loss at epoch (47):  0.12388376\n",
      "Train loss at epoch (48):  0.14919242\n",
      "Test loss at epoch (48):  0.12325472\n",
      "Train loss at epoch (49):  0.1476959\n",
      "Test loss at epoch (49):  0.122744925\n",
      "Train loss at epoch (50):  0.14677508\n",
      "Test loss at epoch (50):  0.122380316\n",
      "Train loss at epoch (51):  0.14672846\n",
      "Test loss at epoch (51):  0.12178391\n",
      "Train loss at epoch (52):  0.14667454\n",
      "Test loss at epoch (52):  0.12138767\n",
      "Train loss at epoch (53):  0.14469966\n",
      "Test loss at epoch (53):  0.12074792\n",
      "Train loss at epoch (54):  0.14396234\n",
      "Test loss at epoch (54):  0.1201594\n",
      "Train loss at epoch (55):  0.14353947\n",
      "Test loss at epoch (55):  0.11953911\n",
      "Train loss at epoch (56):  0.14337766\n",
      "Test loss at epoch (56):  0.11894466\n",
      "Train loss at epoch (57):  0.1420436\n",
      "Test loss at epoch (57):  0.11861323\n",
      "Train loss at epoch (58):  0.14166047\n",
      "Test loss at epoch (58):  0.11785597\n",
      "Train loss at epoch (59):  0.14166223\n",
      "Test loss at epoch (59):  0.11725978\n",
      "Train loss at epoch (60):  0.1400486\n",
      "Test loss at epoch (60):  0.116996504\n",
      "Train loss at epoch (61):  0.13959259\n",
      "Test loss at epoch (61):  0.11640491\n",
      "Train loss at epoch (62):  0.13975476\n",
      "Test loss at epoch (62):  0.11614926\n",
      "Train loss at epoch (63):  0.13966215\n",
      "Test loss at epoch (63):  0.115565866\n",
      "Train loss at epoch (64):  0.13886435\n",
      "Test loss at epoch (64):  0.1151502\n",
      "Train loss at epoch (65):  0.13741386\n",
      "Test loss at epoch (65):  0.114379644\n",
      "Train loss at epoch (66):  0.1369503\n",
      "Test loss at epoch (66):  0.114141226\n",
      "Train loss at epoch (67):  0.13675925\n",
      "Test loss at epoch (67):  0.113593794\n",
      "Train loss at epoch (68):  0.13555567\n",
      "Test loss at epoch (68):  0.11311406\n",
      "Train loss at epoch (69):  0.1353517\n",
      "Test loss at epoch (69):  0.11258918\n",
      "Train loss at epoch (70):  0.13496612\n",
      "Test loss at epoch (70):  0.11211381\n",
      "Train loss at epoch (71):  0.13454764\n",
      "Test loss at epoch (71):  0.11166673\n",
      "Train loss at epoch (72):  0.13393027\n",
      "Test loss at epoch (72):  0.11124814\n",
      "Train loss at epoch (73):  0.13388807\n",
      "Test loss at epoch (73):  0.11074002\n",
      "Train loss at epoch (74):  0.1324057\n",
      "Test loss at epoch (74):  0.11025667\n",
      "Train loss at epoch (75):  0.1319908\n",
      "Test loss at epoch (75):  0.10979524\n",
      "Train loss at epoch (76):  0.13215633\n",
      "Test loss at epoch (76):  0.109259225\n",
      "Train loss at epoch (77):  0.13089612\n",
      "Test loss at epoch (77):  0.10893402\n",
      "Train loss at epoch (78):  0.13058531\n",
      "Test loss at epoch (78):  0.1085117\n",
      "Train loss at epoch (79):  0.129903\n",
      "Test loss at epoch (79):  0.108004235\n",
      "Train loss at epoch (80):  0.13093306\n",
      "Test loss at epoch (80):  0.107661836\n",
      "Train loss at epoch (81):  0.12931265\n",
      "Test loss at epoch (81):  0.10715167\n",
      "Train loss at epoch (82):  0.12846561\n",
      "Test loss at epoch (82):  0.1066789\n",
      "Train loss at epoch (83):  0.12839809\n",
      "Test loss at epoch (83):  0.10605713\n",
      "Train loss at epoch (84):  0.12811376\n",
      "Test loss at epoch (84):  0.105712436\n",
      "Train loss at epoch (85):  0.12794666\n",
      "Test loss at epoch (85):  0.10532806\n",
      "Train loss at epoch (86):  0.12766398\n",
      "Test loss at epoch (86):  0.10481096\n",
      "Train loss at epoch (87):  0.12618344\n",
      "Test loss at epoch (87):  0.1043183\n",
      "Train loss at epoch (88):  0.12658967\n",
      "Test loss at epoch (88):  0.103803225\n",
      "Train loss at epoch (89):  0.12565756\n",
      "Test loss at epoch (89):  0.10344226\n",
      "Train loss at epoch (90):  0.12513895\n",
      "Test loss at epoch (90):  0.102906846\n",
      "Train loss at epoch (91):  0.12503481\n",
      "Test loss at epoch (91):  0.10258268\n",
      "Train loss at epoch (92):  0.124691986\n",
      "Test loss at epoch (92):  0.10221573\n",
      "Train loss at epoch (93):  0.12372951\n",
      "Test loss at epoch (93):  0.10178153\n",
      "Train loss at epoch (94):  0.12383199\n",
      "Test loss at epoch (94):  0.101301454\n",
      "Train loss at epoch (95):  0.12306645\n",
      "Test loss at epoch (95):  0.100907676\n",
      "Train loss at epoch (96):  0.12365443\n",
      "Test loss at epoch (96):  0.100513704\n",
      "Train loss at epoch (97):  0.12305262\n",
      "Test loss at epoch (97):  0.100072004\n",
      "Train loss at epoch (98):  0.12205011\n",
      "Test loss at epoch (98):  0.09967337\n",
      "Train loss at epoch (99):  0.12137923\n",
      "Test loss at epoch (99):  0.099239886\n",
      "Train loss at epoch (100):  0.121092886\n",
      "Test loss at epoch (100):  0.09877184\n",
      "Train loss at epoch (101):  0.12084806\n",
      "Test loss at epoch (101):  0.09842044\n",
      "Train loss at epoch (102):  0.1201232\n",
      "Test loss at epoch (102):  0.098140724\n",
      "Train loss at epoch (103):  0.11987064\n",
      "Test loss at epoch (103):  0.0977806\n",
      "Train loss at epoch (104):  0.11957552\n",
      "Test loss at epoch (104):  0.097282745\n",
      "Train loss at epoch (105):  0.120060116\n",
      "Test loss at epoch (105):  0.096731864\n",
      "Train loss at epoch (106):  0.11896383\n",
      "Test loss at epoch (106):  0.09628448\n",
      "Train loss at epoch (107):  0.11880218\n",
      "Test loss at epoch (107):  0.0959363\n",
      "Train loss at epoch (108):  0.118213706\n",
      "Test loss at epoch (108):  0.09553863\n",
      "Train loss at epoch (109):  0.11787877\n",
      "Test loss at epoch (109):  0.0950215\n",
      "Train loss at epoch (110):  0.11765315\n",
      "Test loss at epoch (110):  0.09472238\n",
      "Train loss at epoch (111):  0.11795775\n",
      "Test loss at epoch (111):  0.09422417\n",
      "Train loss at epoch (112):  0.11666374\n",
      "Test loss at epoch (112):  0.09384491\n",
      "Train loss at epoch (113):  0.11690493\n",
      "Test loss at epoch (113):  0.09359213\n",
      "Train loss at epoch (114):  0.11599917\n",
      "Test loss at epoch (114):  0.09300414\n",
      "Train loss at epoch (115):  0.115496635\n",
      "Test loss at epoch (115):  0.09251192\n",
      "Train loss at epoch (116):  0.115811065\n",
      "Test loss at epoch (116):  0.09222984\n",
      "Train loss at epoch (117):  0.11558169\n",
      "Test loss at epoch (117):  0.09189004\n",
      "Train loss at epoch (118):  0.114995815\n",
      "Test loss at epoch (118):  0.091560826\n",
      "Train loss at epoch (119):  0.11423199\n",
      "Test loss at epoch (119):  0.09121074\n",
      "Train loss at epoch (120):  0.11474516\n",
      "Test loss at epoch (120):  0.09074321\n",
      "Train loss at epoch (121):  0.11403966\n",
      "Test loss at epoch (121):  0.090417966\n",
      "Train loss at epoch (122):  0.11331357\n",
      "Test loss at epoch (122):  0.08988939\n",
      "Train loss at epoch (123):  0.11321343\n",
      "Test loss at epoch (123):  0.089614026\n",
      "Train loss at epoch (124):  0.11333088\n",
      "Test loss at epoch (124):  0.089346744\n",
      "Train loss at epoch (125):  0.11269924\n",
      "Test loss at epoch (125):  0.08881155\n",
      "Train loss at epoch (126):  0.11210326\n",
      "Test loss at epoch (126):  0.088377185\n",
      "Train loss at epoch (127):  0.11167558\n",
      "Test loss at epoch (127):  0.088073485\n",
      "Train loss at epoch (128):  0.11149306\n",
      "Test loss at epoch (128):  0.087607704\n",
      "Train loss at epoch (129):  0.11103128\n",
      "Test loss at epoch (129):  0.08736151\n",
      "Train loss at epoch (130):  0.11138484\n",
      "Test loss at epoch (130):  0.08713507\n",
      "Train loss at epoch (131):  0.11081631\n",
      "Test loss at epoch (131):  0.08672589\n",
      "Train loss at epoch (132):  0.11076449\n",
      "Test loss at epoch (132):  0.0863567\n",
      "Train loss at epoch (133):  0.11029537\n",
      "Test loss at epoch (133):  0.086122\n",
      "Train loss at epoch (134):  0.11031601\n",
      "Test loss at epoch (134):  0.085752554\n",
      "Train loss at epoch (135):  0.10952301\n",
      "Test loss at epoch (135):  0.085220195\n",
      "Train loss at epoch (136):  0.10956631\n",
      "Test loss at epoch (136):  0.08498327\n",
      "Train loss at epoch (137):  0.10879443\n",
      "Test loss at epoch (137):  0.08454624\n",
      "Train loss at epoch (138):  0.10874991\n",
      "Test loss at epoch (138):  0.084273264\n",
      "Train loss at epoch (139):  0.10783043\n",
      "Test loss at epoch (139):  0.083837606\n",
      "Train loss at epoch (140):  0.10796275\n",
      "Test loss at epoch (140):  0.0835318\n",
      "Train loss at epoch (141):  0.10782152\n",
      "Test loss at epoch (141):  0.083206005\n",
      "Train loss at epoch (142):  0.10790605\n",
      "Test loss at epoch (142):  0.0829579\n",
      "Train loss at epoch (143):  0.10753248\n",
      "Test loss at epoch (143):  0.08242168\n",
      "Train loss at epoch (144):  0.107221514\n",
      "Test loss at epoch (144):  0.082145385\n",
      "Train loss at epoch (145):  0.10677652\n",
      "Test loss at epoch (145):  0.081811056\n",
      "Train loss at epoch (146):  0.10631247\n",
      "Test loss at epoch (146):  0.081729196\n",
      "Train loss at epoch (147):  0.10651256\n",
      "Test loss at epoch (147):  0.08108374\n",
      "Train loss at epoch (148):  0.106130324\n",
      "Test loss at epoch (148):  0.08070328\n",
      "Train loss at epoch (149):  0.10591725\n",
      "Test loss at epoch (149):  0.08067572\n",
      "Train loss at epoch (150):  0.10551913\n",
      "Test loss at epoch (150):  0.08008874\n",
      "Train loss at epoch (151):  0.105224304\n",
      "Test loss at epoch (151):  0.07973309\n",
      "Train loss at epoch (152):  0.10518847\n",
      "Test loss at epoch (152):  0.07934978\n",
      "Train loss at epoch (153):  0.10463745\n",
      "Test loss at epoch (153):  0.07915532\n",
      "Train loss at epoch (154):  0.10472563\n",
      "Test loss at epoch (154):  0.078841455\n",
      "Train loss at epoch (155):  0.10442658\n",
      "Test loss at epoch (155):  0.07847147\n",
      "Train loss at epoch (156):  0.104282714\n",
      "Test loss at epoch (156):  0.07823009\n",
      "Train loss at epoch (157):  0.1038222\n",
      "Test loss at epoch (157):  0.0780291\n",
      "Train loss at epoch (158):  0.10327825\n",
      "Test loss at epoch (158):  0.077564046\n",
      "Train loss at epoch (159):  0.10349857\n",
      "Test loss at epoch (159):  0.07725481\n",
      "Train loss at epoch (160):  0.10294533\n",
      "Test loss at epoch (160):  0.07710503\n",
      "Train loss at epoch (161):  0.10312766\n",
      "Test loss at epoch (161):  0.07656805\n",
      "Train loss at epoch (162):  0.10280524\n",
      "Test loss at epoch (162):  0.076337956\n",
      "Train loss at epoch (163):  0.10237115\n",
      "Test loss at epoch (163):  0.075945996\n",
      "Train loss at epoch (164):  0.10200832\n",
      "Test loss at epoch (164):  0.075819805\n",
      "Train loss at epoch (165):  0.10171572\n",
      "Test loss at epoch (165):  0.07537749\n",
      "Train loss at epoch (166):  0.101916805\n",
      "Test loss at epoch (166):  0.075119294\n",
      "Train loss at epoch (167):  0.10190362\n",
      "Test loss at epoch (167):  0.07487055\n",
      "Train loss at epoch (168):  0.10168319\n",
      "Test loss at epoch (168):  0.07450105\n",
      "Train loss at epoch (169):  0.10115225\n",
      "Test loss at epoch (169):  0.07434485\n",
      "Train loss at epoch (170):  0.101123616\n",
      "Test loss at epoch (170):  0.07397912\n",
      "Train loss at epoch (171):  0.10006876\n",
      "Test loss at epoch (171):  0.07388638\n",
      "Train loss at epoch (172):  0.10014842\n",
      "Test loss at epoch (172):  0.073514946\n",
      "Train loss at epoch (173):  0.10055862\n",
      "Test loss at epoch (173):  0.073374785\n",
      "Train loss at epoch (174):  0.099927634\n",
      "Test loss at epoch (174):  0.072984286\n",
      "Train loss at epoch (175):  0.09986616\n",
      "Test loss at epoch (175):  0.072771885\n",
      "Train loss at epoch (176):  0.09984724\n",
      "Test loss at epoch (176):  0.072446704\n",
      "Train loss at epoch (177):  0.09938059\n",
      "Test loss at epoch (177):  0.07210678\n",
      "Train loss at epoch (178):  0.098959945\n",
      "Test loss at epoch (178):  0.07182783\n",
      "Train loss at epoch (179):  0.09903794\n",
      "Test loss at epoch (179):  0.07155149\n",
      "Train loss at epoch (180):  0.09895625\n",
      "Test loss at epoch (180):  0.07133039\n",
      "Train loss at epoch (181):  0.09862954\n",
      "Test loss at epoch (181):  0.0709349\n",
      "Train loss at epoch (182):  0.09863369\n",
      "Test loss at epoch (182):  0.07096687\n",
      "Train loss at epoch (183):  0.098038785\n",
      "Test loss at epoch (183):  0.07078112\n",
      "Train loss at epoch (184):  0.09803773\n",
      "Test loss at epoch (184):  0.07043472\n",
      "Train loss at epoch (185):  0.098162636\n",
      "Test loss at epoch (185):  0.07014552\n",
      "Train loss at epoch (186):  0.09816103\n",
      "Test loss at epoch (186):  0.06992382\n",
      "Train loss at epoch (187):  0.09768565\n",
      "Test loss at epoch (187):  0.069710575\n",
      "Train loss at epoch (188):  0.097156666\n",
      "Test loss at epoch (188):  0.069653384\n",
      "Train loss at epoch (189):  0.09783258\n",
      "Test loss at epoch (189):  0.06935138\n",
      "Train loss at epoch (190):  0.09728068\n",
      "Test loss at epoch (190):  0.06895809\n",
      "Train loss at epoch (191):  0.09719032\n",
      "Test loss at epoch (191):  0.06872979\n",
      "Train loss at epoch (192):  0.09672644\n",
      "Test loss at epoch (192):  0.06858864\n",
      "Train loss at epoch (193):  0.09640363\n",
      "Test loss at epoch (193):  0.068301216\n",
      "Train loss at epoch (194):  0.0964332\n",
      "Test loss at epoch (194):  0.067988336\n",
      "Train loss at epoch (195):  0.09619498\n",
      "Test loss at epoch (195):  0.067893416\n",
      "Train loss at epoch (196):  0.096256666\n",
      "Test loss at epoch (196):  0.06766921\n",
      "Train loss at epoch (197):  0.09608061\n",
      "Test loss at epoch (197):  0.06748247\n",
      "Train loss at epoch (198):  0.09514629\n",
      "Test loss at epoch (198):  0.06726748\n",
      "Train loss at epoch (199):  0.09561271\n",
      "Test loss at epoch (199):  0.06704821\n",
      "Train loss at epoch (200):  0.095207416\n",
      "Test loss at epoch (200):  0.06681505\n",
      "Train loss at epoch (201):  0.09536205\n",
      "Test loss at epoch (201):  0.066503815\n",
      "Train loss at epoch (202):  0.09459335\n",
      "Test loss at epoch (202):  0.066452615\n",
      "Train loss at epoch (203):  0.09424068\n",
      "Test loss at epoch (203):  0.06607868\n",
      "Train loss at epoch (204):  0.09455314\n",
      "Test loss at epoch (204):  0.06582812\n",
      "Train loss at epoch (205):  0.094997734\n",
      "Test loss at epoch (205):  0.06567267\n",
      "Train loss at epoch (206):  0.094575174\n",
      "Test loss at epoch (206):  0.06554408\n",
      "Train loss at epoch (207):  0.09466557\n",
      "Test loss at epoch (207):  0.06528963\n",
      "Train loss at epoch (208):  0.0936376\n",
      "Test loss at epoch (208):  0.06526283\n",
      "Train loss at epoch (209):  0.094017565\n",
      "Test loss at epoch (209):  0.06486217\n",
      "Train loss at epoch (210):  0.093704864\n",
      "Test loss at epoch (210):  0.064700544\n",
      "Train loss at epoch (211):  0.09340367\n",
      "Test loss at epoch (211):  0.06443246\n",
      "Train loss at epoch (212):  0.09309457\n",
      "Test loss at epoch (212):  0.06441251\n",
      "Train loss at epoch (213):  0.09296813\n",
      "Test loss at epoch (213):  0.06429048\n",
      "Train loss at epoch (214):  0.09315974\n",
      "Test loss at epoch (214):  0.06410974\n",
      "Train loss at epoch (215):  0.09320399\n",
      "Test loss at epoch (215):  0.06384441\n",
      "Train loss at epoch (216):  0.093277\n",
      "Test loss at epoch (216):  0.06375771\n",
      "Train loss at epoch (217):  0.09318393\n",
      "Test loss at epoch (217):  0.06367302\n",
      "Train loss at epoch (218):  0.09216215\n",
      "Test loss at epoch (218):  0.06335702\n",
      "Train loss at epoch (219):  0.09273653\n",
      "Test loss at epoch (219):  0.06320375\n",
      "Train loss at epoch (220):  0.09314326\n",
      "Test loss at epoch (220):  0.06300816\n",
      "Train loss at epoch (221):  0.092106834\n",
      "Test loss at epoch (221):  0.06273717\n",
      "Train loss at epoch (222):  0.091978535\n",
      "Test loss at epoch (222):  0.062440395\n",
      "Train loss at epoch (223):  0.09182032\n",
      "Test loss at epoch (223):  0.062299024\n",
      "Train loss at epoch (224):  0.09163802\n",
      "Test loss at epoch (224):  0.062223133\n",
      "Train loss at epoch (225):  0.0916245\n",
      "Test loss at epoch (225):  0.0620883\n",
      "Train loss at epoch (226):  0.09156158\n",
      "Test loss at epoch (226):  0.061951153\n",
      "Train loss at epoch (227):  0.09152205\n",
      "Test loss at epoch (227):  0.06177044\n",
      "Train loss at epoch (228):  0.09071639\n",
      "Test loss at epoch (228):  0.061544444\n",
      "Train loss at epoch (229):  0.09104416\n",
      "Test loss at epoch (229):  0.061344236\n",
      "Train loss at epoch (230):  0.09112721\n",
      "Test loss at epoch (230):  0.06126645\n",
      "Train loss at epoch (231):  0.09073773\n",
      "Test loss at epoch (231):  0.061195586\n",
      "Train loss at epoch (232):  0.09077531\n",
      "Test loss at epoch (232):  0.060957123\n",
      "Train loss at epoch (233):  0.089980714\n",
      "Test loss at epoch (233):  0.060754105\n",
      "Train loss at epoch (234):  0.090121076\n",
      "Test loss at epoch (234):  0.060707983\n",
      "Train loss at epoch (235):  0.09019559\n",
      "Test loss at epoch (235):  0.060637325\n",
      "Train loss at epoch (236):  0.09010644\n",
      "Test loss at epoch (236):  0.060446482\n",
      "Train loss at epoch (237):  0.09016915\n",
      "Test loss at epoch (237):  0.06017495\n",
      "Train loss at epoch (238):  0.08972206\n",
      "Test loss at epoch (238):  0.060316235\n",
      "Train loss at epoch (239):  0.09024374\n",
      "Test loss at epoch (239):  0.059960175\n",
      "Train loss at epoch (240):  0.08955706\n",
      "Test loss at epoch (240):  0.06006451\n",
      "Train loss at epoch (241):  0.089751296\n",
      "Test loss at epoch (241):  0.059683003\n",
      "Train loss at epoch (242):  0.08904585\n",
      "Test loss at epoch (242):  0.05962929\n",
      "Train loss at epoch (243):  0.08954781\n",
      "Test loss at epoch (243):  0.05924607\n",
      "Train loss at epoch (244):  0.089130506\n",
      "Test loss at epoch (244):  0.059282273\n",
      "Train loss at epoch (245):  0.089145295\n",
      "Test loss at epoch (245):  0.059139717\n",
      "Train loss at epoch (246):  0.089232266\n",
      "Test loss at epoch (246):  0.05903859\n",
      "Train loss at epoch (247):  0.08883144\n",
      "Test loss at epoch (247):  0.058888122\n",
      "Train loss at epoch (248):  0.088805325\n",
      "Test loss at epoch (248):  0.058803532\n",
      "Train loss at epoch (249):  0.08892582\n",
      "Test loss at epoch (249):  0.05866621\n",
      "Train loss at epoch (250):  0.08839689\n",
      "Test loss at epoch (250):  0.0584902\n",
      "Train loss at epoch (251):  0.08862226\n",
      "Test loss at epoch (251):  0.058382027\n",
      "Train loss at epoch (252):  0.08845341\n",
      "Test loss at epoch (252):  0.058438133\n",
      "Train loss at epoch (253):  0.0882028\n",
      "Test loss at epoch (253):  0.05792265\n",
      "Train loss at epoch (254):  0.08826362\n",
      "Test loss at epoch (254):  0.057981335\n",
      "Train loss at epoch (255):  0.088455915\n",
      "Test loss at epoch (255):  0.057981383\n",
      "Train loss at epoch (256):  0.08824682\n",
      "Test loss at epoch (256):  0.057787683\n",
      "Train loss at epoch (257):  0.087843545\n",
      "Test loss at epoch (257):  0.05771403\n",
      "Train loss at epoch (258):  0.08765903\n",
      "Test loss at epoch (258):  0.057341926\n",
      "Train loss at epoch (259):  0.08781824\n",
      "Test loss at epoch (259):  0.05729009\n",
      "Train loss at epoch (260):  0.08756859\n",
      "Test loss at epoch (260):  0.057173636\n",
      "Train loss at epoch (261):  0.08755305\n",
      "Test loss at epoch (261):  0.05706986\n",
      "Train loss at epoch (262):  0.08724797\n",
      "Test loss at epoch (262):  0.056914035\n",
      "Train loss at epoch (263):  0.08730929\n",
      "Test loss at epoch (263):  0.05686021\n",
      "Train loss at epoch (264):  0.08711021\n",
      "Test loss at epoch (264):  0.05681388\n",
      "Train loss at epoch (265):  0.086373344\n",
      "Test loss at epoch (265):  0.056748454\n",
      "Train loss at epoch (266):  0.087065816\n",
      "Test loss at epoch (266):  0.05666035\n",
      "Train loss at epoch (267):  0.08698229\n",
      "Test loss at epoch (267):  0.056658387\n",
      "Train loss at epoch (268):  0.08654675\n",
      "Test loss at epoch (268):  0.05640636\n",
      "Train loss at epoch (269):  0.08660972\n",
      "Test loss at epoch (269):  0.056258887\n",
      "Train loss at epoch (270):  0.086549096\n",
      "Test loss at epoch (270):  0.055972245\n",
      "Train loss at epoch (271):  0.086401485\n",
      "Test loss at epoch (271):  0.055876963\n",
      "Train loss at epoch (272):  0.086271524\n",
      "Test loss at epoch (272):  0.055899784\n",
      "Train loss at epoch (273):  0.085792564\n",
      "Test loss at epoch (273):  0.055680707\n",
      "Train loss at epoch (274):  0.08605399\n",
      "Test loss at epoch (274):  0.055577785\n",
      "Train loss at epoch (275):  0.086175084\n",
      "Test loss at epoch (275):  0.055473674\n",
      "Train loss at epoch (276):  0.08551346\n",
      "Test loss at epoch (276):  0.0554366\n",
      "Train loss at epoch (277):  0.0857806\n",
      "Test loss at epoch (277):  0.055355676\n",
      "Train loss at epoch (278):  0.08546907\n",
      "Test loss at epoch (278):  0.055191737\n",
      "Train loss at epoch (279):  0.08571977\n",
      "Test loss at epoch (279):  0.055125266\n",
      "Train loss at epoch (280):  0.08541408\n",
      "Test loss at epoch (280):  0.055009354\n",
      "Train loss at epoch (281):  0.08542727\n",
      "Test loss at epoch (281):  0.05486524\n",
      "Train loss at epoch (282):  0.08529344\n",
      "Test loss at epoch (282):  0.054861564\n",
      "Train loss at epoch (283):  0.085310146\n",
      "Test loss at epoch (283):  0.05471896\n",
      "Train loss at epoch (284):  0.08542995\n",
      "Test loss at epoch (284):  0.054539412\n",
      "Train loss at epoch (285):  0.08495042\n",
      "Test loss at epoch (285):  0.054491784\n",
      "Train loss at epoch (286):  0.08509263\n",
      "Test loss at epoch (286):  0.054484468\n",
      "Train loss at epoch (287):  0.08518863\n",
      "Test loss at epoch (287):  0.054330852\n",
      "Train loss at epoch (288):  0.084851556\n",
      "Test loss at epoch (288):  0.05434638\n",
      "Train loss at epoch (289):  0.0843792\n",
      "Test loss at epoch (289):  0.053953096\n",
      "Train loss at epoch (290):  0.08453476\n",
      "Test loss at epoch (290):  0.054025248\n",
      "Train loss at epoch (291):  0.08425346\n",
      "Test loss at epoch (291):  0.0538778\n",
      "Train loss at epoch (292):  0.08438117\n",
      "Test loss at epoch (292):  0.053842325\n",
      "Train loss at epoch (293):  0.08404634\n",
      "Test loss at epoch (293):  0.053653736\n",
      "Train loss at epoch (294):  0.08442612\n",
      "Test loss at epoch (294):  0.053578466\n",
      "Train loss at epoch (295):  0.08372864\n",
      "Test loss at epoch (295):  0.053642932\n",
      "Train loss at epoch (296):  0.08403789\n",
      "Test loss at epoch (296):  0.05345453\n",
      "Train loss at epoch (297):  0.08369691\n",
      "Test loss at epoch (297):  0.053429633\n",
      "Train loss at epoch (298):  0.08348756\n",
      "Test loss at epoch (298):  0.05334271\n",
      "Train loss at epoch (299):  0.083821885\n",
      "Test loss at epoch (299):  0.05315534\n",
      "Train loss at epoch (300):  0.08386774\n",
      "Test loss at epoch (300):  0.05326061\n",
      "Train loss at epoch (301):  0.08351764\n",
      "Test loss at epoch (301):  0.053124428\n",
      "Train loss at epoch (302):  0.08349787\n",
      "Test loss at epoch (302):  0.05303976\n",
      "Train loss at epoch (303):  0.083541855\n",
      "Test loss at epoch (303):  0.052935116\n",
      "Train loss at epoch (304):  0.08375193\n",
      "Test loss at epoch (304):  0.052941706\n",
      "Train loss at epoch (305):  0.08389956\n",
      "Test loss at epoch (305):  0.052630823\n",
      "Train loss at epoch (306):  0.083576225\n",
      "Test loss at epoch (306):  0.052546665\n",
      "Train loss at epoch (307):  0.082987666\n",
      "Test loss at epoch (307):  0.052620813\n",
      "Train loss at epoch (308):  0.08332435\n",
      "Test loss at epoch (308):  0.052558213\n",
      "Train loss at epoch (309):  0.08339158\n",
      "Test loss at epoch (309):  0.052565537\n",
      "Train loss at epoch (310):  0.082469285\n",
      "Test loss at epoch (310):  0.052298605\n",
      "Train loss at epoch (311):  0.082992144\n",
      "Test loss at epoch (311):  0.052314464\n",
      "Train loss at epoch (312):  0.0829352\n",
      "Test loss at epoch (312):  0.052191768\n",
      "Train loss at epoch (313):  0.08268229\n",
      "Test loss at epoch (313):  0.05203975\n",
      "Train loss at epoch (314):  0.08297481\n",
      "Test loss at epoch (314):  0.051971294\n",
      "Train loss at epoch (315):  0.08250121\n",
      "Test loss at epoch (315):  0.05182028\n",
      "Train loss at epoch (316):  0.08275932\n",
      "Test loss at epoch (316):  0.051866103\n",
      "Train loss at epoch (317):  0.08252629\n",
      "Test loss at epoch (317):  0.051797323\n",
      "Train loss at epoch (318):  0.082099736\n",
      "Test loss at epoch (318):  0.051800847\n",
      "Train loss at epoch (319):  0.082197726\n",
      "Test loss at epoch (319):  0.051754955\n",
      "Train loss at epoch (320):  0.082520306\n",
      "Test loss at epoch (320):  0.0515995\n",
      "Train loss at epoch (321):  0.08207312\n",
      "Test loss at epoch (321):  0.051636163\n",
      "Train loss at epoch (322):  0.08204146\n",
      "Test loss at epoch (322):  0.05148306\n",
      "Train loss at epoch (323):  0.08205534\n",
      "Test loss at epoch (323):  0.051294297\n",
      "Train loss at epoch (324):  0.08188602\n",
      "Test loss at epoch (324):  0.05128947\n",
      "Train loss at epoch (325):  0.08193611\n",
      "Test loss at epoch (325):  0.05118676\n",
      "Train loss at epoch (326):  0.08196515\n",
      "Test loss at epoch (326):  0.051179662\n",
      "Train loss at epoch (327):  0.081486195\n",
      "Test loss at epoch (327):  0.051086575\n",
      "Train loss at epoch (328):  0.081349246\n",
      "Test loss at epoch (328):  0.051099587\n",
      "Train loss at epoch (329):  0.081011884\n",
      "Test loss at epoch (329):  0.05085095\n",
      "Train loss at epoch (330):  0.08157324\n",
      "Test loss at epoch (330):  0.050820578\n",
      "Train loss at epoch (331):  0.08165623\n",
      "Test loss at epoch (331):  0.050806977\n",
      "Train loss at epoch (332):  0.08192225\n",
      "Test loss at epoch (332):  0.050792888\n",
      "Train loss at epoch (333):  0.081299804\n",
      "Test loss at epoch (333):  0.050512232\n",
      "Train loss at epoch (334):  0.08150565\n",
      "Test loss at epoch (334):  0.050591175\n",
      "Train loss at epoch (335):  0.08125138\n",
      "Test loss at epoch (335):  0.0505237\n",
      "Train loss at epoch (336):  0.080670886\n",
      "Test loss at epoch (336):  0.050437234\n",
      "Train loss at epoch (337):  0.08080658\n",
      "Test loss at epoch (337):  0.050553758\n",
      "Train loss at epoch (338):  0.08087466\n",
      "Test loss at epoch (338):  0.050235458\n",
      "Train loss at epoch (339):  0.08079472\n",
      "Test loss at epoch (339):  0.05022005\n",
      "Train loss at epoch (340):  0.08068708\n",
      "Test loss at epoch (340):  0.050192326\n",
      "Train loss at epoch (341):  0.08097665\n",
      "Test loss at epoch (341):  0.050088614\n",
      "Train loss at epoch (342):  0.08069451\n",
      "Test loss at epoch (342):  0.049966644\n",
      "Train loss at epoch (343):  0.08070137\n",
      "Test loss at epoch (343):  0.049961913\n",
      "Train loss at epoch (344):  0.08007011\n",
      "Test loss at epoch (344):  0.049944147\n",
      "Train loss at epoch (345):  0.08050084\n",
      "Test loss at epoch (345):  0.04993236\n",
      "Train loss at epoch (346):  0.08008585\n",
      "Test loss at epoch (346):  0.04967238\n",
      "Train loss at epoch (347):  0.08037855\n",
      "Test loss at epoch (347):  0.049550187\n",
      "Train loss at epoch (348):  0.080149636\n",
      "Test loss at epoch (348):  0.049680386\n",
      "Train loss at epoch (349):  0.079965025\n",
      "Test loss at epoch (349):  0.049569476\n",
      "Train loss at epoch (350):  0.08029878\n",
      "Test loss at epoch (350):  0.049450163\n",
      "Train loss at epoch (351):  0.079718426\n",
      "Test loss at epoch (351):  0.049465504\n",
      "Train loss at epoch (352):  0.080079794\n",
      "Test loss at epoch (352):  0.04925418\n",
      "Train loss at epoch (353):  0.08002214\n",
      "Test loss at epoch (353):  0.04906125\n",
      "Train loss at epoch (354):  0.0799354\n",
      "Test loss at epoch (354):  0.04920384\n",
      "Train loss at epoch (355):  0.07979277\n",
      "Test loss at epoch (355):  0.049059927\n",
      "Train loss at epoch (356):  0.079718225\n",
      "Test loss at epoch (356):  0.049098894\n",
      "Train loss at epoch (357):  0.07963843\n",
      "Test loss at epoch (357):  0.04940186\n",
      "Train loss at epoch (358):  0.07947243\n",
      "Test loss at epoch (358):  0.048993897\n",
      "Train loss at epoch (359):  0.079812504\n",
      "Test loss at epoch (359):  0.04887206\n",
      "Train loss at epoch (360):  0.07939458\n",
      "Test loss at epoch (360):  0.048916284\n",
      "Train loss at epoch (361):  0.07961169\n",
      "Test loss at epoch (361):  0.048847508\n",
      "Train loss at epoch (362):  0.079328865\n",
      "Test loss at epoch (362):  0.04864867\n",
      "Train loss at epoch (363):  0.07938932\n",
      "Test loss at epoch (363):  0.048596628\n",
      "Train loss at epoch (364):  0.07913782\n",
      "Test loss at epoch (364):  0.048551567\n",
      "Train loss at epoch (365):  0.07960474\n",
      "Test loss at epoch (365):  0.048563965\n",
      "Train loss at epoch (366):  0.079280384\n",
      "Test loss at epoch (366):  0.048539724\n",
      "Train loss at epoch (367):  0.07880887\n",
      "Test loss at epoch (367):  0.048588146\n",
      "Train loss at epoch (368):  0.07890579\n",
      "Test loss at epoch (368):  0.048448626\n",
      "Train loss at epoch (369):  0.07895238\n",
      "Test loss at epoch (369):  0.048320085\n",
      "Train loss at epoch (370):  0.078656346\n",
      "Test loss at epoch (370):  0.048312206\n",
      "Train loss at epoch (371):  0.07922508\n",
      "Test loss at epoch (371):  0.048283875\n",
      "Train loss at epoch (372):  0.07896449\n",
      "Test loss at epoch (372):  0.048188135\n",
      "Train loss at epoch (373):  0.07866939\n",
      "Test loss at epoch (373):  0.048257984\n",
      "Train loss at epoch (374):  0.07889768\n",
      "Test loss at epoch (374):  0.048081152\n",
      "Train loss at epoch (375):  0.07826282\n",
      "Test loss at epoch (375):  0.047974218\n",
      "Train loss at epoch (376):  0.07872275\n",
      "Test loss at epoch (376):  0.047999825\n",
      "Train loss at epoch (377):  0.07811781\n",
      "Test loss at epoch (377):  0.047865346\n",
      "Train loss at epoch (378):  0.0782836\n",
      "Test loss at epoch (378):  0.047760412\n",
      "Train loss at epoch (379):  0.07827042\n",
      "Test loss at epoch (379):  0.0478519\n",
      "Train loss at epoch (380):  0.078478046\n",
      "Test loss at epoch (380):  0.047655992\n",
      "Train loss at epoch (381):  0.078464925\n",
      "Test loss at epoch (381):  0.04775916\n",
      "Train loss at epoch (382):  0.078741334\n",
      "Test loss at epoch (382):  0.047687244\n",
      "Train loss at epoch (383):  0.07827155\n",
      "Test loss at epoch (383):  0.047795437\n",
      "Train loss at epoch (384):  0.07822559\n",
      "Test loss at epoch (384):  0.047638446\n",
      "Train loss at epoch (385):  0.07790801\n",
      "Test loss at epoch (385):  0.04748205\n",
      "Train loss at epoch (386):  0.07774621\n",
      "Test loss at epoch (386):  0.047394034\n",
      "Train loss at epoch (387):  0.0779451\n",
      "Test loss at epoch (387):  0.0473337\n",
      "Train loss at epoch (388):  0.07817212\n",
      "Test loss at epoch (388):  0.047451995\n",
      "Train loss at epoch (389):  0.077810265\n",
      "Test loss at epoch (389):  0.047323767\n",
      "Train loss at epoch (390):  0.0774269\n",
      "Test loss at epoch (390):  0.047208246\n",
      "Train loss at epoch (391):  0.07765004\n",
      "Test loss at epoch (391):  0.047081538\n",
      "Train loss at epoch (392):  0.07764454\n",
      "Test loss at epoch (392):  0.04707027\n",
      "Train loss at epoch (393):  0.07730785\n",
      "Test loss at epoch (393):  0.046899788\n",
      "Train loss at epoch (394):  0.07760873\n",
      "Test loss at epoch (394):  0.046911847\n",
      "Train loss at epoch (395):  0.07742819\n",
      "Test loss at epoch (395):  0.04690968\n",
      "Train loss at epoch (396):  0.076818965\n",
      "Test loss at epoch (396):  0.046821956\n",
      "Train loss at epoch (397):  0.07746449\n",
      "Test loss at epoch (397):  0.04698665\n",
      "Train loss at epoch (398):  0.07706093\n",
      "Test loss at epoch (398):  0.046768915\n",
      "Train loss at epoch (399):  0.07734721\n",
      "Test loss at epoch (399):  0.046728782\n",
      "Train loss at epoch (400):  0.077148676\n",
      "Test loss at epoch (400):  0.046744768\n",
      "Train loss at epoch (401):  0.07715009\n",
      "Test loss at epoch (401):  0.0466779\n",
      "Train loss at epoch (402):  0.07731245\n",
      "Test loss at epoch (402):  0.04654305\n",
      "Train loss at epoch (403):  0.077269085\n",
      "Test loss at epoch (403):  0.04664266\n",
      "Train loss at epoch (404):  0.0771593\n",
      "Test loss at epoch (404):  0.04661573\n",
      "Train loss at epoch (405):  0.077151544\n",
      "Test loss at epoch (405):  0.04640554\n",
      "Train loss at epoch (406):  0.0764368\n",
      "Test loss at epoch (406):  0.046376187\n",
      "Train loss at epoch (407):  0.0768025\n",
      "Test loss at epoch (407):  0.046362754\n",
      "Train loss at epoch (408):  0.0770244\n",
      "Test loss at epoch (408):  0.046312574\n",
      "Train loss at epoch (409):  0.07675189\n",
      "Test loss at epoch (409):  0.04636889\n",
      "Train loss at epoch (410):  0.076663494\n",
      "Test loss at epoch (410):  0.046361864\n",
      "Train loss at epoch (411):  0.076542445\n",
      "Test loss at epoch (411):  0.04608946\n",
      "Train loss at epoch (412):  0.076666564\n",
      "Test loss at epoch (412):  0.046026725\n",
      "Train loss at epoch (413):  0.076531254\n",
      "Test loss at epoch (413):  0.046156853\n",
      "Train loss at epoch (414):  0.07680795\n",
      "Test loss at epoch (414):  0.046052836\n",
      "Train loss at epoch (415):  0.07682211\n",
      "Test loss at epoch (415):  0.046035994\n",
      "Train loss at epoch (416):  0.076337256\n",
      "Test loss at epoch (416):  0.046139102\n",
      "Train loss at epoch (417):  0.07669848\n",
      "Test loss at epoch (417):  0.04597048\n",
      "Train loss at epoch (418):  0.07634921\n",
      "Test loss at epoch (418):  0.046062853\n",
      "Train loss at epoch (419):  0.07633802\n",
      "Test loss at epoch (419):  0.046155576\n",
      "Train loss at epoch (420):  0.07631219\n",
      "Test loss at epoch (420):  0.045881495\n",
      "Train loss at epoch (421):  0.076190285\n",
      "Test loss at epoch (421):  0.045782294\n",
      "Train loss at epoch (422):  0.076419726\n",
      "Test loss at epoch (422):  0.04587487\n",
      "Train loss at epoch (423):  0.07642128\n",
      "Test loss at epoch (423):  0.04577417\n",
      "Train loss at epoch (424):  0.07651977\n",
      "Test loss at epoch (424):  0.045577385\n",
      "Train loss at epoch (425):  0.07659506\n",
      "Test loss at epoch (425):  0.045579087\n",
      "Train loss at epoch (426):  0.075789355\n",
      "Test loss at epoch (426):  0.045584425\n",
      "Train loss at epoch (427):  0.07583213\n",
      "Test loss at epoch (427):  0.045671526\n",
      "Train loss at epoch (428):  0.075628586\n",
      "Test loss at epoch (428):  0.045481343\n",
      "Train loss at epoch (429):  0.07566688\n",
      "Test loss at epoch (429):  0.045411643\n",
      "Train loss at epoch (430):  0.07636591\n",
      "Test loss at epoch (430):  0.04545444\n",
      "Train loss at epoch (431):  0.0758341\n",
      "Test loss at epoch (431):  0.045357443\n",
      "Train loss at epoch (432):  0.07569852\n",
      "Test loss at epoch (432):  0.045202743\n",
      "Train loss at epoch (433):  0.07552622\n",
      "Test loss at epoch (433):  0.04510377\n",
      "Train loss at epoch (434):  0.07606392\n",
      "Test loss at epoch (434):  0.045122717\n",
      "Train loss at epoch (435):  0.075727515\n",
      "Test loss at epoch (435):  0.04515514\n",
      "Train loss at epoch (436):  0.07565883\n",
      "Test loss at epoch (436):  0.045207668\n",
      "Train loss at epoch (437):  0.075846046\n",
      "Test loss at epoch (437):  0.045245707\n",
      "Train loss at epoch (438):  0.07556298\n",
      "Test loss at epoch (438):  0.04504068\n",
      "Train loss at epoch (439):  0.07563875\n",
      "Test loss at epoch (439):  0.045303605\n",
      "Train loss at epoch (440):  0.07597619\n",
      "Test loss at epoch (440):  0.045044288\n",
      "Train loss at epoch (441):  0.07534716\n",
      "Test loss at epoch (441):  0.044989083\n",
      "Train loss at epoch (442):  0.075219534\n",
      "Test loss at epoch (442):  0.044908237\n",
      "Train loss at epoch (443):  0.075421244\n",
      "Test loss at epoch (443):  0.044878434\n",
      "Train loss at epoch (444):  0.07524757\n",
      "Test loss at epoch (444):  0.044924114\n",
      "Train loss at epoch (445):  0.075209886\n",
      "Test loss at epoch (445):  0.044816643\n",
      "Train loss at epoch (446):  0.07533909\n",
      "Test loss at epoch (446):  0.04474867\n",
      "Train loss at epoch (447):  0.075120084\n",
      "Test loss at epoch (447):  0.044665236\n",
      "Train loss at epoch (448):  0.07459595\n",
      "Test loss at epoch (448):  0.044655047\n",
      "Train loss at epoch (449):  0.0748619\n",
      "Test loss at epoch (449):  0.044695262\n",
      "Train loss at epoch (450):  0.07516567\n",
      "Test loss at epoch (450):  0.04465121\n",
      "Train loss at epoch (451):  0.07456254\n",
      "Test loss at epoch (451):  0.044656564\n",
      "Train loss at epoch (452):  0.0747002\n",
      "Test loss at epoch (452):  0.044576064\n",
      "Train loss at epoch (453):  0.07484997\n",
      "Test loss at epoch (453):  0.04447521\n",
      "Train loss at epoch (454):  0.07474284\n",
      "Test loss at epoch (454):  0.044447552\n",
      "Train loss at epoch (455):  0.07454574\n",
      "Test loss at epoch (455):  0.04438327\n",
      "Train loss at epoch (456):  0.07433665\n",
      "Test loss at epoch (456):  0.0443467\n",
      "Train loss at epoch (457):  0.074525945\n",
      "Test loss at epoch (457):  0.044511188\n",
      "Train loss at epoch (458):  0.0749118\n",
      "Test loss at epoch (458):  0.0443294\n",
      "Train loss at epoch (459):  0.07479777\n",
      "Test loss at epoch (459):  0.04439496\n",
      "Train loss at epoch (460):  0.07481232\n",
      "Test loss at epoch (460):  0.044219643\n",
      "Train loss at epoch (461):  0.07437827\n",
      "Test loss at epoch (461):  0.044207852\n",
      "Train loss at epoch (462):  0.07465391\n",
      "Test loss at epoch (462):  0.044125486\n",
      "Train loss at epoch (463):  0.07432663\n",
      "Test loss at epoch (463):  0.044268884\n",
      "Train loss at epoch (464):  0.07441868\n",
      "Test loss at epoch (464):  0.04410379\n",
      "Train loss at epoch (465):  0.07409938\n",
      "Test loss at epoch (465):  0.044053916\n",
      "Train loss at epoch (466):  0.07407713\n",
      "Test loss at epoch (466):  0.04420863\n",
      "Train loss at epoch (467):  0.074407056\n",
      "Test loss at epoch (467):  0.044113874\n",
      "Train loss at epoch (468):  0.07402891\n",
      "Test loss at epoch (468):  0.044074584\n",
      "Train loss at epoch (469):  0.074298315\n",
      "Test loss at epoch (469):  0.04391026\n",
      "Train loss at epoch (470):  0.073874325\n",
      "Test loss at epoch (470):  0.043935332\n",
      "Train loss at epoch (471):  0.07411072\n",
      "Test loss at epoch (471):  0.0438671\n",
      "Train loss at epoch (472):  0.07381034\n",
      "Test loss at epoch (472):  0.043856274\n",
      "Train loss at epoch (473):  0.07365375\n",
      "Test loss at epoch (473):  0.04377024\n",
      "Train loss at epoch (474):  0.07471478\n",
      "Test loss at epoch (474):  0.043826073\n",
      "Train loss at epoch (475):  0.07368675\n",
      "Test loss at epoch (475):  0.043629263\n",
      "Train loss at epoch (476):  0.073741734\n",
      "Test loss at epoch (476):  0.043752436\n",
      "Train loss at epoch (477):  0.07406983\n",
      "Test loss at epoch (477):  0.043617565\n",
      "Train loss at epoch (478):  0.07340025\n",
      "Test loss at epoch (478):  0.043560777\n",
      "Train loss at epoch (479):  0.07373766\n",
      "Test loss at epoch (479):  0.043505866\n",
      "Train loss at epoch (480):  0.07362857\n",
      "Test loss at epoch (480):  0.0435813\n",
      "Train loss at epoch (481):  0.073971495\n",
      "Test loss at epoch (481):  0.04354904\n",
      "Train loss at epoch (482):  0.073574804\n",
      "Test loss at epoch (482):  0.043537296\n",
      "Train loss at epoch (483):  0.073237106\n",
      "Test loss at epoch (483):  0.043478504\n",
      "Train loss at epoch (484):  0.07387737\n",
      "Test loss at epoch (484):  0.043459326\n",
      "Train loss at epoch (485):  0.07354677\n",
      "Test loss at epoch (485):  0.043472733\n",
      "Train loss at epoch (486):  0.073246546\n",
      "Test loss at epoch (486):  0.04345974\n",
      "Train loss at epoch (487):  0.073698595\n",
      "Test loss at epoch (487):  0.04331381\n",
      "Train loss at epoch (488):  0.07370502\n",
      "Test loss at epoch (488):  0.0433088\n",
      "Train loss at epoch (489):  0.07352176\n",
      "Test loss at epoch (489):  0.0433128\n",
      "Train loss at epoch (490):  0.073696814\n",
      "Test loss at epoch (490):  0.043313976\n",
      "Train loss at epoch (491):  0.07338336\n",
      "Test loss at epoch (491):  0.043270614\n",
      "Train loss at epoch (492):  0.073470525\n",
      "Test loss at epoch (492):  0.043226745\n",
      "Train loss at epoch (493):  0.073731184\n",
      "Test loss at epoch (493):  0.043112475\n",
      "Train loss at epoch (494):  0.073140286\n",
      "Test loss at epoch (494):  0.04309967\n",
      "Train loss at epoch (495):  0.07320342\n",
      "Test loss at epoch (495):  0.043113124\n",
      "Train loss at epoch (496):  0.07299483\n",
      "Test loss at epoch (496):  0.043053653\n",
      "Train loss at epoch (497):  0.07302552\n",
      "Test loss at epoch (497):  0.04302948\n",
      "Train loss at epoch (498):  0.07300341\n",
      "Test loss at epoch (498):  0.042891618\n",
      "Train loss at epoch (499):  0.07347677\n",
      "Test loss at epoch (499):  0.0429915\n",
      "Train loss at epoch (500):  0.07277842\n",
      "Test loss at epoch (500):  0.043045275\n",
      "Train loss at epoch (501):  0.0728363\n",
      "Test loss at epoch (501):  0.042981323\n",
      "Train loss at epoch (502):  0.07336441\n",
      "Test loss at epoch (502):  0.042897172\n",
      "Train loss at epoch (503):  0.07297695\n",
      "Test loss at epoch (503):  0.04293939\n",
      "Train loss at epoch (504):  0.072948754\n",
      "Test loss at epoch (504):  0.042927295\n",
      "Train loss at epoch (505):  0.0732299\n",
      "Test loss at epoch (505):  0.04295327\n",
      "Train loss at epoch (506):  0.07287958\n",
      "Test loss at epoch (506):  0.04282069\n",
      "Train loss at epoch (507):  0.072639905\n",
      "Test loss at epoch (507):  0.04270824\n",
      "Train loss at epoch (508):  0.0727626\n",
      "Test loss at epoch (508):  0.042621586\n",
      "Train loss at epoch (509):  0.07288783\n",
      "Test loss at epoch (509):  0.042615402\n",
      "Train loss at epoch (510):  0.072798274\n",
      "Test loss at epoch (510):  0.042674895\n",
      "Train loss at epoch (511):  0.07239416\n",
      "Test loss at epoch (511):  0.042492017\n",
      "Train loss at epoch (512):  0.07272837\n",
      "Test loss at epoch (512):  0.042591523\n",
      "Train loss at epoch (513):  0.07279077\n",
      "Test loss at epoch (513):  0.04259184\n",
      "Train loss at epoch (514):  0.072964\n",
      "Test loss at epoch (514):  0.042649724\n",
      "Train loss at epoch (515):  0.072429776\n",
      "Test loss at epoch (515):  0.04257498\n",
      "Train loss at epoch (516):  0.072293386\n",
      "Test loss at epoch (516):  0.04256825\n",
      "Train loss at epoch (517):  0.07237141\n",
      "Test loss at epoch (517):  0.04247695\n",
      "Train loss at epoch (518):  0.07253689\n",
      "Test loss at epoch (518):  0.04242299\n",
      "Train loss at epoch (519):  0.072096325\n",
      "Test loss at epoch (519):  0.042405132\n",
      "Train loss at epoch (520):  0.0721124\n",
      "Test loss at epoch (520):  0.04234867\n",
      "Train loss at epoch (521):  0.072679915\n",
      "Test loss at epoch (521):  0.042396676\n",
      "Train loss at epoch (522):  0.07249213\n",
      "Test loss at epoch (522):  0.04228486\n",
      "Train loss at epoch (523):  0.07222028\n",
      "Test loss at epoch (523):  0.04225838\n",
      "Train loss at epoch (524):  0.072119996\n",
      "Test loss at epoch (524):  0.04228347\n",
      "Train loss at epoch (525):  0.07193968\n",
      "Test loss at epoch (525):  0.04236989\n",
      "Train loss at epoch (526):  0.0724429\n",
      "Test loss at epoch (526):  0.042199656\n",
      "Train loss at epoch (527):  0.071751595\n",
      "Test loss at epoch (527):  0.0421768\n",
      "Train loss at epoch (528):  0.072369024\n",
      "Test loss at epoch (528):  0.042130947\n",
      "Train loss at epoch (529):  0.07219818\n",
      "Test loss at epoch (529):  0.042133603\n",
      "Train loss at epoch (530):  0.071973346\n",
      "Test loss at epoch (530):  0.042200215\n",
      "Train loss at epoch (531):  0.07239111\n",
      "Test loss at epoch (531):  0.042159803\n",
      "Train loss at epoch (532):  0.07195864\n",
      "Test loss at epoch (532):  0.042095073\n",
      "Train loss at epoch (533):  0.07215559\n",
      "Test loss at epoch (533):  0.042091765\n",
      "Train loss at epoch (534):  0.07225383\n",
      "Test loss at epoch (534):  0.04199326\n",
      "Train loss at epoch (535):  0.07213284\n",
      "Test loss at epoch (535):  0.04207187\n",
      "Train loss at epoch (536):  0.07228459\n",
      "Test loss at epoch (536):  0.0420351\n",
      "Train loss at epoch (537):  0.071902305\n",
      "Test loss at epoch (537):  0.041920904\n",
      "Train loss at epoch (538):  0.07185582\n",
      "Test loss at epoch (538):  0.041849792\n",
      "Train loss at epoch (539):  0.071777865\n",
      "Test loss at epoch (539):  0.041870173\n",
      "Train loss at epoch (540):  0.07166692\n",
      "Test loss at epoch (540):  0.041875374\n",
      "Train loss at epoch (541):  0.071631975\n",
      "Test loss at epoch (541):  0.041889936\n",
      "Train loss at epoch (542):  0.07184886\n",
      "Test loss at epoch (542):  0.041814525\n",
      "Train loss at epoch (543):  0.07186241\n",
      "Test loss at epoch (543):  0.041822363\n",
      "Train loss at epoch (544):  0.071623795\n",
      "Test loss at epoch (544):  0.041769613\n",
      "Train loss at epoch (545):  0.07159584\n",
      "Test loss at epoch (545):  0.04181993\n",
      "Train loss at epoch (546):  0.071568444\n",
      "Test loss at epoch (546):  0.041779827\n",
      "Train loss at epoch (547):  0.07145795\n",
      "Test loss at epoch (547):  0.041772094\n",
      "Train loss at epoch (548):  0.07147312\n",
      "Test loss at epoch (548):  0.041665874\n",
      "Train loss at epoch (549):  0.07124602\n",
      "Test loss at epoch (549):  0.041657735\n",
      "Train loss at epoch (550):  0.07134803\n",
      "Test loss at epoch (550):  0.041613925\n",
      "Train loss at epoch (551):  0.07153725\n",
      "Test loss at epoch (551):  0.04161241\n",
      "Train loss at epoch (552):  0.07140983\n",
      "Test loss at epoch (552):  0.041587863\n",
      "Train loss at epoch (553):  0.07109382\n",
      "Test loss at epoch (553):  0.04160737\n",
      "Train loss at epoch (554):  0.07156231\n",
      "Test loss at epoch (554):  0.041584063\n",
      "Train loss at epoch (555):  0.071607895\n",
      "Test loss at epoch (555):  0.04144714\n",
      "Train loss at epoch (556):  0.07154312\n",
      "Test loss at epoch (556):  0.04148035\n",
      "Train loss at epoch (557):  0.071235485\n",
      "Test loss at epoch (557):  0.041514333\n",
      "Train loss at epoch (558):  0.07107102\n",
      "Test loss at epoch (558):  0.041472066\n",
      "Train loss at epoch (559):  0.07113521\n",
      "Test loss at epoch (559):  0.041529007\n",
      "Train loss at epoch (560):  0.07119075\n",
      "Test loss at epoch (560):  0.041466277\n",
      "Train loss at epoch (561):  0.071235165\n",
      "Test loss at epoch (561):  0.04153535\n",
      "Train loss at epoch (562):  0.070912115\n",
      "Test loss at epoch (562):  0.041451495\n",
      "Train loss at epoch (563):  0.07097625\n",
      "Test loss at epoch (563):  0.041475978\n",
      "Train loss at epoch (564):  0.071457244\n",
      "Test loss at epoch (564):  0.041394178\n",
      "Train loss at epoch (565):  0.070996195\n",
      "Test loss at epoch (565):  0.041322757\n",
      "Train loss at epoch (566):  0.07098551\n",
      "Test loss at epoch (566):  0.04128392\n",
      "Train loss at epoch (567):  0.07089639\n",
      "Test loss at epoch (567):  0.04129301\n",
      "Train loss at epoch (568):  0.071026616\n",
      "Test loss at epoch (568):  0.04123421\n",
      "Train loss at epoch (569):  0.07129701\n",
      "Test loss at epoch (569):  0.04116323\n",
      "Train loss at epoch (570):  0.07097855\n",
      "Test loss at epoch (570):  0.041219287\n",
      "Train loss at epoch (571):  0.070911914\n",
      "Test loss at epoch (571):  0.04120743\n",
      "Train loss at epoch (572):  0.07068909\n",
      "Test loss at epoch (572):  0.041097473\n",
      "Train loss at epoch (573):  0.07062756\n",
      "Test loss at epoch (573):  0.04115788\n",
      "Train loss at epoch (574):  0.07125975\n",
      "Test loss at epoch (574):  0.0411229\n",
      "Train loss at epoch (575):  0.07053546\n",
      "Test loss at epoch (575):  0.04104166\n",
      "Train loss at epoch (576):  0.07111331\n",
      "Test loss at epoch (576):  0.041040644\n",
      "Train loss at epoch (577):  0.07046761\n",
      "Test loss at epoch (577):  0.040996958\n",
      "Train loss at epoch (578):  0.07090799\n",
      "Test loss at epoch (578):  0.041083764\n",
      "Train loss at epoch (579):  0.070701554\n",
      "Test loss at epoch (579):  0.040958196\n",
      "Train loss at epoch (580):  0.07001029\n",
      "Test loss at epoch (580):  0.041035432\n",
      "Train loss at epoch (581):  0.07057795\n",
      "Test loss at epoch (581):  0.041067015\n",
      "Train loss at epoch (582):  0.07064631\n",
      "Test loss at epoch (582):  0.040878844\n",
      "Train loss at epoch (583):  0.070610404\n",
      "Test loss at epoch (583):  0.040922184\n",
      "Train loss at epoch (584):  0.07050124\n",
      "Test loss at epoch (584):  0.04093213\n",
      "Train loss at epoch (585):  0.07040479\n",
      "Test loss at epoch (585):  0.040848825\n",
      "Train loss at epoch (586):  0.07051533\n",
      "Test loss at epoch (586):  0.040828887\n",
      "Train loss at epoch (587):  0.07039428\n",
      "Test loss at epoch (587):  0.040875867\n",
      "Train loss at epoch (588):  0.07045313\n",
      "Test loss at epoch (588):  0.040867776\n",
      "Train loss at epoch (589):  0.07039102\n",
      "Test loss at epoch (589):  0.04066928\n",
      "Train loss at epoch (590):  0.0702778\n",
      "Test loss at epoch (590):  0.04069327\n",
      "Train loss at epoch (591):  0.07023224\n",
      "Test loss at epoch (591):  0.040787414\n",
      "Train loss at epoch (592):  0.070585616\n",
      "Test loss at epoch (592):  0.0406688\n",
      "Train loss at epoch (593):  0.06983014\n",
      "Test loss at epoch (593):  0.040581208\n",
      "Train loss at epoch (594):  0.0700432\n",
      "Test loss at epoch (594):  0.040490497\n",
      "Train loss at epoch (595):  0.07031812\n",
      "Test loss at epoch (595):  0.04057943\n",
      "Train loss at epoch (596):  0.07030623\n",
      "Test loss at epoch (596):  0.040564507\n",
      "Train loss at epoch (597):  0.070197426\n",
      "Test loss at epoch (597):  0.040581573\n",
      "Train loss at epoch (598):  0.0704132\n",
      "Test loss at epoch (598):  0.04049195\n",
      "Train loss at epoch (599):  0.07032427\n",
      "Test loss at epoch (599):  0.04055061\n",
      "Train loss at epoch (600):  0.0701231\n",
      "Test loss at epoch (600):  0.040484637\n",
      "Train loss at epoch (601):  0.07002574\n",
      "Test loss at epoch (601):  0.040421277\n",
      "Train loss at epoch (602):  0.07015671\n",
      "Test loss at epoch (602):  0.04038386\n",
      "Train loss at epoch (603):  0.070138246\n",
      "Test loss at epoch (603):  0.04044357\n",
      "Train loss at epoch (604):  0.07002178\n",
      "Test loss at epoch (604):  0.040367927\n",
      "Train loss at epoch (605):  0.06986647\n",
      "Test loss at epoch (605):  0.040430106\n",
      "Train loss at epoch (606):  0.06994094\n",
      "Test loss at epoch (606):  0.04032102\n",
      "Train loss at epoch (607):  0.069719836\n",
      "Test loss at epoch (607):  0.04036956\n",
      "Train loss at epoch (608):  0.07000714\n",
      "Test loss at epoch (608):  0.040260695\n",
      "Train loss at epoch (609):  0.0701648\n",
      "Test loss at epoch (609):  0.04047993\n",
      "Train loss at epoch (610):  0.06964446\n",
      "Test loss at epoch (610):  0.04039141\n",
      "Train loss at epoch (611):  0.06961213\n",
      "Test loss at epoch (611):  0.04032054\n",
      "Train loss at epoch (612):  0.07000362\n",
      "Test loss at epoch (612):  0.040368747\n",
      "Train loss at epoch (613):  0.069648765\n",
      "Test loss at epoch (613):  0.040367913\n",
      "Train loss at epoch (614):  0.069691464\n",
      "Test loss at epoch (614):  0.040266324\n",
      "Train loss at epoch (615):  0.06998227\n",
      "Test loss at epoch (615):  0.040296264\n",
      "Train loss at epoch (616):  0.069785446\n",
      "Test loss at epoch (616):  0.04026061\n",
      "Train loss at epoch (617):  0.06974479\n",
      "Test loss at epoch (617):  0.04033228\n",
      "Train loss at epoch (618):  0.06980537\n",
      "Test loss at epoch (618):  0.040238235\n",
      "Train loss at epoch (619):  0.069926396\n",
      "Test loss at epoch (619):  0.040301926\n",
      "Train loss at epoch (620):  0.06947342\n",
      "Test loss at epoch (620):  0.040342778\n",
      "Train loss at epoch (621):  0.06983181\n",
      "Test loss at epoch (621):  0.040268995\n",
      "Train loss at epoch (622):  0.069961436\n",
      "Test loss at epoch (622):  0.040205874\n",
      "Train loss at epoch (623):  0.06969357\n",
      "Test loss at epoch (623):  0.040245518\n",
      "Train loss at epoch (624):  0.06975341\n",
      "Test loss at epoch (624):  0.040117484\n",
      "Train loss at epoch (625):  0.06944219\n",
      "Test loss at epoch (625):  0.04017016\n",
      "Train loss at epoch (626):  0.06941606\n",
      "Test loss at epoch (626):  0.040049933\n",
      "Train loss at epoch (627):  0.06942953\n",
      "Test loss at epoch (627):  0.04003191\n",
      "Train loss at epoch (628):  0.06994073\n",
      "Test loss at epoch (628):  0.040160667\n",
      "Train loss at epoch (629):  0.06942896\n",
      "Test loss at epoch (629):  0.04012742\n",
      "Train loss at epoch (630):  0.06943536\n",
      "Test loss at epoch (630):  0.04008125\n",
      "Train loss at epoch (631):  0.06903904\n",
      "Test loss at epoch (631):  0.040081423\n",
      "Train loss at epoch (632):  0.06928845\n",
      "Test loss at epoch (632):  0.039921056\n",
      "Train loss at epoch (633):  0.06966842\n",
      "Test loss at epoch (633):  0.040006623\n",
      "Train loss at epoch (634):  0.06956049\n",
      "Test loss at epoch (634):  0.039996568\n",
      "Train loss at epoch (635):  0.06898354\n",
      "Test loss at epoch (635):  0.039891943\n",
      "Train loss at epoch (636):  0.069379725\n",
      "Test loss at epoch (636):  0.039915074\n",
      "Train loss at epoch (637):  0.069564275\n",
      "Test loss at epoch (637):  0.03997724\n",
      "Train loss at epoch (638):  0.069239795\n",
      "Test loss at epoch (638):  0.04000941\n",
      "Train loss at epoch (639):  0.06947408\n",
      "Test loss at epoch (639):  0.03996028\n",
      "Train loss at epoch (640):  0.069215275\n",
      "Test loss at epoch (640):  0.039974693\n",
      "Train loss at epoch (641):  0.06914162\n",
      "Test loss at epoch (641):  0.0399723\n",
      "Train loss at epoch (642):  0.06871407\n",
      "Test loss at epoch (642):  0.0398833\n",
      "Train loss at epoch (643):  0.069369584\n",
      "Test loss at epoch (643):  0.03989184\n",
      "Train loss at epoch (644):  0.069018826\n",
      "Test loss at epoch (644):  0.03989922\n",
      "Train loss at epoch (645):  0.06894339\n",
      "Test loss at epoch (645):  0.039802317\n",
      "Train loss at epoch (646):  0.06907559\n",
      "Test loss at epoch (646):  0.03985511\n",
      "Train loss at epoch (647):  0.0690619\n",
      "Test loss at epoch (647):  0.03979267\n",
      "Train loss at epoch (648):  0.06906932\n",
      "Test loss at epoch (648):  0.039758626\n",
      "Train loss at epoch (649):  0.069316484\n",
      "Test loss at epoch (649):  0.039761256\n",
      "Train loss at epoch (650):  0.06862364\n",
      "Test loss at epoch (650):  0.03977286\n",
      "Train loss at epoch (651):  0.069104634\n",
      "Test loss at epoch (651):  0.039761353\n",
      "Train loss at epoch (652):  0.06865045\n",
      "Test loss at epoch (652):  0.039668977\n",
      "Train loss at epoch (653):  0.06937843\n",
      "Test loss at epoch (653):  0.039648473\n",
      "Train loss at epoch (654):  0.06870446\n",
      "Test loss at epoch (654):  0.039657243\n",
      "Train loss at epoch (655):  0.06867412\n",
      "Test loss at epoch (655):  0.039546717\n",
      "Train loss at epoch (656):  0.069013245\n",
      "Test loss at epoch (656):  0.03958701\n",
      "Train loss at epoch (657):  0.06915749\n",
      "Test loss at epoch (657):  0.03965282\n",
      "Train loss at epoch (658):  0.068921804\n",
      "Test loss at epoch (658):  0.039480925\n",
      "Train loss at epoch (659):  0.06895619\n",
      "Test loss at epoch (659):  0.039548516\n",
      "Train loss at epoch (660):  0.068743795\n",
      "Test loss at epoch (660):  0.039518088\n",
      "Train loss at epoch (661):  0.06918916\n",
      "Test loss at epoch (661):  0.03953145\n",
      "Train loss at epoch (662):  0.069141634\n",
      "Test loss at epoch (662):  0.039549075\n",
      "Train loss at epoch (663):  0.0685423\n",
      "Test loss at epoch (663):  0.039461643\n",
      "Train loss at epoch (664):  0.06875582\n",
      "Test loss at epoch (664):  0.039469603\n",
      "Train loss at epoch (665):  0.06851947\n",
      "Test loss at epoch (665):  0.039451886\n",
      "Train loss at epoch (666):  0.068973914\n",
      "Test loss at epoch (666):  0.03947986\n",
      "Train loss at epoch (667):  0.06871456\n",
      "Test loss at epoch (667):  0.03944599\n",
      "Train loss at epoch (668):  0.0687273\n",
      "Test loss at epoch (668):  0.039330978\n",
      "Train loss at epoch (669):  0.068721615\n",
      "Test loss at epoch (669):  0.039373152\n",
      "Train loss at epoch (670):  0.06858288\n",
      "Test loss at epoch (670):  0.039353177\n",
      "Train loss at epoch (671):  0.06889038\n",
      "Test loss at epoch (671):  0.03937076\n",
      "Train loss at epoch (672):  0.06841047\n",
      "Test loss at epoch (672):  0.039323427\n",
      "Train loss at epoch (673):  0.06865311\n",
      "Test loss at epoch (673):  0.03930206\n",
      "Train loss at epoch (674):  0.06911936\n",
      "Test loss at epoch (674):  0.039381817\n",
      "Train loss at epoch (675):  0.06867059\n",
      "Test loss at epoch (675):  0.03938763\n",
      "Train loss at epoch (676):  0.06859437\n",
      "Test loss at epoch (676):  0.039354026\n",
      "Train loss at epoch (677):  0.06876306\n",
      "Test loss at epoch (677):  0.039363947\n",
      "Train loss at epoch (678):  0.06838055\n",
      "Test loss at epoch (678):  0.039262045\n",
      "Train loss at epoch (679):  0.06866835\n",
      "Test loss at epoch (679):  0.039213404\n",
      "Train loss at epoch (680):  0.06828943\n",
      "Test loss at epoch (680):  0.03918422\n",
      "Train loss at epoch (681):  0.06844469\n",
      "Test loss at epoch (681):  0.039141227\n",
      "Train loss at epoch (682):  0.068259634\n",
      "Test loss at epoch (682):  0.039205633\n",
      "Train loss at epoch (683):  0.06859347\n",
      "Test loss at epoch (683):  0.03919224\n",
      "Train loss at epoch (684):  0.06806101\n",
      "Test loss at epoch (684):  0.03915574\n",
      "Train loss at epoch (685):  0.068107426\n",
      "Test loss at epoch (685):  0.039098397\n",
      "Train loss at epoch (686):  0.06855252\n",
      "Test loss at epoch (686):  0.039069004\n",
      "Train loss at epoch (687):  0.06840352\n",
      "Test loss at epoch (687):  0.039088707\n",
      "Train loss at epoch (688):  0.0684763\n",
      "Test loss at epoch (688):  0.039048154\n",
      "Train loss at epoch (689):  0.06843123\n",
      "Test loss at epoch (689):  0.039060142\n",
      "Train loss at epoch (690):  0.068280436\n",
      "Test loss at epoch (690):  0.03910611\n",
      "Train loss at epoch (691):  0.06833355\n",
      "Test loss at epoch (691):  0.03911093\n",
      "Train loss at epoch (692):  0.06812419\n",
      "Test loss at epoch (692):  0.039064974\n",
      "Train loss at epoch (693):  0.068324886\n",
      "Test loss at epoch (693):  0.039032564\n",
      "Train loss at epoch (694):  0.06829851\n",
      "Test loss at epoch (694):  0.038996864\n",
      "Train loss at epoch (695):  0.068127155\n",
      "Test loss at epoch (695):  0.039004803\n",
      "Train loss at epoch (696):  0.068296015\n",
      "Test loss at epoch (696):  0.039071843\n",
      "Train loss at epoch (697):  0.06791424\n",
      "Test loss at epoch (697):  0.03905649\n",
      "Train loss at epoch (698):  0.06799463\n",
      "Test loss at epoch (698):  0.039096862\n",
      "Train loss at epoch (699):  0.068093866\n",
      "Test loss at epoch (699):  0.038941003\n",
      "Train loss at epoch (700):  0.06853852\n",
      "Test loss at epoch (700):  0.038930096\n",
      "Train loss at epoch (701):  0.06807548\n",
      "Test loss at epoch (701):  0.039028317\n",
      "Train loss at epoch (702):  0.06807308\n",
      "Test loss at epoch (702):  0.038910285\n",
      "Train loss at epoch (703):  0.067733586\n",
      "Test loss at epoch (703):  0.03886695\n",
      "Train loss at epoch (704):  0.068010844\n",
      "Test loss at epoch (704):  0.03884463\n",
      "Train loss at epoch (705):  0.06793346\n",
      "Test loss at epoch (705):  0.03879704\n",
      "Train loss at epoch (706):  0.06816624\n",
      "Test loss at epoch (706):  0.038799208\n",
      "Train loss at epoch (707):  0.06787653\n",
      "Test loss at epoch (707):  0.038794547\n",
      "Train loss at epoch (708):  0.06765799\n",
      "Test loss at epoch (708):  0.038842898\n",
      "Train loss at epoch (709):  0.06811963\n",
      "Test loss at epoch (709):  0.038781855\n",
      "Train loss at epoch (710):  0.06771595\n",
      "Test loss at epoch (710):  0.038777202\n",
      "Train loss at epoch (711):  0.06787051\n",
      "Test loss at epoch (711):  0.038774103\n",
      "Train loss at epoch (712):  0.06793524\n",
      "Test loss at epoch (712):  0.038807202\n",
      "Train loss at epoch (713):  0.067910135\n",
      "Test loss at epoch (713):  0.0388635\n",
      "Train loss at epoch (714):  0.06807685\n",
      "Test loss at epoch (714):  0.038742807\n",
      "Train loss at epoch (715):  0.06785109\n",
      "Test loss at epoch (715):  0.038764566\n",
      "Train loss at epoch (716):  0.06777783\n",
      "Test loss at epoch (716):  0.038786694\n",
      "Train loss at epoch (717):  0.067978494\n",
      "Test loss at epoch (717):  0.038685333\n",
      "Train loss at epoch (718):  0.0678738\n",
      "Test loss at epoch (718):  0.0386923\n",
      "Train loss at epoch (719):  0.06798628\n",
      "Test loss at epoch (719):  0.038758904\n",
      "Train loss at epoch (720):  0.06761956\n",
      "Test loss at epoch (720):  0.038806904\n",
      "Train loss at epoch (721):  0.06753347\n",
      "Test loss at epoch (721):  0.038830873\n",
      "Train loss at epoch (722):  0.067630865\n",
      "Test loss at epoch (722):  0.0387406\n",
      "Train loss at epoch (723):  0.067393236\n",
      "Test loss at epoch (723):  0.038665283\n",
      "Train loss at epoch (724):  0.06751948\n",
      "Test loss at epoch (724):  0.03869831\n",
      "Train loss at epoch (725):  0.06741714\n",
      "Test loss at epoch (725):  0.03858167\n",
      "Train loss at epoch (726):  0.06758127\n",
      "Test loss at epoch (726):  0.038597573\n",
      "Train loss at epoch (727):  0.06786964\n",
      "Test loss at epoch (727):  0.03864893\n",
      "Train loss at epoch (728):  0.06756672\n",
      "Test loss at epoch (728):  0.038608488\n",
      "Train loss at epoch (729):  0.06763488\n",
      "Test loss at epoch (729):  0.03864017\n",
      "Train loss at epoch (730):  0.06782056\n",
      "Test loss at epoch (730):  0.038618054\n",
      "Train loss at epoch (731):  0.067321\n",
      "Test loss at epoch (731):  0.038508784\n",
      "Train loss at epoch (732):  0.06738119\n",
      "Test loss at epoch (732):  0.038421396\n",
      "Train loss at epoch (733):  0.06760777\n",
      "Test loss at epoch (733):  0.038448665\n",
      "Train loss at epoch (734):  0.06742341\n",
      "Test loss at epoch (734):  0.03844029\n",
      "Train loss at epoch (735):  0.06769789\n",
      "Test loss at epoch (735):  0.03854775\n",
      "Train loss at epoch (736):  0.06757791\n",
      "Test loss at epoch (736):  0.0384528\n",
      "Train loss at epoch (737):  0.067245185\n",
      "Test loss at epoch (737):  0.038610823\n",
      "Train loss at epoch (738):  0.067556925\n",
      "Test loss at epoch (738):  0.03848027\n",
      "Train loss at epoch (739):  0.06733908\n",
      "Test loss at epoch (739):  0.03849151\n",
      "Train loss at epoch (740):  0.067473754\n",
      "Test loss at epoch (740):  0.038482677\n",
      "Train loss at epoch (741):  0.06719316\n",
      "Test loss at epoch (741):  0.03845147\n",
      "Train loss at epoch (742):  0.067746535\n",
      "Test loss at epoch (742):  0.038379997\n",
      "Train loss at epoch (743):  0.0677889\n",
      "Test loss at epoch (743):  0.038473174\n",
      "Train loss at epoch (744):  0.06706132\n",
      "Test loss at epoch (744):  0.038415022\n",
      "Train loss at epoch (745):  0.06739989\n",
      "Test loss at epoch (745):  0.03836052\n",
      "Train loss at epoch (746):  0.06736026\n",
      "Test loss at epoch (746):  0.038414683\n",
      "Train loss at epoch (747):  0.0671741\n",
      "Test loss at epoch (747):  0.038376\n",
      "Train loss at epoch (748):  0.06711619\n",
      "Test loss at epoch (748):  0.03829751\n",
      "Train loss at epoch (749):  0.06717898\n",
      "Test loss at epoch (749):  0.038298093\n",
      "Train loss at epoch (750):  0.06717894\n",
      "Test loss at epoch (750):  0.038281012\n",
      "Train loss at epoch (751):  0.06717827\n",
      "Test loss at epoch (751):  0.03834281\n",
      "Train loss at epoch (752):  0.0672755\n",
      "Test loss at epoch (752):  0.03825482\n",
      "Train loss at epoch (753):  0.06694368\n",
      "Test loss at epoch (753):  0.038246658\n",
      "Train loss at epoch (754):  0.06726522\n",
      "Test loss at epoch (754):  0.03824296\n",
      "Train loss at epoch (755):  0.06714173\n",
      "Test loss at epoch (755):  0.038237665\n",
      "Train loss at epoch (756):  0.06748475\n",
      "Test loss at epoch (756):  0.038178455\n",
      "Train loss at epoch (757):  0.06713885\n",
      "Test loss at epoch (757):  0.038181674\n",
      "Train loss at epoch (758):  0.06708007\n",
      "Test loss at epoch (758):  0.038225543\n",
      "Train loss at epoch (759):  0.06723522\n",
      "Test loss at epoch (759):  0.038304742\n",
      "Train loss at epoch (760):  0.06707204\n",
      "Test loss at epoch (760):  0.038284305\n",
      "Train loss at epoch (761):  0.067044556\n",
      "Test loss at epoch (761):  0.038240686\n",
      "Train loss at epoch (762):  0.06718406\n",
      "Test loss at epoch (762):  0.038157746\n",
      "Train loss at epoch (763):  0.06709623\n",
      "Test loss at epoch (763):  0.038204663\n",
      "Train loss at epoch (764):  0.066930965\n",
      "Test loss at epoch (764):  0.03818261\n",
      "Train loss at epoch (765):  0.067224875\n",
      "Test loss at epoch (765):  0.038273405\n",
      "Train loss at epoch (766):  0.06701753\n",
      "Test loss at epoch (766):  0.038188826\n",
      "Train loss at epoch (767):  0.067289166\n",
      "Test loss at epoch (767):  0.038194355\n",
      "Train loss at epoch (768):  0.0669998\n",
      "Test loss at epoch (768):  0.03822391\n",
      "Train loss at epoch (769):  0.06708806\n",
      "Test loss at epoch (769):  0.03816518\n",
      "Train loss at epoch (770):  0.06745649\n",
      "Test loss at epoch (770):  0.03813888\n",
      "Train loss at epoch (771):  0.06710041\n",
      "Test loss at epoch (771):  0.038073048\n",
      "Train loss at epoch (772):  0.06703787\n",
      "Test loss at epoch (772):  0.038101196\n",
      "Train loss at epoch (773):  0.06688569\n",
      "Test loss at epoch (773):  0.038165558\n",
      "Train loss at epoch (774):  0.06695849\n",
      "Test loss at epoch (774):  0.038125165\n",
      "Train loss at epoch (775):  0.066998914\n",
      "Test loss at epoch (775):  0.03802932\n",
      "Train loss at epoch (776):  0.06742019\n",
      "Test loss at epoch (776):  0.038067035\n",
      "Train loss at epoch (777):  0.067036316\n",
      "Test loss at epoch (777):  0.037974305\n",
      "Train loss at epoch (778):  0.06693348\n",
      "Test loss at epoch (778):  0.037953753\n",
      "Train loss at epoch (779):  0.06684811\n",
      "Test loss at epoch (779):  0.038029294\n",
      "Train loss at epoch (780):  0.0670269\n",
      "Test loss at epoch (780):  0.038046088\n",
      "Train loss at epoch (781):  0.06705326\n",
      "Test loss at epoch (781):  0.037998885\n",
      "Train loss at epoch (782):  0.06699228\n",
      "Test loss at epoch (782):  0.037924457\n",
      "Train loss at epoch (783):  0.06685152\n",
      "Test loss at epoch (783):  0.037922487\n",
      "Train loss at epoch (784):  0.06726862\n",
      "Test loss at epoch (784):  0.038051397\n",
      "Train loss at epoch (785):  0.06702254\n",
      "Test loss at epoch (785):  0.03804186\n",
      "Train loss at epoch (786):  0.06669683\n",
      "Test loss at epoch (786):  0.03796112\n",
      "Train loss at epoch (787):  0.06668801\n",
      "Test loss at epoch (787):  0.037993424\n",
      "Train loss at epoch (788):  0.06711752\n",
      "Test loss at epoch (788):  0.038020577\n",
      "Train loss at epoch (789):  0.06654818\n",
      "Test loss at epoch (789):  0.03792117\n",
      "Train loss at epoch (790):  0.06686009\n",
      "Test loss at epoch (790):  0.037889723\n",
      "Train loss at epoch (791):  0.066612504\n",
      "Test loss at epoch (791):  0.0379511\n",
      "Train loss at epoch (792):  0.066991724\n",
      "Test loss at epoch (792):  0.03790415\n",
      "Train loss at epoch (793):  0.0670376\n",
      "Test loss at epoch (793):  0.037918\n",
      "Train loss at epoch (794):  0.06686171\n",
      "Test loss at epoch (794):  0.037867453\n",
      "Train loss at epoch (795):  0.06695084\n",
      "Test loss at epoch (795):  0.037966635\n",
      "Train loss at epoch (796):  0.06689205\n",
      "Test loss at epoch (796):  0.037913904\n",
      "Train loss at epoch (797):  0.06660065\n",
      "Test loss at epoch (797):  0.03787129\n",
      "Train loss at epoch (798):  0.06656853\n",
      "Test loss at epoch (798):  0.037776288\n",
      "Train loss at epoch (799):  0.0667567\n",
      "Test loss at epoch (799):  0.037800565\n",
      "Train loss at epoch (800):  0.066573076\n",
      "Test loss at epoch (800):  0.037865233\n",
      "Train loss at epoch (801):  0.06688124\n",
      "Test loss at epoch (801):  0.037841536\n",
      "Train loss at epoch (802):  0.0667525\n",
      "Test loss at epoch (802):  0.0377934\n",
      "Train loss at epoch (803):  0.06647318\n",
      "Test loss at epoch (803):  0.037733275\n",
      "Train loss at epoch (804):  0.066540666\n",
      "Test loss at epoch (804):  0.03772464\n",
      "Train loss at epoch (805):  0.06637233\n",
      "Test loss at epoch (805):  0.037703544\n",
      "Train loss at epoch (806):  0.06613863\n",
      "Test loss at epoch (806):  0.037660655\n",
      "Train loss at epoch (807):  0.06655947\n",
      "Test loss at epoch (807):  0.037720133\n",
      "Train loss at epoch (808):  0.06666531\n",
      "Test loss at epoch (808):  0.03773983\n",
      "Train loss at epoch (809):  0.066489846\n",
      "Test loss at epoch (809):  0.03771193\n",
      "Train loss at epoch (810):  0.06618837\n",
      "Test loss at epoch (810):  0.037608676\n",
      "Train loss at epoch (811):  0.066588044\n",
      "Test loss at epoch (811):  0.03772309\n",
      "Train loss at epoch (812):  0.066703655\n",
      "Test loss at epoch (812):  0.037656497\n",
      "Train loss at epoch (813):  0.06664805\n",
      "Test loss at epoch (813):  0.037588708\n",
      "Train loss at epoch (814):  0.06658825\n",
      "Test loss at epoch (814):  0.037653875\n",
      "Train loss at epoch (815):  0.06656237\n",
      "Test loss at epoch (815):  0.03764529\n",
      "Train loss at epoch (816):  0.066371\n",
      "Test loss at epoch (816):  0.03757162\n",
      "Train loss at epoch (817):  0.06635134\n",
      "Test loss at epoch (817):  0.03759864\n",
      "Train loss at epoch (818):  0.06602115\n",
      "Test loss at epoch (818):  0.037577055\n",
      "Train loss at epoch (819):  0.066183016\n",
      "Test loss at epoch (819):  0.037605256\n",
      "Train loss at epoch (820):  0.06644265\n",
      "Test loss at epoch (820):  0.037588686\n",
      "Train loss at epoch (821):  0.06614406\n",
      "Test loss at epoch (821):  0.037535783\n",
      "Train loss at epoch (822):  0.06622885\n",
      "Test loss at epoch (822):  0.03755771\n",
      "Train loss at epoch (823):  0.06608169\n",
      "Test loss at epoch (823):  0.037577044\n",
      "Train loss at epoch (824):  0.06634992\n",
      "Test loss at epoch (824):  0.037547335\n",
      "Train loss at epoch (825):  0.06618971\n",
      "Test loss at epoch (825):  0.03753362\n",
      "Train loss at epoch (826):  0.06623692\n",
      "Test loss at epoch (826):  0.037550583\n",
      "Train loss at epoch (827):  0.066410944\n",
      "Test loss at epoch (827):  0.03757777\n",
      "Train loss at epoch (828):  0.066150516\n",
      "Test loss at epoch (828):  0.03758503\n",
      "Train loss at epoch (829):  0.066310324\n",
      "Test loss at epoch (829):  0.03755479\n",
      "Train loss at epoch (830):  0.066189006\n",
      "Test loss at epoch (830):  0.03757412\n",
      "Train loss at epoch (831):  0.06664334\n",
      "Test loss at epoch (831):  0.037513535\n",
      "Train loss at epoch (832):  0.06634938\n",
      "Test loss at epoch (832):  0.03758418\n",
      "Train loss at epoch (833):  0.066185296\n",
      "Test loss at epoch (833):  0.037492324\n",
      "Train loss at epoch (834):  0.06623595\n",
      "Test loss at epoch (834):  0.0374975\n",
      "Train loss at epoch (835):  0.066003285\n",
      "Test loss at epoch (835):  0.03757623\n",
      "Train loss at epoch (836):  0.06624745\n",
      "Test loss at epoch (836):  0.03745675\n",
      "Train loss at epoch (837):  0.06616316\n",
      "Test loss at epoch (837):  0.03751001\n",
      "Train loss at epoch (838):  0.06632428\n",
      "Test loss at epoch (838):  0.037434217\n",
      "Train loss at epoch (839):  0.06632282\n",
      "Test loss at epoch (839):  0.037405144\n",
      "Train loss at epoch (840):  0.06599409\n",
      "Test loss at epoch (840):  0.03750975\n",
      "Train loss at epoch (841):  0.06605676\n",
      "Test loss at epoch (841):  0.03743754\n",
      "Train loss at epoch (842):  0.06635893\n",
      "Test loss at epoch (842):  0.037519507\n",
      "Train loss at epoch (843):  0.06586588\n",
      "Test loss at epoch (843):  0.037462257\n",
      "Train loss at epoch (844):  0.06607233\n",
      "Test loss at epoch (844):  0.03742759\n",
      "Train loss at epoch (845):  0.06624376\n",
      "Test loss at epoch (845):  0.037435364\n",
      "Train loss at epoch (846):  0.06612784\n",
      "Test loss at epoch (846):  0.037425503\n",
      "Train loss at epoch (847):  0.06647069\n",
      "Test loss at epoch (847):  0.03739019\n",
      "Train loss at epoch (848):  0.066553056\n",
      "Test loss at epoch (848):  0.03739481\n",
      "Train loss at epoch (849):  0.06590607\n",
      "Test loss at epoch (849):  0.03741126\n",
      "Train loss at epoch (850):  0.065866366\n",
      "Test loss at epoch (850):  0.037355736\n",
      "Train loss at epoch (851):  0.06582655\n",
      "Test loss at epoch (851):  0.037326645\n",
      "Train loss at epoch (852):  0.066053584\n",
      "Test loss at epoch (852):  0.037316635\n",
      "Train loss at epoch (853):  0.06570834\n",
      "Test loss at epoch (853):  0.037322972\n",
      "Train loss at epoch (854):  0.065551005\n",
      "Test loss at epoch (854):  0.037288208\n",
      "Train loss at epoch (855):  0.0657885\n",
      "Test loss at epoch (855):  0.03735069\n",
      "Train loss at epoch (856):  0.06580889\n",
      "Test loss at epoch (856):  0.03728627\n",
      "Train loss at epoch (857):  0.06602877\n",
      "Test loss at epoch (857):  0.03726507\n",
      "Train loss at epoch (858):  0.06586258\n",
      "Test loss at epoch (858):  0.037278958\n",
      "Train loss at epoch (859):  0.06626264\n",
      "Test loss at epoch (859):  0.037335094\n",
      "Train loss at epoch (860):  0.06614207\n",
      "Test loss at epoch (860):  0.037375886\n",
      "Train loss at epoch (861):  0.06557528\n",
      "Test loss at epoch (861):  0.03722793\n",
      "Train loss at epoch (862):  0.06617413\n",
      "Test loss at epoch (862):  0.03724587\n",
      "Train loss at epoch (863):  0.065663\n",
      "Test loss at epoch (863):  0.037252743\n",
      "Train loss at epoch (864):  0.0661479\n",
      "Test loss at epoch (864):  0.037222784\n",
      "Train loss at epoch (865):  0.06595884\n",
      "Test loss at epoch (865):  0.037236553\n",
      "Train loss at epoch (866):  0.065697566\n",
      "Test loss at epoch (866):  0.037280053\n",
      "Train loss at epoch (867):  0.06582592\n",
      "Test loss at epoch (867):  0.03722978\n",
      "Train loss at epoch (868):  0.0659499\n",
      "Test loss at epoch (868):  0.037209556\n",
      "Train loss at epoch (869):  0.065820284\n",
      "Test loss at epoch (869):  0.03722453\n",
      "Train loss at epoch (870):  0.06571098\n",
      "Test loss at epoch (870):  0.03716045\n",
      "Train loss at epoch (871):  0.0654723\n",
      "Test loss at epoch (871):  0.037158385\n",
      "Train loss at epoch (872):  0.06618591\n",
      "Test loss at epoch (872):  0.03720357\n",
      "Train loss at epoch (873):  0.06569049\n",
      "Test loss at epoch (873):  0.037227582\n",
      "Train loss at epoch (874):  0.06587402\n",
      "Test loss at epoch (874):  0.037228823\n",
      "Train loss at epoch (875):  0.06590315\n",
      "Test loss at epoch (875):  0.0372176\n",
      "Train loss at epoch (876):  0.06613201\n",
      "Test loss at epoch (876):  0.03719243\n",
      "Train loss at epoch (877):  0.065766804\n",
      "Test loss at epoch (877):  0.037134957\n",
      "Train loss at epoch (878):  0.065663144\n",
      "Test loss at epoch (878):  0.03718402\n",
      "Train loss at epoch (879):  0.065129116\n",
      "Test loss at epoch (879):  0.03712023\n",
      "Train loss at epoch (880):  0.06633839\n",
      "Test loss at epoch (880):  0.037129372\n",
      "Train loss at epoch (881):  0.06584672\n",
      "Test loss at epoch (881):  0.037092276\n",
      "Train loss at epoch (882):  0.06573111\n",
      "Test loss at epoch (882):  0.037055183\n",
      "Train loss at epoch (883):  0.06587455\n",
      "Test loss at epoch (883):  0.037089694\n",
      "Train loss at epoch (884):  0.0655829\n",
      "Test loss at epoch (884):  0.037105173\n",
      "Train loss at epoch (885):  0.066161074\n",
      "Test loss at epoch (885):  0.03705642\n",
      "Train loss at epoch (886):  0.06591341\n",
      "Test loss at epoch (886):  0.037071493\n",
      "Train loss at epoch (887):  0.06611342\n",
      "Test loss at epoch (887):  0.037052933\n",
      "Train loss at epoch (888):  0.065523446\n",
      "Test loss at epoch (888):  0.03704175\n",
      "Train loss at epoch (889):  0.06590364\n",
      "Test loss at epoch (889):  0.037045594\n",
      "Train loss at epoch (890):  0.0656481\n",
      "Test loss at epoch (890):  0.0369779\n",
      "Train loss at epoch (891):  0.06584777\n",
      "Test loss at epoch (891):  0.03702413\n",
      "Train loss at epoch (892):  0.06567407\n",
      "Test loss at epoch (892):  0.03705049\n",
      "Train loss at epoch (893):  0.06590147\n",
      "Test loss at epoch (893):  0.037020236\n",
      "Train loss at epoch (894):  0.06567049\n",
      "Test loss at epoch (894):  0.037068386\n",
      "Train loss at epoch (895):  0.06569482\n",
      "Test loss at epoch (895):  0.037042435\n",
      "Train loss at epoch (896):  0.06554224\n",
      "Test loss at epoch (896):  0.036985993\n",
      "Train loss at epoch (897):  0.0656235\n",
      "Test loss at epoch (897):  0.03697256\n",
      "Train loss at epoch (898):  0.06599615\n",
      "Test loss at epoch (898):  0.036977023\n",
      "Train loss at epoch (899):  0.06551014\n",
      "Test loss at epoch (899):  0.037069052\n",
      "Train loss at epoch (900):  0.06582926\n",
      "Test loss at epoch (900):  0.036940053\n",
      "Train loss at epoch (901):  0.06556177\n",
      "Test loss at epoch (901):  0.036946665\n",
      "Train loss at epoch (902):  0.065924264\n",
      "Test loss at epoch (902):  0.03701126\n",
      "Train loss at epoch (903):  0.06563641\n",
      "Test loss at epoch (903):  0.036886342\n",
      "Train loss at epoch (904):  0.06562729\n",
      "Test loss at epoch (904):  0.036907785\n",
      "Train loss at epoch (905):  0.065614946\n",
      "Test loss at epoch (905):  0.03695901\n",
      "Train loss at epoch (906):  0.06550809\n",
      "Test loss at epoch (906):  0.036990747\n",
      "Train loss at epoch (907):  0.06577552\n",
      "Test loss at epoch (907):  0.036962613\n",
      "Train loss at epoch (908):  0.06555227\n",
      "Test loss at epoch (908):  0.03696966\n",
      "Train loss at epoch (909):  0.065687984\n",
      "Test loss at epoch (909):  0.03694253\n",
      "Train loss at epoch (910):  0.06554524\n",
      "Test loss at epoch (910):  0.03692026\n",
      "Train loss at epoch (911):  0.06560614\n",
      "Test loss at epoch (911):  0.036903333\n",
      "Train loss at epoch (912):  0.06528387\n",
      "Test loss at epoch (912):  0.03690235\n",
      "Train loss at epoch (913):  0.06561117\n",
      "Test loss at epoch (913):  0.03693795\n",
      "Train loss at epoch (914):  0.06551528\n",
      "Test loss at epoch (914):  0.03695831\n",
      "Train loss at epoch (915):  0.06589755\n",
      "Test loss at epoch (915):  0.03690279\n",
      "Train loss at epoch (916):  0.06529871\n",
      "Test loss at epoch (916):  0.036846466\n",
      "Train loss at epoch (917):  0.0654695\n",
      "Test loss at epoch (917):  0.03680228\n",
      "Train loss at epoch (918):  0.06521402\n",
      "Test loss at epoch (918):  0.036865383\n",
      "Train loss at epoch (919):  0.065189876\n",
      "Test loss at epoch (919):  0.036797963\n",
      "Train loss at epoch (920):  0.065747805\n",
      "Test loss at epoch (920):  0.03684023\n",
      "Train loss at epoch (921):  0.06555726\n",
      "Test loss at epoch (921):  0.036889087\n",
      "Train loss at epoch (922):  0.065364525\n",
      "Test loss at epoch (922):  0.03682828\n",
      "Train loss at epoch (923):  0.06550635\n",
      "Test loss at epoch (923):  0.03688362\n",
      "Train loss at epoch (924):  0.0651313\n",
      "Test loss at epoch (924):  0.036826134\n",
      "Train loss at epoch (925):  0.0656595\n",
      "Test loss at epoch (925):  0.036840722\n",
      "Train loss at epoch (926):  0.0651022\n",
      "Test loss at epoch (926):  0.036862027\n",
      "Train loss at epoch (927):  0.06527834\n",
      "Test loss at epoch (927):  0.036799524\n",
      "Train loss at epoch (928):  0.065166935\n",
      "Test loss at epoch (928):  0.036854237\n",
      "Train loss at epoch (929):  0.064978495\n",
      "Test loss at epoch (929):  0.03686128\n",
      "Train loss at epoch (930):  0.0652819\n",
      "Test loss at epoch (930):  0.03685042\n",
      "Train loss at epoch (931):  0.065285854\n",
      "Test loss at epoch (931):  0.03678621\n",
      "Train loss at epoch (932):  0.065129586\n",
      "Test loss at epoch (932):  0.036798935\n",
      "Train loss at epoch (933):  0.06502244\n",
      "Test loss at epoch (933):  0.036822807\n",
      "Train loss at epoch (934):  0.06549263\n",
      "Test loss at epoch (934):  0.036818612\n",
      "Train loss at epoch (935):  0.06545358\n",
      "Test loss at epoch (935):  0.03681761\n",
      "Train loss at epoch (936):  0.06544701\n",
      "Test loss at epoch (936):  0.036785815\n",
      "Train loss at epoch (937):  0.06532075\n",
      "Test loss at epoch (937):  0.036806062\n",
      "Train loss at epoch (938):  0.065412104\n",
      "Test loss at epoch (938):  0.03677747\n",
      "Train loss at epoch (939):  0.065602586\n",
      "Test loss at epoch (939):  0.0367994\n",
      "Train loss at epoch (940):  0.065147184\n",
      "Test loss at epoch (940):  0.036793735\n",
      "Train loss at epoch (941):  0.06517468\n",
      "Test loss at epoch (941):  0.03684278\n",
      "Train loss at epoch (942):  0.0654021\n",
      "Test loss at epoch (942):  0.03685072\n",
      "Train loss at epoch (943):  0.06521966\n",
      "Test loss at epoch (943):  0.036768764\n",
      "Train loss at epoch (944):  0.065186\n",
      "Test loss at epoch (944):  0.036769103\n",
      "Train loss at epoch (945):  0.065163404\n",
      "Test loss at epoch (945):  0.03666969\n",
      "Train loss at epoch (946):  0.065298565\n",
      "Test loss at epoch (946):  0.036730394\n",
      "Train loss at epoch (947):  0.065279335\n",
      "Test loss at epoch (947):  0.03678744\n",
      "Train loss at epoch (948):  0.065206766\n",
      "Test loss at epoch (948):  0.036679424\n",
      "Train loss at epoch (949):  0.06510685\n",
      "Test loss at epoch (949):  0.03665488\n",
      "Train loss at epoch (950):  0.0650245\n",
      "Test loss at epoch (950):  0.036682736\n",
      "Train loss at epoch (951):  0.065186255\n",
      "Test loss at epoch (951):  0.03668649\n",
      "Train loss at epoch (952):  0.06515918\n",
      "Test loss at epoch (952):  0.036683146\n",
      "Train loss at epoch (953):  0.064955436\n",
      "Test loss at epoch (953):  0.036682952\n",
      "Train loss at epoch (954):  0.06527505\n",
      "Test loss at epoch (954):  0.036678933\n",
      "Train loss at epoch (955):  0.06541357\n",
      "Test loss at epoch (955):  0.036681503\n",
      "Train loss at epoch (956):  0.06519603\n",
      "Test loss at epoch (956):  0.03666024\n",
      "Train loss at epoch (957):  0.06491103\n",
      "Test loss at epoch (957):  0.036637735\n",
      "Train loss at epoch (958):  0.065138936\n",
      "Test loss at epoch (958):  0.036680445\n",
      "Train loss at epoch (959):  0.065286\n",
      "Test loss at epoch (959):  0.03668446\n",
      "Train loss at epoch (960):  0.065157354\n",
      "Test loss at epoch (960):  0.03664049\n",
      "Train loss at epoch (961):  0.06522284\n",
      "Test loss at epoch (961):  0.03668519\n",
      "Train loss at epoch (962):  0.06505092\n",
      "Test loss at epoch (962):  0.036691755\n",
      "Train loss at epoch (963):  0.06490886\n",
      "Test loss at epoch (963):  0.03665864\n",
      "Train loss at epoch (964):  0.06495687\n",
      "Test loss at epoch (964):  0.036663633\n",
      "Train loss at epoch (965):  0.0653116\n",
      "Test loss at epoch (965):  0.036671676\n",
      "Train loss at epoch (966):  0.064957835\n",
      "Test loss at epoch (966):  0.03669863\n",
      "Train loss at epoch (967):  0.065158516\n",
      "Test loss at epoch (967):  0.036645025\n",
      "Train loss at epoch (968):  0.06512402\n",
      "Test loss at epoch (968):  0.0366366\n",
      "Train loss at epoch (969):  0.06497166\n",
      "Test loss at epoch (969):  0.03670321\n",
      "Train loss at epoch (970):  0.06496742\n",
      "Test loss at epoch (970):  0.036703583\n",
      "Train loss at epoch (971):  0.06501691\n",
      "Test loss at epoch (971):  0.036725007\n",
      "Train loss at epoch (972):  0.065148994\n",
      "Test loss at epoch (972):  0.036610004\n",
      "Train loss at epoch (973):  0.06518935\n",
      "Test loss at epoch (973):  0.03658365\n",
      "Train loss at epoch (974):  0.06499095\n",
      "Test loss at epoch (974):  0.036620777\n",
      "Train loss at epoch (975):  0.065075904\n",
      "Test loss at epoch (975):  0.03656316\n",
      "Train loss at epoch (976):  0.0649681\n",
      "Test loss at epoch (976):  0.036537234\n",
      "Train loss at epoch (977):  0.06491241\n",
      "Test loss at epoch (977):  0.03660294\n",
      "Train loss at epoch (978):  0.06499436\n",
      "Test loss at epoch (978):  0.036560424\n",
      "Train loss at epoch (979):  0.06512517\n",
      "Test loss at epoch (979):  0.036525946\n",
      "Train loss at epoch (980):  0.06522625\n",
      "Test loss at epoch (980):  0.03654493\n",
      "Train loss at epoch (981):  0.06468272\n",
      "Test loss at epoch (981):  0.036524307\n",
      "Train loss at epoch (982):  0.06505405\n",
      "Test loss at epoch (982):  0.03648868\n",
      "Train loss at epoch (983):  0.06531424\n",
      "Test loss at epoch (983):  0.036482196\n",
      "Train loss at epoch (984):  0.06482083\n",
      "Test loss at epoch (984):  0.03652278\n",
      "Train loss at epoch (985):  0.06494736\n",
      "Test loss at epoch (985):  0.036508463\n",
      "Train loss at epoch (986):  0.06465538\n",
      "Test loss at epoch (986):  0.036560014\n",
      "Train loss at epoch (987):  0.065318085\n",
      "Test loss at epoch (987):  0.036526375\n",
      "Train loss at epoch (988):  0.06491524\n",
      "Test loss at epoch (988):  0.036534656\n",
      "Train loss at epoch (989):  0.06474159\n",
      "Test loss at epoch (989):  0.036556717\n",
      "Train loss at epoch (990):  0.06480761\n",
      "Test loss at epoch (990):  0.036540307\n",
      "Train loss at epoch (991):  0.065138295\n",
      "Test loss at epoch (991):  0.03654037\n",
      "Train loss at epoch (992):  0.06497124\n",
      "Test loss at epoch (992):  0.036504555\n",
      "Train loss at epoch (993):  0.06470123\n",
      "Test loss at epoch (993):  0.036471542\n",
      "Train loss at epoch (994):  0.06465104\n",
      "Test loss at epoch (994):  0.03647594\n",
      "Train loss at epoch (995):  0.064758174\n",
      "Test loss at epoch (995):  0.036412653\n",
      "Train loss at epoch (996):  0.06506149\n",
      "Test loss at epoch (996):  0.036502033\n",
      "Train loss at epoch (997):  0.0652186\n",
      "Test loss at epoch (997):  0.036452286\n",
      "Train loss at epoch (998):  0.06475785\n",
      "Test loss at epoch (998):  0.03645408\n",
      "Train loss at epoch (999):  0.065006405\n",
      "Test loss at epoch (999):  0.036461003\n",
      "Train loss at epoch (1000):  0.06465757\n",
      "Test loss at epoch (1000):  0.03651799\n",
      "Train loss at epoch (1001):  0.06436397\n",
      "Test loss at epoch (1001):  0.03648904\n",
      "Train loss at epoch (1002):  0.0647979\n",
      "Test loss at epoch (1002):  0.03643953\n",
      "Train loss at epoch (1003):  0.06483872\n",
      "Test loss at epoch (1003):  0.036494743\n",
      "Train loss at epoch (1004):  0.06491911\n",
      "Test loss at epoch (1004):  0.03642845\n",
      "Train loss at epoch (1005):  0.06503228\n",
      "Test loss at epoch (1005):  0.036456194\n",
      "Train loss at epoch (1006):  0.06462815\n",
      "Test loss at epoch (1006):  0.036479894\n",
      "Train loss at epoch (1007):  0.064496994\n",
      "Test loss at epoch (1007):  0.036445986\n",
      "Train loss at epoch (1008):  0.064595014\n",
      "Test loss at epoch (1008):  0.036429964\n",
      "Train loss at epoch (1009):  0.064655855\n",
      "Test loss at epoch (1009):  0.036441155\n",
      "Train loss at epoch (1010):  0.06492859\n",
      "Test loss at epoch (1010):  0.036427677\n",
      "Train loss at epoch (1011):  0.06496475\n",
      "Test loss at epoch (1011):  0.036432758\n",
      "Train loss at epoch (1012):  0.06501964\n",
      "Test loss at epoch (1012):  0.036489557\n",
      "Train loss at epoch (1013):  0.064721316\n",
      "Test loss at epoch (1013):  0.03648758\n",
      "Train loss at epoch (1014):  0.06464934\n",
      "Test loss at epoch (1014):  0.03637052\n",
      "Train loss at epoch (1015):  0.064774945\n",
      "Test loss at epoch (1015):  0.036399014\n",
      "Train loss at epoch (1016):  0.06483896\n",
      "Test loss at epoch (1016):  0.03644778\n",
      "Train loss at epoch (1017):  0.06493018\n",
      "Test loss at epoch (1017):  0.036371704\n",
      "Train loss at epoch (1018):  0.064628996\n",
      "Test loss at epoch (1018):  0.036372125\n",
      "Train loss at epoch (1019):  0.06499689\n",
      "Test loss at epoch (1019):  0.03636212\n",
      "Train loss at epoch (1020):  0.064461485\n",
      "Test loss at epoch (1020):  0.036388416\n",
      "Train loss at epoch (1021):  0.06456852\n",
      "Test loss at epoch (1021):  0.0364165\n",
      "Train loss at epoch (1022):  0.06441056\n",
      "Test loss at epoch (1022):  0.036374845\n",
      "Train loss at epoch (1023):  0.06472496\n",
      "Test loss at epoch (1023):  0.036381934\n",
      "Train loss at epoch (1024):  0.06491111\n",
      "Test loss at epoch (1024):  0.036369286\n",
      "Train loss at epoch (1025):  0.06460424\n",
      "Test loss at epoch (1025):  0.03634043\n",
      "Train loss at epoch (1026):  0.06461123\n",
      "Test loss at epoch (1026):  0.036396604\n",
      "Train loss at epoch (1027):  0.06489595\n",
      "Test loss at epoch (1027):  0.036368042\n",
      "Train loss at epoch (1028):  0.06517065\n",
      "Test loss at epoch (1028):  0.036330156\n",
      "Train loss at epoch (1029):  0.06467707\n",
      "Test loss at epoch (1029):  0.03638199\n",
      "Train loss at epoch (1030):  0.06468312\n",
      "Test loss at epoch (1030):  0.03635128\n",
      "Train loss at epoch (1031):  0.06445748\n",
      "Test loss at epoch (1031):  0.03638054\n",
      "Train loss at epoch (1032):  0.06498572\n",
      "Test loss at epoch (1032):  0.036350783\n",
      "Train loss at epoch (1033):  0.06454655\n",
      "Test loss at epoch (1033):  0.036357936\n",
      "Train loss at epoch (1034):  0.06495976\n",
      "Test loss at epoch (1034):  0.03637139\n",
      "Train loss at epoch (1035):  0.06478785\n",
      "Test loss at epoch (1035):  0.03639219\n",
      "Train loss at epoch (1036):  0.0649727\n",
      "Test loss at epoch (1036):  0.036367115\n",
      "Train loss at epoch (1037):  0.06479845\n",
      "Test loss at epoch (1037):  0.03633891\n",
      "Train loss at epoch (1038):  0.064454935\n",
      "Test loss at epoch (1038):  0.036382105\n",
      "Train loss at epoch (1039):  0.06479484\n",
      "Test loss at epoch (1039):  0.036334965\n",
      "Train loss at epoch (1040):  0.06455408\n",
      "Test loss at epoch (1040):  0.036397953\n",
      "Train loss at epoch (1041):  0.06475168\n",
      "Test loss at epoch (1041):  0.03639191\n",
      "Train loss at epoch (1042):  0.06436188\n",
      "Test loss at epoch (1042):  0.036403883\n",
      "Train loss at epoch (1043):  0.0645498\n",
      "Test loss at epoch (1043):  0.036399774\n",
      "Train loss at epoch (1044):  0.06455252\n",
      "Test loss at epoch (1044):  0.036363374\n",
      "Train loss at epoch (1045):  0.064608395\n",
      "Test loss at epoch (1045):  0.036351997\n",
      "Train loss at epoch (1046):  0.06507867\n",
      "Test loss at epoch (1046):  0.036329076\n",
      "Train loss at epoch (1047):  0.064782076\n",
      "Test loss at epoch (1047):  0.03632047\n",
      "Train loss at epoch (1048):  0.06462001\n",
      "Test loss at epoch (1048):  0.03631226\n",
      "Train loss at epoch (1049):  0.064641364\n",
      "Test loss at epoch (1049):  0.036347132\n",
      "Train loss at epoch (1050):  0.06469105\n",
      "Test loss at epoch (1050):  0.036270767\n",
      "Train loss at epoch (1051):  0.064633235\n",
      "Test loss at epoch (1051):  0.036277037\n",
      "Train loss at epoch (1052):  0.06477801\n",
      "Test loss at epoch (1052):  0.036314454\n",
      "Train loss at epoch (1053):  0.06467275\n",
      "Test loss at epoch (1053):  0.03625787\n",
      "Train loss at epoch (1054):  0.06460518\n",
      "Test loss at epoch (1054):  0.036316015\n",
      "Train loss at epoch (1055):  0.0646337\n",
      "Test loss at epoch (1055):  0.036283147\n",
      "Train loss at epoch (1056):  0.06452134\n",
      "Test loss at epoch (1056):  0.0362498\n",
      "Train loss at epoch (1057):  0.06435509\n",
      "Test loss at epoch (1057):  0.03623953\n",
      "Train loss at epoch (1058):  0.06462375\n",
      "Test loss at epoch (1058):  0.03618151\n",
      "Train loss at epoch (1059):  0.06425435\n",
      "Test loss at epoch (1059):  0.036257476\n",
      "Train loss at epoch (1060):  0.064613275\n",
      "Test loss at epoch (1060):  0.036210943\n",
      "Train loss at epoch (1061):  0.06424961\n",
      "Test loss at epoch (1061):  0.036212943\n",
      "Train loss at epoch (1062):  0.06470499\n",
      "Test loss at epoch (1062):  0.036219947\n",
      "Train loss at epoch (1063):  0.064404055\n",
      "Test loss at epoch (1063):  0.036255628\n",
      "Train loss at epoch (1064):  0.06429057\n",
      "Test loss at epoch (1064):  0.036249656\n",
      "Train loss at epoch (1065):  0.06441877\n",
      "Test loss at epoch (1065):  0.036185503\n",
      "Train loss at epoch (1066):  0.06439502\n",
      "Test loss at epoch (1066):  0.03623001\n",
      "Train loss at epoch (1067):  0.064868264\n",
      "Test loss at epoch (1067):  0.036251534\n",
      "Train loss at epoch (1068):  0.064459845\n",
      "Test loss at epoch (1068):  0.036261827\n",
      "Train loss at epoch (1069):  0.06465054\n",
      "Test loss at epoch (1069):  0.036213003\n",
      "Train loss at epoch (1070):  0.06453894\n",
      "Test loss at epoch (1070):  0.036181364\n",
      "Train loss at epoch (1071):  0.06431985\n",
      "Test loss at epoch (1071):  0.036166858\n",
      "Train loss at epoch (1072):  0.06424542\n",
      "Test loss at epoch (1072):  0.036203805\n",
      "Train loss at epoch (1073):  0.06464219\n",
      "Test loss at epoch (1073):  0.03618329\n",
      "Train loss at epoch (1074):  0.06469481\n",
      "Test loss at epoch (1074):  0.036176458\n",
      "Train loss at epoch (1075):  0.06437447\n",
      "Test loss at epoch (1075):  0.036178052\n",
      "Train loss at epoch (1076):  0.06465419\n",
      "Test loss at epoch (1076):  0.03617487\n",
      "Train loss at epoch (1077):  0.06477908\n",
      "Test loss at epoch (1077):  0.036185533\n",
      "Train loss at epoch (1078):  0.0641212\n",
      "Test loss at epoch (1078):  0.036192805\n",
      "Train loss at epoch (1079):  0.064607084\n",
      "Test loss at epoch (1079):  0.036183976\n",
      "Train loss at epoch (1080):  0.0644852\n",
      "Test loss at epoch (1080):  0.036148693\n",
      "Train loss at epoch (1081):  0.06449254\n",
      "Test loss at epoch (1081):  0.036182035\n",
      "Train loss at epoch (1082):  0.06436489\n",
      "Test loss at epoch (1082):  0.03620418\n",
      "Train loss at epoch (1083):  0.0644435\n",
      "Test loss at epoch (1083):  0.036143303\n",
      "Train loss at epoch (1084):  0.06436441\n",
      "Test loss at epoch (1084):  0.036145937\n",
      "Train loss at epoch (1085):  0.064545095\n",
      "Test loss at epoch (1085):  0.036153153\n",
      "Train loss at epoch (1086):  0.06429835\n",
      "Test loss at epoch (1086):  0.036119293\n",
      "Train loss at epoch (1087):  0.064733095\n",
      "Test loss at epoch (1087):  0.0361552\n",
      "Train loss at epoch (1088):  0.0641968\n",
      "Test loss at epoch (1088):  0.036179937\n",
      "Train loss at epoch (1089):  0.06457062\n",
      "Test loss at epoch (1089):  0.036139645\n",
      "Train loss at epoch (1090):  0.0645744\n",
      "Test loss at epoch (1090):  0.036179524\n",
      "Train loss at epoch (1091):  0.064438745\n",
      "Test loss at epoch (1091):  0.036134176\n",
      "Train loss at epoch (1092):  0.06454231\n",
      "Test loss at epoch (1092):  0.03614925\n",
      "Train loss at epoch (1093):  0.06518223\n",
      "Test loss at epoch (1093):  0.036136582\n",
      "Train loss at epoch (1094):  0.064526476\n",
      "Test loss at epoch (1094):  0.036137253\n",
      "Train loss at epoch (1095):  0.06448886\n",
      "Test loss at epoch (1095):  0.036150303\n",
      "Train loss at epoch (1096):  0.064587735\n",
      "Test loss at epoch (1096):  0.03612406\n",
      "Train loss at epoch (1097):  0.06419109\n",
      "Test loss at epoch (1097):  0.036098577\n",
      "Train loss at epoch (1098):  0.064496934\n",
      "Test loss at epoch (1098):  0.036137186\n",
      "Train loss at epoch (1099):  0.064372584\n",
      "Test loss at epoch (1099):  0.036140636\n",
      "Train loss at epoch (1100):  0.06443155\n",
      "Test loss at epoch (1100):  0.03615655\n",
      "Train loss at epoch (1101):  0.064496495\n",
      "Test loss at epoch (1101):  0.03611629\n",
      "Train loss at epoch (1102):  0.064168066\n",
      "Test loss at epoch (1102):  0.03611503\n",
      "Train loss at epoch (1103):  0.06460053\n",
      "Test loss at epoch (1103):  0.03608291\n",
      "Train loss at epoch (1104):  0.06452991\n",
      "Test loss at epoch (1104):  0.036100868\n",
      "Train loss at epoch (1105):  0.06427646\n",
      "Test loss at epoch (1105):  0.036057107\n",
      "Train loss at epoch (1106):  0.06418059\n",
      "Test loss at epoch (1106):  0.03604646\n",
      "Train loss at epoch (1107):  0.064530104\n",
      "Test loss at epoch (1107):  0.03608973\n",
      "Train loss at epoch (1108):  0.06411122\n",
      "Test loss at epoch (1108):  0.03606817\n",
      "Train loss at epoch (1109):  0.06444228\n",
      "Test loss at epoch (1109):  0.036091328\n",
      "Train loss at epoch (1110):  0.064447775\n",
      "Test loss at epoch (1110):  0.036041986\n",
      "Train loss at epoch (1111):  0.064681314\n",
      "Test loss at epoch (1111):  0.036084868\n",
      "Train loss at epoch (1112):  0.064315304\n",
      "Test loss at epoch (1112):  0.0360581\n",
      "Train loss at epoch (1113):  0.064025596\n",
      "Test loss at epoch (1113):  0.03608459\n",
      "Train loss at epoch (1114):  0.06465433\n",
      "Test loss at epoch (1114):  0.03608434\n",
      "Train loss at epoch (1115):  0.064390436\n",
      "Test loss at epoch (1115):  0.03606182\n",
      "Train loss at epoch (1116):  0.06408741\n",
      "Test loss at epoch (1116):  0.03607623\n",
      "Train loss at epoch (1117):  0.0644044\n",
      "Test loss at epoch (1117):  0.035995565\n",
      "Train loss at epoch (1118):  0.0640862\n",
      "Test loss at epoch (1118):  0.036027443\n",
      "Train loss at epoch (1119):  0.06396973\n",
      "Test loss at epoch (1119):  0.036042713\n",
      "Train loss at epoch (1120):  0.064423874\n",
      "Test loss at epoch (1120):  0.03606539\n",
      "Train loss at epoch (1121):  0.06412346\n",
      "Test loss at epoch (1121):  0.036129396\n",
      "Train loss at epoch (1122):  0.06431597\n",
      "Test loss at epoch (1122):  0.036053665\n",
      "Train loss at epoch (1123):  0.064145595\n",
      "Test loss at epoch (1123):  0.03610344\n",
      "Train loss at epoch (1124):  0.0644068\n",
      "Test loss at epoch (1124):  0.036117684\n",
      "Train loss at epoch (1125):  0.06414969\n",
      "Test loss at epoch (1125):  0.036120795\n",
      "Train loss at epoch (1126):  0.06397654\n",
      "Test loss at epoch (1126):  0.036052037\n",
      "Train loss at epoch (1127):  0.06435854\n",
      "Test loss at epoch (1127):  0.036086768\n",
      "Train loss at epoch (1128):  0.06441608\n",
      "Test loss at epoch (1128):  0.03605099\n",
      "Train loss at epoch (1129):  0.06423323\n",
      "Test loss at epoch (1129):  0.03605531\n",
      "Train loss at epoch (1130):  0.064424194\n",
      "Test loss at epoch (1130):  0.036066644\n",
      "Train loss at epoch (1131):  0.064077996\n",
      "Test loss at epoch (1131):  0.03605257\n",
      "Train loss at epoch (1132):  0.064405024\n",
      "Test loss at epoch (1132):  0.036074802\n",
      "Train loss at epoch (1133):  0.06426786\n",
      "Test loss at epoch (1133):  0.036075164\n",
      "Train loss at epoch (1134):  0.06438987\n",
      "Test loss at epoch (1134):  0.036024377\n",
      "Train loss at epoch (1135):  0.06433522\n",
      "Test loss at epoch (1135):  0.03604734\n",
      "Train loss at epoch (1136):  0.064457536\n",
      "Test loss at epoch (1136):  0.03604066\n",
      "Train loss at epoch (1137):  0.064198814\n",
      "Test loss at epoch (1137):  0.036055952\n",
      "Train loss at epoch (1138):  0.064422846\n",
      "Test loss at epoch (1138):  0.036043197\n",
      "Train loss at epoch (1139):  0.06419357\n",
      "Test loss at epoch (1139):  0.036011267\n",
      "Train loss at epoch (1140):  0.06438041\n",
      "Test loss at epoch (1140):  0.03598858\n",
      "Train loss at epoch (1141):  0.064212784\n",
      "Test loss at epoch (1141):  0.036001783\n",
      "Train loss at epoch (1142):  0.064118154\n",
      "Test loss at epoch (1142):  0.036052816\n",
      "Train loss at epoch (1143):  0.06443737\n",
      "Test loss at epoch (1143):  0.036029886\n",
      "Train loss at epoch (1144):  0.0642048\n",
      "Test loss at epoch (1144):  0.035992414\n",
      "Train loss at epoch (1145):  0.06427691\n",
      "Test loss at epoch (1145):  0.036003787\n",
      "Train loss at epoch (1146):  0.064044066\n",
      "Test loss at epoch (1146):  0.036026597\n",
      "Train loss at epoch (1147):  0.06422018\n",
      "Test loss at epoch (1147):  0.03598974\n",
      "Train loss at epoch (1148):  0.06444425\n",
      "Test loss at epoch (1148):  0.036000572\n",
      "Train loss at epoch (1149):  0.06392728\n",
      "Test loss at epoch (1149):  0.036002513\n",
      "Train loss at epoch (1150):  0.06450254\n",
      "Test loss at epoch (1150):  0.035966475\n",
      "Train loss at epoch (1151):  0.06440428\n",
      "Test loss at epoch (1151):  0.03599021\n",
      "Train loss at epoch (1152):  0.06434324\n",
      "Test loss at epoch (1152):  0.03599741\n",
      "Train loss at epoch (1153):  0.064458966\n",
      "Test loss at epoch (1153):  0.035998795\n",
      "Train loss at epoch (1154):  0.06435704\n",
      "Test loss at epoch (1154):  0.036015652\n",
      "Train loss at epoch (1155):  0.06396292\n",
      "Test loss at epoch (1155):  0.03600069\n",
      "Train loss at epoch (1156):  0.06425425\n",
      "Test loss at epoch (1156):  0.03598409\n",
      "Train loss at epoch (1157):  0.064072415\n",
      "Test loss at epoch (1157):  0.03596132\n",
      "Train loss at epoch (1158):  0.06417661\n",
      "Test loss at epoch (1158):  0.035947494\n",
      "Train loss at epoch (1159):  0.06437074\n",
      "Test loss at epoch (1159):  0.035966244\n",
      "Train loss at epoch (1160):  0.06434785\n",
      "Test loss at epoch (1160):  0.035940524\n",
      "Train loss at epoch (1161):  0.06406378\n",
      "Test loss at epoch (1161):  0.035963327\n",
      "Train loss at epoch (1162):  0.064267404\n",
      "Test loss at epoch (1162):  0.03594332\n",
      "Train loss at epoch (1163):  0.06405126\n",
      "Test loss at epoch (1163):  0.035952\n",
      "Train loss at epoch (1164):  0.06414472\n",
      "Test loss at epoch (1164):  0.036007375\n",
      "Train loss at epoch (1165):  0.06437767\n",
      "Test loss at epoch (1165):  0.035960436\n",
      "Train loss at epoch (1166):  0.06401183\n",
      "Test loss at epoch (1166):  0.03596277\n",
      "Train loss at epoch (1167):  0.06430629\n",
      "Test loss at epoch (1167):  0.035951387\n",
      "Train loss at epoch (1168):  0.06430269\n",
      "Test loss at epoch (1168):  0.035942886\n",
      "Train loss at epoch (1169):  0.06427648\n",
      "Test loss at epoch (1169):  0.035956234\n",
      "Train loss at epoch (1170):  0.06400184\n",
      "Test loss at epoch (1170):  0.035923894\n",
      "Train loss at epoch (1171):  0.06440905\n",
      "Test loss at epoch (1171):  0.035937827\n",
      "Train loss at epoch (1172):  0.06395693\n",
      "Test loss at epoch (1172):  0.035923053\n",
      "Train loss at epoch (1173):  0.06436786\n",
      "Test loss at epoch (1173):  0.035900444\n",
      "Train loss at epoch (1174):  0.06399173\n",
      "Test loss at epoch (1174):  0.035910808\n",
      "Train loss at epoch (1175):  0.06429193\n",
      "Test loss at epoch (1175):  0.035935525\n",
      "Train loss at epoch (1176):  0.06371424\n",
      "Test loss at epoch (1176):  0.03596293\n",
      "Train loss at epoch (1177):  0.06421779\n",
      "Test loss at epoch (1177):  0.035945687\n",
      "Train loss at epoch (1178):  0.06411457\n",
      "Test loss at epoch (1178):  0.035902087\n",
      "Train loss at epoch (1179):  0.0643014\n",
      "Test loss at epoch (1179):  0.035918795\n",
      "Train loss at epoch (1180):  0.064022824\n",
      "Test loss at epoch (1180):  0.035887185\n",
      "Train loss at epoch (1181):  0.06422089\n",
      "Test loss at epoch (1181):  0.035917196\n",
      "Train loss at epoch (1182):  0.06425419\n",
      "Test loss at epoch (1182):  0.035886984\n",
      "Train loss at epoch (1183):  0.06406509\n",
      "Test loss at epoch (1183):  0.03588657\n",
      "Train loss at epoch (1184):  0.06432574\n",
      "Test loss at epoch (1184):  0.03594448\n",
      "Train loss at epoch (1185):  0.063782096\n",
      "Test loss at epoch (1185):  0.035922356\n",
      "Train loss at epoch (1186):  0.064132966\n",
      "Test loss at epoch (1186):  0.035900164\n",
      "Train loss at epoch (1187):  0.063821375\n",
      "Test loss at epoch (1187):  0.03593161\n",
      "Train loss at epoch (1188):  0.06415092\n",
      "Test loss at epoch (1188):  0.035915125\n",
      "Train loss at epoch (1189):  0.06419055\n",
      "Test loss at epoch (1189):  0.035913307\n",
      "Train loss at epoch (1190):  0.06432637\n",
      "Test loss at epoch (1190):  0.035909444\n",
      "Train loss at epoch (1191):  0.06384175\n",
      "Test loss at epoch (1191):  0.035907194\n",
      "Train loss at epoch (1192):  0.06442943\n",
      "Test loss at epoch (1192):  0.035910074\n",
      "Train loss at epoch (1193):  0.064192556\n",
      "Test loss at epoch (1193):  0.035920013\n",
      "Train loss at epoch (1194):  0.06425309\n",
      "Test loss at epoch (1194):  0.035928275\n",
      "Train loss at epoch (1195):  0.06392458\n",
      "Test loss at epoch (1195):  0.03589058\n",
      "Train loss at epoch (1196):  0.064252265\n",
      "Test loss at epoch (1196):  0.0359019\n",
      "Train loss at epoch (1197):  0.06424708\n",
      "Test loss at epoch (1197):  0.0358931\n",
      "Train loss at epoch (1198):  0.064289205\n",
      "Test loss at epoch (1198):  0.035906855\n",
      "Train loss at epoch (1199):  0.06429198\n",
      "Test loss at epoch (1199):  0.03589378\n",
      "Train loss at epoch (1200):  0.064218126\n",
      "Test loss at epoch (1200):  0.035914913\n",
      "Train loss at epoch (1201):  0.06417007\n",
      "Test loss at epoch (1201):  0.03592133\n",
      "Train loss at epoch (1202):  0.0638104\n",
      "Test loss at epoch (1202):  0.035892677\n",
      "Train loss at epoch (1203):  0.06427091\n",
      "Test loss at epoch (1203):  0.035898697\n",
      "Train loss at epoch (1204):  0.06428072\n",
      "Test loss at epoch (1204):  0.035921905\n",
      "Train loss at epoch (1205):  0.06390955\n",
      "Test loss at epoch (1205):  0.035878766\n",
      "Train loss at epoch (1206):  0.06398967\n",
      "Test loss at epoch (1206):  0.035958875\n",
      "Train loss at epoch (1207):  0.06386074\n",
      "Test loss at epoch (1207):  0.035879605\n",
      "Train loss at epoch (1208):  0.064184226\n",
      "Test loss at epoch (1208):  0.03590688\n",
      "Train loss at epoch (1209):  0.06392178\n",
      "Test loss at epoch (1209):  0.0359051\n",
      "Train loss at epoch (1210):  0.06394716\n",
      "Test loss at epoch (1210):  0.035896875\n",
      "Train loss at epoch (1211):  0.06396583\n",
      "Test loss at epoch (1211):  0.03588467\n",
      "Train loss at epoch (1212):  0.064323924\n",
      "Test loss at epoch (1212):  0.035879474\n",
      "Train loss at epoch (1213):  0.06384995\n",
      "Test loss at epoch (1213):  0.035905126\n",
      "Train loss at epoch (1214):  0.06410399\n",
      "Test loss at epoch (1214):  0.035895716\n",
      "Train loss at epoch (1215):  0.06419195\n",
      "Test loss at epoch (1215):  0.03586567\n",
      "Train loss at epoch (1216):  0.064157456\n",
      "Test loss at epoch (1216):  0.035944045\n",
      "Train loss at epoch (1217):  0.06412414\n",
      "Test loss at epoch (1217):  0.0358796\n",
      "Train loss at epoch (1218):  0.06406123\n",
      "Test loss at epoch (1218):  0.035895195\n",
      "Train loss at epoch (1219):  0.06379936\n",
      "Test loss at epoch (1219):  0.035870243\n",
      "Train loss at epoch (1220):  0.06419761\n",
      "Test loss at epoch (1220):  0.03588887\n",
      "Train loss at epoch (1221):  0.06388377\n",
      "Test loss at epoch (1221):  0.03586367\n",
      "Train loss at epoch (1222):  0.064092495\n",
      "Test loss at epoch (1222):  0.035882358\n",
      "Train loss at epoch (1223):  0.06431266\n",
      "Test loss at epoch (1223):  0.035871908\n",
      "Train loss at epoch (1224):  0.06405754\n",
      "Test loss at epoch (1224):  0.035895377\n",
      "Train loss at epoch (1225):  0.06413327\n",
      "Test loss at epoch (1225):  0.03584944\n",
      "Train loss at epoch (1226):  0.06408945\n",
      "Test loss at epoch (1226):  0.035863146\n",
      "Train loss at epoch (1227):  0.0642979\n",
      "Test loss at epoch (1227):  0.03589464\n",
      "Train loss at epoch (1228):  0.06389399\n",
      "Test loss at epoch (1228):  0.035926133\n",
      "Train loss at epoch (1229):  0.06428664\n",
      "Test loss at epoch (1229):  0.03590208\n",
      "Train loss at epoch (1230):  0.06398532\n",
      "Test loss at epoch (1230):  0.035902787\n",
      "Train loss at epoch (1231):  0.064118005\n",
      "Test loss at epoch (1231):  0.035879925\n",
      "Train loss at epoch (1232):  0.064215854\n",
      "Test loss at epoch (1232):  0.035882097\n",
      "Train loss at epoch (1233):  0.064099304\n",
      "Test loss at epoch (1233):  0.03588191\n",
      "Train loss at epoch (1234):  0.064003736\n",
      "Test loss at epoch (1234):  0.035860166\n",
      "Train loss at epoch (1235):  0.064154\n",
      "Test loss at epoch (1235):  0.035887226\n",
      "Train loss at epoch (1236):  0.063967645\n",
      "Test loss at epoch (1236):  0.035895582\n",
      "Train loss at epoch (1237):  0.06426369\n",
      "Test loss at epoch (1237):  0.035872396\n",
      "Train loss at epoch (1238):  0.06389927\n",
      "Test loss at epoch (1238):  0.035832584\n",
      "Train loss at epoch (1239):  0.06418043\n",
      "Test loss at epoch (1239):  0.03582368\n",
      "Train loss at epoch (1240):  0.06397334\n",
      "Test loss at epoch (1240):  0.035844333\n",
      "Train loss at epoch (1241):  0.06384913\n",
      "Test loss at epoch (1241):  0.03585871\n",
      "Train loss at epoch (1242):  0.06390166\n",
      "Test loss at epoch (1242):  0.035881456\n",
      "Train loss at epoch (1243):  0.06376269\n",
      "Test loss at epoch (1243):  0.035878412\n",
      "Train loss at epoch (1244):  0.06400613\n",
      "Test loss at epoch (1244):  0.035886716\n",
      "Train loss at epoch (1245):  0.06389081\n",
      "Test loss at epoch (1245):  0.035871934\n",
      "Train loss at epoch (1246):  0.06404451\n",
      "Test loss at epoch (1246):  0.035868134\n",
      "Train loss at epoch (1247):  0.06396324\n",
      "Test loss at epoch (1247):  0.035865713\n",
      "Train loss at epoch (1248):  0.064138606\n",
      "Test loss at epoch (1248):  0.035830423\n",
      "Train loss at epoch (1249):  0.06416397\n",
      "Test loss at epoch (1249):  0.0358619\n",
      "Train loss at epoch (1250):  0.063804604\n",
      "Test loss at epoch (1250):  0.03585797\n",
      "Train loss at epoch (1251):  0.06426003\n",
      "Test loss at epoch (1251):  0.035828248\n",
      "Train loss at epoch (1252):  0.06386192\n",
      "Test loss at epoch (1252):  0.035841547\n",
      "Train loss at epoch (1253):  0.06411388\n",
      "Test loss at epoch (1253):  0.035851754\n",
      "Train loss at epoch (1254):  0.06408381\n",
      "Test loss at epoch (1254):  0.03587009\n",
      "Train loss at epoch (1255):  0.0641566\n",
      "Test loss at epoch (1255):  0.03583033\n",
      "Train loss at epoch (1256):  0.06405117\n",
      "Test loss at epoch (1256):  0.035845328\n",
      "Train loss at epoch (1257):  0.06396896\n",
      "Test loss at epoch (1257):  0.035857245\n",
      "Train loss at epoch (1258):  0.063995175\n",
      "Test loss at epoch (1258):  0.035862505\n",
      "Train loss at epoch (1259):  0.06453705\n",
      "Test loss at epoch (1259):  0.03585309\n",
      "Train loss at epoch (1260):  0.06397794\n",
      "Test loss at epoch (1260):  0.03588147\n",
      "Train loss at epoch (1261):  0.0638096\n",
      "Test loss at epoch (1261):  0.035811156\n",
      "Train loss at epoch (1262):  0.06391627\n",
      "Test loss at epoch (1262):  0.035814375\n",
      "Train loss at epoch (1263):  0.06376153\n",
      "Test loss at epoch (1263):  0.035838436\n",
      "Train loss at epoch (1264):  0.06417856\n",
      "Test loss at epoch (1264):  0.035849106\n",
      "Train loss at epoch (1265):  0.06399434\n",
      "Test loss at epoch (1265):  0.035845138\n",
      "Train loss at epoch (1266):  0.06399786\n",
      "Test loss at epoch (1266):  0.035852198\n",
      "Train loss at epoch (1267):  0.06391818\n",
      "Test loss at epoch (1267):  0.03580819\n",
      "Train loss at epoch (1268):  0.06380775\n",
      "Test loss at epoch (1268):  0.03584692\n",
      "Train loss at epoch (1269):  0.06407575\n",
      "Test loss at epoch (1269):  0.03583165\n",
      "Train loss at epoch (1270):  0.06415327\n",
      "Test loss at epoch (1270):  0.035818502\n",
      "Train loss at epoch (1271):  0.06408075\n",
      "Test loss at epoch (1271):  0.035865538\n",
      "Train loss at epoch (1272):  0.06388702\n",
      "Test loss at epoch (1272):  0.035846088\n",
      "Train loss at epoch (1273):  0.06421662\n",
      "Test loss at epoch (1273):  0.03581943\n",
      "Train loss at epoch (1274):  0.06388723\n",
      "Test loss at epoch (1274):  0.035868794\n",
      "Train loss at epoch (1275):  0.06405962\n",
      "Test loss at epoch (1275):  0.035841733\n",
      "Train loss at epoch (1276):  0.064287476\n",
      "Test loss at epoch (1276):  0.035863712\n",
      "Train loss at epoch (1277):  0.06420018\n",
      "Test loss at epoch (1277):  0.035822816\n",
      "Train loss at epoch (1278):  0.06421191\n",
      "Test loss at epoch (1278):  0.035822704\n",
      "Train loss at epoch (1279):  0.06419751\n",
      "Test loss at epoch (1279):  0.03585711\n",
      "Train loss at epoch (1280):  0.06440903\n",
      "Test loss at epoch (1280):  0.035781275\n",
      "Train loss at epoch (1281):  0.06379295\n",
      "Test loss at epoch (1281):  0.035839427\n",
      "Train loss at epoch (1282):  0.06408134\n",
      "Test loss at epoch (1282):  0.03579783\n",
      "Train loss at epoch (1283):  0.06379288\n",
      "Test loss at epoch (1283):  0.035802342\n",
      "Train loss at epoch (1284):  0.06419814\n",
      "Test loss at epoch (1284):  0.03582628\n",
      "Train loss at epoch (1285):  0.06403006\n",
      "Test loss at epoch (1285):  0.03581677\n",
      "Train loss at epoch (1286):  0.06395653\n",
      "Test loss at epoch (1286):  0.035830148\n",
      "Train loss at epoch (1287):  0.0639717\n",
      "Test loss at epoch (1287):  0.035804275\n",
      "Train loss at epoch (1288):  0.06406791\n",
      "Test loss at epoch (1288):  0.035837606\n",
      "Train loss at epoch (1289):  0.06407636\n",
      "Test loss at epoch (1289):  0.035828024\n",
      "Train loss at epoch (1290):  0.06417113\n",
      "Test loss at epoch (1290):  0.035848804\n",
      "Train loss at epoch (1291):  0.06419977\n",
      "Test loss at epoch (1291):  0.03583484\n",
      "Train loss at epoch (1292):  0.064000264\n",
      "Test loss at epoch (1292):  0.035838507\n",
      "Train loss at epoch (1293):  0.06410506\n",
      "Test loss at epoch (1293):  0.03581316\n",
      "Train loss at epoch (1294):  0.06384032\n",
      "Test loss at epoch (1294):  0.035803065\n",
      "Train loss at epoch (1295):  0.06385683\n",
      "Test loss at epoch (1295):  0.035771534\n",
      "Train loss at epoch (1296):  0.06378439\n",
      "Test loss at epoch (1296):  0.035799965\n",
      "Train loss at epoch (1297):  0.06400269\n",
      "Test loss at epoch (1297):  0.03583185\n",
      "Train loss at epoch (1298):  0.06405147\n",
      "Test loss at epoch (1298):  0.035800733\n",
      "Train loss at epoch (1299):  0.06383604\n",
      "Test loss at epoch (1299):  0.03586088\n",
      "Train loss at epoch (1300):  0.06417186\n",
      "Test loss at epoch (1300):  0.03580542\n",
      "Train loss at epoch (1301):  0.06388727\n",
      "Test loss at epoch (1301):  0.03580283\n",
      "Train loss at epoch (1302):  0.06369945\n",
      "Test loss at epoch (1302):  0.035811145\n",
      "Train loss at epoch (1303):  0.06389378\n",
      "Test loss at epoch (1303):  0.035811763\n",
      "Train loss at epoch (1304):  0.06376048\n",
      "Test loss at epoch (1304):  0.035845708\n",
      "Train loss at epoch (1305):  0.06387731\n",
      "Test loss at epoch (1305):  0.035836563\n",
      "Train loss at epoch (1306):  0.06383815\n",
      "Test loss at epoch (1306):  0.035821382\n",
      "Train loss at epoch (1307):  0.064024776\n",
      "Test loss at epoch (1307):  0.0358163\n",
      "Train loss at epoch (1308):  0.06408507\n",
      "Test loss at epoch (1308):  0.035806548\n",
      "Train loss at epoch (1309):  0.06422594\n",
      "Test loss at epoch (1309):  0.03579073\n",
      "Train loss at epoch (1310):  0.063852295\n",
      "Test loss at epoch (1310):  0.035806786\n",
      "Train loss at epoch (1311):  0.06396337\n",
      "Test loss at epoch (1311):  0.035789408\n",
      "Train loss at epoch (1312):  0.06380644\n",
      "Test loss at epoch (1312):  0.035791837\n",
      "Train loss at epoch (1313):  0.063641004\n",
      "Test loss at epoch (1313):  0.035757158\n",
      "Train loss at epoch (1314):  0.06401925\n",
      "Test loss at epoch (1314):  0.035796233\n",
      "Train loss at epoch (1315):  0.063984044\n",
      "Test loss at epoch (1315):  0.035813514\n",
      "Train loss at epoch (1316):  0.06381153\n",
      "Test loss at epoch (1316):  0.035811428\n",
      "Train loss at epoch (1317):  0.06372109\n",
      "Test loss at epoch (1317):  0.035812348\n",
      "Train loss at epoch (1318):  0.06398316\n",
      "Test loss at epoch (1318):  0.035797566\n",
      "Train loss at epoch (1319):  0.063937366\n",
      "Test loss at epoch (1319):  0.035777513\n",
      "Train loss at epoch (1320):  0.06392293\n",
      "Test loss at epoch (1320):  0.035761356\n",
      "Train loss at epoch (1321):  0.064030305\n",
      "Test loss at epoch (1321):  0.035783965\n",
      "Train loss at epoch (1322):  0.06401649\n",
      "Test loss at epoch (1322):  0.035808593\n",
      "Train loss at epoch (1323):  0.0638068\n",
      "Test loss at epoch (1323):  0.03578521\n",
      "Train loss at epoch (1324):  0.06386461\n",
      "Test loss at epoch (1324):  0.035754073\n",
      "Train loss at epoch (1325):  0.06402218\n",
      "Test loss at epoch (1325):  0.035783354\n",
      "Train loss at epoch (1326):  0.063892744\n",
      "Test loss at epoch (1326):  0.035788924\n",
      "Train loss at epoch (1327):  0.064365104\n",
      "Test loss at epoch (1327):  0.035783276\n",
      "Train loss at epoch (1328):  0.06358544\n",
      "Test loss at epoch (1328):  0.035782002\n",
      "Train loss at epoch (1329):  0.063505605\n",
      "Test loss at epoch (1329):  0.035785682\n",
      "Train loss at epoch (1330):  0.06403898\n",
      "Test loss at epoch (1330):  0.035763297\n",
      "Train loss at epoch (1331):  0.063974336\n",
      "Test loss at epoch (1331):  0.035775464\n",
      "Train loss at epoch (1332):  0.06430468\n",
      "Test loss at epoch (1332):  0.0357952\n",
      "Train loss at epoch (1333):  0.06377858\n",
      "Test loss at epoch (1333):  0.03579171\n",
      "Train loss at epoch (1334):  0.063771956\n",
      "Test loss at epoch (1334):  0.035778955\n",
      "Train loss at epoch (1335):  0.063947916\n",
      "Test loss at epoch (1335):  0.03576194\n",
      "Train loss at epoch (1336):  0.06429889\n",
      "Test loss at epoch (1336):  0.035779003\n",
      "Train loss at epoch (1337):  0.06370222\n",
      "Test loss at epoch (1337):  0.035763174\n",
      "Train loss at epoch (1338):  0.06394999\n",
      "Test loss at epoch (1338):  0.035733894\n",
      "Train loss at epoch (1339):  0.06399725\n",
      "Test loss at epoch (1339):  0.0357924\n",
      "Train loss at epoch (1340):  0.064019054\n",
      "Test loss at epoch (1340):  0.03578452\n",
      "Train loss at epoch (1341):  0.06426709\n",
      "Test loss at epoch (1341):  0.035771057\n",
      "Train loss at epoch (1342):  0.0640214\n",
      "Test loss at epoch (1342):  0.0357662\n",
      "Train loss at epoch (1343):  0.06414856\n",
      "Test loss at epoch (1343):  0.035764117\n",
      "Train loss at epoch (1344):  0.06409741\n",
      "Test loss at epoch (1344):  0.035778318\n",
      "Train loss at epoch (1345):  0.06359245\n",
      "Test loss at epoch (1345):  0.035757713\n",
      "Train loss at epoch (1346):  0.06368658\n",
      "Test loss at epoch (1346):  0.03577597\n",
      "Train loss at epoch (1347):  0.06397682\n",
      "Test loss at epoch (1347):  0.03575075\n",
      "Train loss at epoch (1348):  0.06416623\n",
      "Test loss at epoch (1348):  0.035750993\n",
      "Train loss at epoch (1349):  0.06424653\n",
      "Test loss at epoch (1349):  0.035777636\n",
      "Train loss at epoch (1350):  0.064167395\n",
      "Test loss at epoch (1350):  0.035766553\n",
      "Train loss at epoch (1351):  0.06369482\n",
      "Test loss at epoch (1351):  0.035756975\n",
      "Train loss at epoch (1352):  0.06374892\n",
      "Test loss at epoch (1352):  0.035782382\n",
      "Train loss at epoch (1353):  0.064248994\n",
      "Test loss at epoch (1353):  0.035760336\n",
      "Train loss at epoch (1354):  0.06377101\n",
      "Test loss at epoch (1354):  0.03576684\n",
      "Train loss at epoch (1355):  0.06408425\n",
      "Test loss at epoch (1355):  0.03574561\n",
      "Train loss at epoch (1356):  0.06398566\n",
      "Test loss at epoch (1356):  0.035811853\n",
      "Train loss at epoch (1357):  0.064242445\n",
      "Test loss at epoch (1357):  0.03576217\n",
      "Train loss at epoch (1358):  0.06387125\n",
      "Test loss at epoch (1358):  0.035776626\n",
      "Train loss at epoch (1359):  0.06385805\n",
      "Test loss at epoch (1359):  0.035781395\n",
      "Train loss at epoch (1360):  0.06362033\n",
      "Test loss at epoch (1360):  0.03577851\n",
      "Train loss at epoch (1361):  0.064033754\n",
      "Test loss at epoch (1361):  0.035771456\n",
      "Train loss at epoch (1362):  0.06406004\n",
      "Test loss at epoch (1362):  0.03573514\n",
      "Train loss at epoch (1363):  0.06430641\n",
      "Test loss at epoch (1363):  0.035788022\n",
      "Train loss at epoch (1364):  0.06376647\n",
      "Test loss at epoch (1364):  0.035758123\n",
      "Train loss at epoch (1365):  0.06423502\n",
      "Test loss at epoch (1365):  0.03575926\n",
      "Train loss at epoch (1366):  0.06398781\n",
      "Test loss at epoch (1366):  0.03577446\n",
      "Train loss at epoch (1367):  0.06383003\n",
      "Test loss at epoch (1367):  0.035784025\n",
      "Train loss at epoch (1368):  0.063840136\n",
      "Test loss at epoch (1368):  0.035821408\n",
      "Train loss at epoch (1369):  0.06390157\n",
      "Test loss at epoch (1369):  0.03576383\n",
      "Train loss at epoch (1370):  0.06393487\n",
      "Test loss at epoch (1370):  0.03575269\n",
      "Train loss at epoch (1371):  0.063976176\n",
      "Test loss at epoch (1371):  0.035788428\n",
      "Train loss at epoch (1372):  0.06367234\n",
      "Test loss at epoch (1372):  0.03578102\n",
      "Train loss at epoch (1373):  0.063965864\n",
      "Test loss at epoch (1373):  0.035771284\n",
      "Train loss at epoch (1374):  0.06410923\n",
      "Test loss at epoch (1374):  0.03579936\n",
      "Train loss at epoch (1375):  0.06396433\n",
      "Test loss at epoch (1375):  0.035777546\n",
      "Train loss at epoch (1376):  0.06397671\n",
      "Test loss at epoch (1376):  0.035781693\n",
      "Train loss at epoch (1377):  0.06412394\n",
      "Test loss at epoch (1377):  0.03577638\n",
      "Train loss at epoch (1378):  0.063868605\n",
      "Test loss at epoch (1378):  0.035775285\n",
      "Train loss at epoch (1379):  0.064111255\n",
      "Test loss at epoch (1379):  0.035763197\n",
      "Train loss at epoch (1380):  0.06426752\n",
      "Test loss at epoch (1380):  0.03577991\n",
      "Train loss at epoch (1381):  0.06359382\n",
      "Test loss at epoch (1381):  0.035799567\n",
      "Train loss at epoch (1382):  0.063814506\n",
      "Test loss at epoch (1382):  0.03577166\n",
      "Train loss at epoch (1383):  0.06388644\n",
      "Test loss at epoch (1383):  0.035775892\n",
      "Train loss at epoch (1384):  0.06420287\n",
      "Test loss at epoch (1384):  0.03580745\n",
      "Train loss at epoch (1385):  0.06419319\n",
      "Test loss at epoch (1385):  0.035756793\n",
      "Train loss at epoch (1386):  0.06424563\n",
      "Test loss at epoch (1386):  0.035779383\n",
      "Train loss at epoch (1387):  0.06386603\n",
      "Test loss at epoch (1387):  0.03575126\n",
      "Train loss at epoch (1388):  0.0636907\n",
      "Test loss at epoch (1388):  0.035774793\n",
      "Train loss at epoch (1389):  0.063856095\n",
      "Test loss at epoch (1389):  0.03575401\n",
      "Train loss at epoch (1390):  0.063759886\n",
      "Test loss at epoch (1390):  0.03575658\n",
      "Train loss at epoch (1391):  0.06415518\n",
      "Test loss at epoch (1391):  0.03575504\n",
      "Train loss at epoch (1392):  0.063611224\n",
      "Test loss at epoch (1392):  0.03578494\n",
      "Train loss at epoch (1393):  0.06397998\n",
      "Test loss at epoch (1393):  0.03578493\n",
      "Train loss at epoch (1394):  0.06390688\n",
      "Test loss at epoch (1394):  0.035788525\n",
      "Train loss at epoch (1395):  0.064061806\n",
      "Test loss at epoch (1395):  0.035767525\n",
      "Train loss at epoch (1396):  0.063929655\n",
      "Test loss at epoch (1396):  0.035772145\n",
      "Train loss at epoch (1397):  0.06411715\n",
      "Test loss at epoch (1397):  0.035772085\n",
      "Train loss at epoch (1398):  0.06378251\n",
      "Test loss at epoch (1398):  0.03575585\n",
      "Train loss at epoch (1399):  0.06385387\n",
      "Test loss at epoch (1399):  0.03575408\n",
      "Train loss at epoch (1400):  0.064232215\n",
      "Test loss at epoch (1400):  0.035750765\n",
      "Train loss at epoch (1401):  0.063865475\n",
      "Test loss at epoch (1401):  0.03578661\n",
      "Train loss at epoch (1402):  0.06384132\n",
      "Test loss at epoch (1402):  0.035756815\n",
      "Train loss at epoch (1403):  0.064136885\n",
      "Test loss at epoch (1403):  0.03578785\n",
      "Train loss at epoch (1404):  0.06392212\n",
      "Test loss at epoch (1404):  0.035775583\n",
      "Train loss at epoch (1405):  0.063862205\n",
      "Test loss at epoch (1405):  0.03578303\n",
      "Train loss at epoch (1406):  0.06373261\n",
      "Test loss at epoch (1406):  0.035764612\n",
      "Train loss at epoch (1407):  0.06397011\n",
      "Test loss at epoch (1407):  0.03576849\n",
      "Train loss at epoch (1408):  0.0637396\n",
      "Test loss at epoch (1408):  0.035783246\n",
      "Train loss at epoch (1409):  0.06398119\n",
      "Test loss at epoch (1409):  0.03577541\n",
      "Train loss at epoch (1410):  0.06407611\n",
      "Test loss at epoch (1410):  0.035750963\n",
      "Train loss at epoch (1411):  0.06407178\n",
      "Test loss at epoch (1411):  0.035774894\n",
      "Train loss at epoch (1412):  0.063933715\n",
      "Test loss at epoch (1412):  0.03577976\n",
      "Train loss at epoch (1413):  0.06357589\n",
      "Test loss at epoch (1413):  0.035761796\n",
      "Train loss at epoch (1414):  0.06386593\n",
      "Test loss at epoch (1414):  0.03579308\n",
      "Train loss at epoch (1415):  0.06435938\n",
      "Test loss at epoch (1415):  0.03578273\n",
      "Train loss at epoch (1416):  0.063595414\n",
      "Test loss at epoch (1416):  0.035777073\n",
      "Train loss at epoch (1417):  0.06395878\n",
      "Test loss at epoch (1417):  0.035788447\n",
      "Train loss at epoch (1418):  0.06410853\n",
      "Test loss at epoch (1418):  0.035757214\n",
      "Train loss at epoch (1419):  0.063832715\n",
      "Test loss at epoch (1419):  0.03577205\n",
      "Train loss at epoch (1420):  0.06369798\n",
      "Test loss at epoch (1420):  0.035791464\n",
      "Train loss at epoch (1421):  0.064101554\n",
      "Test loss at epoch (1421):  0.03577178\n",
      "Train loss at epoch (1422):  0.06394095\n",
      "Test loss at epoch (1422):  0.03575867\n",
      "Train loss at epoch (1423):  0.063877515\n",
      "Test loss at epoch (1423):  0.035778683\n",
      "Train loss at epoch (1424):  0.0638376\n",
      "Test loss at epoch (1424):  0.03576792\n",
      "Train loss at epoch (1425):  0.06390803\n",
      "Test loss at epoch (1425):  0.035763998\n",
      "Train loss at epoch (1426):  0.063916035\n",
      "Test loss at epoch (1426):  0.03578434\n",
      "Train loss at epoch (1427):  0.063848324\n",
      "Test loss at epoch (1427):  0.035760295\n",
      "Train loss at epoch (1428):  0.0638595\n",
      "Test loss at epoch (1428):  0.035759598\n",
      "Train loss at epoch (1429):  0.06428337\n",
      "Test loss at epoch (1429):  0.035774685\n",
      "Train loss at epoch (1430):  0.063866384\n",
      "Test loss at epoch (1430):  0.03577758\n",
      "Train loss at epoch (1431):  0.06379936\n",
      "Test loss at epoch (1431):  0.035766847\n",
      "Train loss at epoch (1432):  0.06388433\n",
      "Test loss at epoch (1432):  0.035752106\n",
      "Train loss at epoch (1433):  0.063961394\n",
      "Test loss at epoch (1433):  0.035779305\n",
      "Train loss at epoch (1434):  0.064177774\n",
      "Test loss at epoch (1434):  0.03576201\n",
      "Train loss at epoch (1435):  0.063941255\n",
      "Test loss at epoch (1435):  0.035730876\n",
      "Train loss at epoch (1436):  0.06373577\n",
      "Test loss at epoch (1436):  0.03575035\n",
      "Train loss at epoch (1437):  0.0634201\n",
      "Test loss at epoch (1437):  0.03579973\n",
      "Train loss at epoch (1438):  0.06402467\n",
      "Test loss at epoch (1438):  0.035737783\n",
      "Train loss at epoch (1439):  0.06394094\n",
      "Test loss at epoch (1439):  0.035752762\n",
      "Train loss at epoch (1440):  0.06391949\n",
      "Test loss at epoch (1440):  0.035763763\n",
      "Train loss at epoch (1441):  0.064164974\n",
      "Test loss at epoch (1441):  0.03576814\n",
      "Train loss at epoch (1442):  0.06398998\n",
      "Test loss at epoch (1442):  0.035773147\n",
      "Train loss at epoch (1443):  0.06377394\n",
      "Test loss at epoch (1443):  0.03575095\n",
      "Train loss at epoch (1444):  0.063592725\n",
      "Test loss at epoch (1444):  0.035773095\n",
      "Train loss at epoch (1445):  0.06378576\n",
      "Test loss at epoch (1445):  0.035779133\n",
      "Train loss at epoch (1446):  0.06399971\n",
      "Test loss at epoch (1446):  0.03575089\n",
      "Train loss at epoch (1447):  0.06376063\n",
      "Test loss at epoch (1447):  0.03578505\n",
      "Train loss at epoch (1448):  0.06407435\n",
      "Test loss at epoch (1448):  0.03577101\n",
      "Train loss at epoch (1449):  0.0638201\n",
      "Test loss at epoch (1449):  0.035741355\n",
      "Train loss at epoch (1450):  0.0639173\n",
      "Test loss at epoch (1450):  0.035748094\n",
      "Train loss at epoch (1451):  0.063688785\n",
      "Test loss at epoch (1451):  0.035757955\n",
      "Train loss at epoch (1452):  0.063901424\n",
      "Test loss at epoch (1452):  0.035752404\n",
      "Train loss at epoch (1453):  0.06411234\n",
      "Test loss at epoch (1453):  0.035760183\n",
      "Train loss at epoch (1454):  0.0638353\n",
      "Test loss at epoch (1454):  0.03577585\n",
      "Train loss at epoch (1455):  0.06404852\n",
      "Test loss at epoch (1455):  0.035739183\n",
      "Train loss at epoch (1456):  0.06386336\n",
      "Test loss at epoch (1456):  0.035787653\n",
      "Train loss at epoch (1457):  0.06387839\n",
      "Test loss at epoch (1457):  0.03576637\n",
      "Train loss at epoch (1458):  0.063763626\n",
      "Test loss at epoch (1458):  0.035740327\n",
      "Train loss at epoch (1459):  0.063678354\n",
      "Test loss at epoch (1459):  0.035768587\n",
      "Train loss at epoch (1460):  0.06333384\n",
      "Test loss at epoch (1460):  0.03573572\n",
      "Train loss at epoch (1461):  0.06396931\n",
      "Test loss at epoch (1461):  0.03576312\n",
      "Train loss at epoch (1462):  0.064028494\n",
      "Test loss at epoch (1462):  0.03576991\n",
      "Train loss at epoch (1463):  0.064101204\n",
      "Test loss at epoch (1463):  0.03578605\n",
      "Train loss at epoch (1464):  0.06383036\n",
      "Test loss at epoch (1464):  0.035768952\n",
      "Train loss at epoch (1465):  0.06383321\n",
      "Test loss at epoch (1465):  0.03576322\n",
      "Train loss at epoch (1466):  0.06385257\n",
      "Test loss at epoch (1466):  0.03573908\n",
      "Train loss at epoch (1467):  0.06365802\n",
      "Test loss at epoch (1467):  0.03573782\n",
      "Train loss at epoch (1468):  0.06425131\n",
      "Test loss at epoch (1468):  0.035740882\n",
      "Train loss at epoch (1469):  0.06409829\n",
      "Test loss at epoch (1469):  0.035759766\n",
      "Train loss at epoch (1470):  0.063681394\n",
      "Test loss at epoch (1470):  0.035777763\n",
      "Train loss at epoch (1471):  0.063578606\n",
      "Test loss at epoch (1471):  0.03577215\n",
      "Train loss at epoch (1472):  0.06430112\n",
      "Test loss at epoch (1472):  0.035770316\n",
      "Train loss at epoch (1473):  0.063677855\n",
      "Test loss at epoch (1473):  0.035746668\n",
      "Train loss at epoch (1474):  0.06365795\n",
      "Test loss at epoch (1474):  0.03577186\n",
      "Train loss at epoch (1475):  0.06417901\n",
      "Test loss at epoch (1475):  0.0357677\n",
      "Train loss at epoch (1476):  0.06393721\n",
      "Test loss at epoch (1476):  0.035769552\n",
      "Train loss at epoch (1477):  0.06380776\n",
      "Test loss at epoch (1477):  0.03577445\n",
      "Train loss at epoch (1478):  0.06383882\n",
      "Test loss at epoch (1478):  0.03578602\n",
      "Train loss at epoch (1479):  0.06404834\n",
      "Test loss at epoch (1479):  0.035745203\n",
      "Train loss at epoch (1480):  0.0637891\n",
      "Test loss at epoch (1480):  0.03575964\n",
      "Train loss at epoch (1481):  0.06382006\n",
      "Test loss at epoch (1481):  0.03577149\n",
      "Train loss at epoch (1482):  0.06379462\n",
      "Test loss at epoch (1482):  0.0357541\n",
      "Train loss at epoch (1483):  0.06369424\n",
      "Test loss at epoch (1483):  0.035731424\n",
      "Train loss at epoch (1484):  0.063535854\n",
      "Test loss at epoch (1484):  0.035759877\n",
      "Train loss at epoch (1485):  0.06367248\n",
      "Test loss at epoch (1485):  0.03577866\n",
      "Train loss at epoch (1486):  0.063927926\n",
      "Test loss at epoch (1486):  0.03575572\n",
      "Train loss at epoch (1487):  0.06386077\n",
      "Test loss at epoch (1487):  0.03576979\n",
      "Train loss at epoch (1488):  0.06382442\n",
      "Test loss at epoch (1488):  0.035754308\n",
      "Train loss at epoch (1489):  0.064083084\n",
      "Test loss at epoch (1489):  0.035767376\n",
      "Train loss at epoch (1490):  0.06379507\n",
      "Test loss at epoch (1490):  0.03578355\n",
      "Train loss at epoch (1491):  0.06416107\n",
      "Test loss at epoch (1491):  0.035773624\n",
      "Train loss at epoch (1492):  0.06412438\n",
      "Test loss at epoch (1492):  0.035744276\n",
      "Train loss at epoch (1493):  0.06376633\n",
      "Test loss at epoch (1493):  0.03577758\n",
      "Train loss at epoch (1494):  0.063731894\n",
      "Test loss at epoch (1494):  0.035757404\n",
      "Train loss at epoch (1495):  0.063716725\n",
      "Test loss at epoch (1495):  0.035757612\n",
      "Train loss at epoch (1496):  0.06363715\n",
      "Test loss at epoch (1496):  0.03574344\n",
      "Train loss at epoch (1497):  0.06403554\n",
      "Test loss at epoch (1497):  0.0357575\n",
      "Train loss at epoch (1498):  0.06391033\n",
      "Test loss at epoch (1498):  0.03577123\n",
      "Train loss at epoch (1499):  0.06396235\n",
      "Test loss at epoch (1499):  0.03574749\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "EPOCHS = 1500\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "cached_decoder = CachedTransformerDecoder(cfg)\n",
    "optimizer = SGD(cached_decoder.parameters(), lr=1e-4, momentum=0.9)\n",
    "lr_scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "trainer = Trainer(\n",
    "    train_data_loader=train_data_loader,\n",
    "    test_data_loader=test_data_loader,\n",
    "    # optimizer=AdamW(decoder.parameters(), lr=1e-5),\n",
    "    optimizer=optimizer,\n",
    "    model=cached_decoder,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    epochs=EPOCHS\n",
    ")\n",
    "\n",
    "trainer.train(loss_fn=nn.L1Loss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss at epoch (0):  0.6432881\n",
      "Test loss at epoch (0):  0.49729246\n",
      "Train loss at epoch (1):  0.4719696\n",
      "Test loss at epoch (1):  0.40295056\n",
      "Train loss at epoch (2):  0.4011773\n",
      "Test loss at epoch (2):  0.34844404\n",
      "Train loss at epoch (3):  0.3635843\n",
      "Test loss at epoch (3):  0.31810597\n",
      "Train loss at epoch (4):  0.34107456\n",
      "Test loss at epoch (4):  0.29560176\n",
      "Train loss at epoch (5):  0.32872155\n",
      "Test loss at epoch (5):  0.2838705\n",
      "Train loss at epoch (6):  0.31828487\n",
      "Test loss at epoch (6):  0.27757052\n",
      "Train loss at epoch (7):  0.3100376\n",
      "Test loss at epoch (7):  0.26726827\n",
      "Train loss at epoch (8):  0.30267662\n",
      "Test loss at epoch (8):  0.26167277\n",
      "Train loss at epoch (9):  0.29836196\n",
      "Test loss at epoch (9):  0.254898\n",
      "Train loss at epoch (10):  0.2927416\n",
      "Test loss at epoch (10):  0.249565\n",
      "Train loss at epoch (11):  0.28723517\n",
      "Test loss at epoch (11):  0.24387653\n",
      "Train loss at epoch (12):  0.28262806\n",
      "Test loss at epoch (12):  0.23978396\n",
      "Train loss at epoch (13):  0.27912968\n",
      "Test loss at epoch (13):  0.23437163\n",
      "Train loss at epoch (14):  0.2745747\n",
      "Test loss at epoch (14):  0.23251416\n",
      "Train loss at epoch (15):  0.27083436\n",
      "Test loss at epoch (15):  0.22513987\n",
      "Train loss at epoch (16):  0.26550895\n",
      "Test loss at epoch (16):  0.22334604\n",
      "Train loss at epoch (17):  0.26135123\n",
      "Test loss at epoch (17):  0.21966113\n",
      "Train loss at epoch (18):  0.25799757\n",
      "Test loss at epoch (18):  0.2145367\n",
      "Train loss at epoch (19):  0.2544191\n",
      "Test loss at epoch (19):  0.21025948\n",
      "Train loss at epoch (20):  0.25037235\n",
      "Test loss at epoch (20):  0.20762001\n",
      "Train loss at epoch (21):  0.24671759\n",
      "Test loss at epoch (21):  0.20334618\n",
      "Train loss at epoch (22):  0.24436788\n",
      "Test loss at epoch (22):  0.19947308\n",
      "Train loss at epoch (23):  0.24046463\n",
      "Test loss at epoch (23):  0.19590104\n",
      "Train loss at epoch (24):  0.23689388\n",
      "Test loss at epoch (24):  0.1927251\n",
      "Train loss at epoch (25):  0.23404618\n",
      "Test loss at epoch (25):  0.18894085\n",
      "Train loss at epoch (26):  0.23031196\n",
      "Test loss at epoch (26):  0.18614079\n",
      "Train loss at epoch (27):  0.22837809\n",
      "Test loss at epoch (27):  0.18058686\n",
      "Train loss at epoch (28):  0.2233064\n",
      "Test loss at epoch (28):  0.17790464\n",
      "Train loss at epoch (29):  0.22339635\n",
      "Test loss at epoch (29):  0.17470522\n",
      "Train loss at epoch (30):  0.21898727\n",
      "Test loss at epoch (30):  0.17092626\n",
      "Train loss at epoch (31):  0.21641189\n",
      "Test loss at epoch (31):  0.16767058\n",
      "Train loss at epoch (32):  0.2142858\n",
      "Test loss at epoch (32):  0.16327342\n",
      "Train loss at epoch (33):  0.21067475\n",
      "Test loss at epoch (33):  0.16004479\n",
      "Train loss at epoch (34):  0.20772967\n",
      "Test loss at epoch (34):  0.15656061\n",
      "Train loss at epoch (35):  0.20522378\n",
      "Test loss at epoch (35):  0.15339944\n",
      "Train loss at epoch (36):  0.20376238\n",
      "Test loss at epoch (36):  0.14962412\n",
      "Train loss at epoch (37):  0.19920786\n",
      "Test loss at epoch (37):  0.14617369\n",
      "Train loss at epoch (38):  0.19770245\n",
      "Test loss at epoch (38):  0.14418063\n",
      "Train loss at epoch (39):  0.19602077\n",
      "Test loss at epoch (39):  0.1409365\n",
      "Train loss at epoch (40):  0.19374475\n",
      "Test loss at epoch (40):  0.13767456\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "EPOCHS = 750\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "cached_decoder = Transformer(cfg)\n",
    "optimizer = SGD(cached_decoder.parameters(), lr=1e-4, momentum=0.9)\n",
    "lr_scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "trainer = Trainer(\n",
    "    train_data_loader=train_data_loader,\n",
    "    test_data_loader=test_data_loader,\n",
    "    # optimizer=AdamW(decoder.parameters(), lr=1e-5),\n",
    "    optimizer=optimizer,\n",
    "    model=cached_decoder,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    epochs=EPOCHS\n",
    ")\n",
    "\n",
    "trainer.train(loss_fn=nn.L1Loss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, n_tokens, inputs):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_len = inputs.size(-2)\n",
    "        for i in range(n_tokens):\n",
    "            inputs = torch.cat((inputs, model.predict(inputs)), dim=-2)\n",
    "        return inputs[:, input_len:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Transformer' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m x_sample, y_sample \u001b[38;5;241m=\u001b[39m test_dataset[np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, TEST_SIZE)]\n\u001b[0;32m----> 2\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mcached_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m(x_sample\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)[:, :SOURCE_SEQ_LEN, :], TARGET_SEQ_LEN)\n\u001b[1;32m      3\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((x_sample, y_sample[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:]))\n\u001b[1;32m      5\u001b[0m plot_circle(\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mlabels\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m:],\n\u001b[1;32m      7\u001b[0m     prediction\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcat((labels[\u001b[38;5;241m0\u001b[39m:SOURCE_SEQ_LEN, :], predictions[\u001b[38;5;241m0\u001b[39m]))\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m:],\n\u001b[1;32m      8\u001b[0m )\n",
      "File \u001b[0;32m~/mosquitto/ml/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1933\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Transformer' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "x_sample, y_sample = test_dataset[np.random.randint(0, TEST_SIZE)]\n",
    "predictions = cached_decoder.predict(x_sample.unsqueeze(0)[:, :SOURCE_SEQ_LEN, :], TARGET_SEQ_LEN)\n",
    "labels = torch.cat((x_sample, y_sample[-1:]))\n",
    "\n",
    "plot_circle(\n",
    "    input=labels.detach().numpy()[0:],\n",
    "    prediction=torch.cat((labels[0:SOURCE_SEQ_LEN, :], predictions[0])).detach().numpy()[0:],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'decoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m x_sample, y_sample \u001b[38;5;241m=\u001b[39m test_dataset[np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, TEST_SIZE)]\n\u001b[0;32m----> 2\u001b[0m predictions \u001b[38;5;241m=\u001b[39m predict(\u001b[43mdecoder\u001b[49m, TARGET_SEQ_LEN, x_sample\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)[:, :SOURCE_SEQ_LEN, :])\n\u001b[1;32m      3\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((x_sample, y_sample[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:]))\n\u001b[1;32m      5\u001b[0m plot_circle(\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mlabels\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m:],\n\u001b[1;32m      7\u001b[0m     prediction\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcat((labels[\u001b[38;5;241m0\u001b[39m:SOURCE_SEQ_LEN, :], predictions[\u001b[38;5;241m0\u001b[39m]))\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m:],\n\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'decoder' is not defined"
     ]
    }
   ],
   "source": [
    "x_sample, y_sample = test_dataset[np.random.randint(0, TEST_SIZE)]\n",
    "predictions = predict(decoder, TARGET_SEQ_LEN, x_sample.unsqueeze(0)[:, :SOURCE_SEQ_LEN, :])\n",
    "labels = torch.cat((x_sample, y_sample[-1:]))\n",
    "\n",
    "plot_circle(\n",
    "    input=labels.detach().numpy()[0:],\n",
    "    prediction=torch.cat((labels[0:SOURCE_SEQ_LEN, :], predictions[0])).detach().numpy()[0:],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_sample, y_sample = test_dataset[np.random.randint(0, TEST_SIZE)]\n",
    "predictions = cached_decoder(x_sample.unsqueeze(0)[:, :SOURCE_SEQ_LEN, :])\n",
    "labels = torch.cat((x_sample, y_sample[-1:]))\n",
    "\n",
    "plot_circle(\n",
    "    input=labels.detach().numpy()[0:],\n",
    "    prediction=torch.cat((labels[0:SOURCE_SEQ_LEN, :], predictions[0])).detach().numpy()[0:],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached:  0.0032138898537959902 0.001621722942688389\n",
      "Cacheless:  0.003723135533509776 0.0011225037988922212\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "cached_time = []\n",
    "with torch.no_grad():\n",
    "    for i in range(1000):\n",
    "        s = time.perf_counter()\n",
    "        x_sample, y_sample = test_dataset[np.random.randint(0, TEST_SIZE)]\n",
    "        predictions = cached_decoder.predict(x_sample.unsqueeze(0)[:, :SOURCE_SEQ_LEN, :], TARGET_SEQ_LEN)\n",
    "        cached_time.append(time.perf_counter() - s)\n",
    "\n",
    "cacheless_time = []\n",
    "for i in range(1000):\n",
    "    s = time.perf_counter()\n",
    "    predictions = predict(decoder, TARGET_SEQ_LEN, x_sample.unsqueeze(0)[:, :SOURCE_SEQ_LEN, :])\n",
    "    labels = torch.cat((x_sample, y_sample[-1:]))\n",
    "    cacheless_time.append(time.perf_counter() - s)\n",
    "\n",
    "print(\"Cached: \", np.array(cached_time).mean(), np.array(cached_time).std())\n",
    "print(\"Cacheless: \", np.array(cacheless_time).mean(), np.array(cacheless_time).std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
