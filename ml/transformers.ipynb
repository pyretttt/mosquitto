{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW, Optimizer, SGD\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from scratchers.attn import Attention, MultiheadAttention\n",
    "from scratchers.transformer import TransformerBlock, TransformerDecoder, Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_SEQ_LEN = 12\n",
    "TARGET_SEQ_LEN = 12\n",
    "\n",
    "DATA_SIZE = 1500\n",
    "TRAIN_SIZE = int(DATA_SIZE * 0.8)\n",
    "TEST_SIZE = DATA_SIZE - TRAIN_SIZE\n",
    "\n",
    "# radii = np.random.ranf((DATA_SIZE)) # * 9 + 1\n",
    "radii = np.ones((DATA_SIZE))\n",
    "starting_radian = np.random.ranf((DATA_SIZE)) * 2 * np.pi\n",
    "directions = np.random.randint(2, size=DATA_SIZE) * 2 - 1\n",
    "\n",
    "def to_cartesian(\n",
    "    radius: float,\n",
    "    start_radian: float, \n",
    "    direction: int\n",
    "):\n",
    "    delta = 2 * np.pi / (SOURCE_SEQ_LEN + TARGET_SEQ_LEN)\n",
    "    seq = np.array([\n",
    "        np.array([\n",
    "            radius * np.cos(start_radian + (i * direction * delta)),\n",
    "            radius * np.sin(start_radian + (i * direction * delta))\n",
    "        ])\n",
    "        for i in range(SOURCE_SEQ_LEN + TARGET_SEQ_LEN)\n",
    "    ])\n",
    "    source_seq = seq[:-1]\n",
    "    target_seq = seq[1:]\n",
    "    return source_seq, target_seq\n",
    "\n",
    "def make_circles_data():\n",
    "    X = np.empty((DATA_SIZE, (SOURCE_SEQ_LEN + TARGET_SEQ_LEN) - 1, 2))\n",
    "    Y = np.empty((DATA_SIZE, (SOURCE_SEQ_LEN + TARGET_SEQ_LEN) - 1, 2))\n",
    "    for idx, (radius, start_radian, direction) in enumerate(zip(radii, starting_radian, directions)):\n",
    "        x, y = to_cartesian(radius, start_radian, direction)\n",
    "        X[idx, :, :] = x\n",
    "        Y[idx, :, :] = y\n",
    "\n",
    "    return (\n",
    "        torch.from_numpy(X[:TRAIN_SIZE]).float(), \n",
    "        torch.from_numpy(Y[:TRAIN_SIZE]).float(),\n",
    "        torch.from_numpy(X[TRAIN_SIZE:]).float(), \n",
    "        torch.from_numpy(Y[TRAIN_SIZE:]).float()\n",
    "    )\n",
    "\n",
    "def make_squares_data():\n",
    "    X = np.empty((DATA_SIZE, (2 + 2) - 1, 2))\n",
    "    Y = np.empty((DATA_SIZE, (2 + 2) - 1, 2))\n",
    "\n",
    "    def map(elements):\n",
    "        res = []\n",
    "        for element in elements:\n",
    "            if element == 0:\n",
    "                res.append(np.array([-1, -1]))\n",
    "            elif element == 1:\n",
    "                res.append(np.array([1, -1]))\n",
    "            elif element == 2:\n",
    "                res.append(np.array([1, 1]))\n",
    "            elif element == 3:\n",
    "                res.append(np.array([-1, 1]))\n",
    "\n",
    "        return np.array(res)\n",
    "\n",
    "    first = np.random.randint(0, 4, size=(DATA_SIZE))\n",
    "    second = (first + 1) % 4\n",
    "    third = (second + 1) % 4\n",
    "    fourth = (third + 1) % 4\n",
    "    first = map(first)\n",
    "    second = map(second)\n",
    "    third = map(third)\n",
    "    fourth = map(fourth)\n",
    "\n",
    "    X[:, 0, :] = first\n",
    "    X[:, 1, :] = second\n",
    "    X[:, 2, :] = third\n",
    "    Y[:, 0, :] = second\n",
    "    Y[:, 1, :] = third\n",
    "    Y[:, 2, :] = fourth\n",
    "\n",
    "    return (\n",
    "        torch.from_numpy(X[:TRAIN_SIZE]).float(), \n",
    "        torch.from_numpy(Y[:TRAIN_SIZE]).float(),\n",
    "        torch.from_numpy(X[TRAIN_SIZE:]).float(), \n",
    "        torch.from_numpy(Y[TRAIN_SIZE:]).float()\n",
    "    )\n",
    "\n",
    "\n",
    "X_train, y_train, X_test, y_test = make_circles_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_circle(input, prediction, figsize=(6, 6)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    colors = cm.rainbow(np.linspace(0, 1, len(input)))\n",
    "    plt.scatter(\n",
    "        input[:, 0], input[:, 1], label=\"input\", marker=\"*\", s=250, alpha=0.5, color=colors\n",
    "    )\n",
    "    plt.scatter(prediction[:, 0], prediction[:, 1], label=\"prediction\", color=colors)\n",
    "    plt.legend()\n",
    "\n",
    "plot_circle(X_train[4], y_train[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config(\n",
    "    input_size=2,\n",
    "    attn_d_k=32,\n",
    "    transformer_proj_dim=64,\n",
    "    dropout=0.2,\n",
    "    nlayers=2,\n",
    "    is_self_attn=False,\n",
    "    max_seq_len=TARGET_SEQ_LEN + SOURCE_SEQ_LEN,\n",
    "    nheads=2,\n",
    "    pre_layer_norm=True\n",
    ")\n",
    "\n",
    "def attn_factory(config: Config):\n",
    "    return Attention(\n",
    "        d_k=config.attn_d_k,\n",
    "        # nheads=config.nheads,\n",
    "        is_self_attn=config.is_self_attn,\n",
    "        dropout=config.dropout\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_data_loader: DataLoader,\n",
    "        test_data_loader: DataLoader,\n",
    "        optimizer: Optimizer,\n",
    "        model: nn.Module,\n",
    "        lr_scheduler = None,\n",
    "        epochs: int = 10,\n",
    "    ):\n",
    "        self.train_data_loader = train_data_loader\n",
    "        self.test_data_loader = test_data_loader\n",
    "        self.optimizer = optimizer\n",
    "        self.model = model\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def train(self, loss_fn: callable):\n",
    "        for epoch in range(self.epochs):\n",
    "            losses = []\n",
    "            self.model.train()\n",
    "            for x_batch, y_batch in self.train_data_loader:\n",
    "                out = self.model(x_batch)\n",
    "                loss = loss_fn(out, y_batch)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                losses.append(loss.detach().cpu().numpy())\n",
    "\n",
    "            if self.lr_scheduler is not None:\n",
    "                self.lr_scheduler.step()\n",
    "\n",
    "            print(f\"Train loss at epoch ({epoch}): \", np.array(losses).mean())\n",
    "\n",
    "            with torch.no_grad():\n",
    "                self.model.eval()\n",
    "                test_losses = []\n",
    "                for x_batch, y_batch in self.test_data_loader:\n",
    "                    out = self.model(x_batch)\n",
    "                    loss = loss_fn(out, y_batch)\n",
    "                    test_losses.append(loss.detach().cpu().numpy())\n",
    "\n",
    "                print(f\"Test loss at epoch ({epoch}): \", np.array(test_losses).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss at epoch (0):  0.39269736\n",
      "Test loss at epoch (0):  0.20490548\n",
      "Train loss at epoch (1):  0.20300595\n",
      "Test loss at epoch (1):  0.14398511\n",
      "Train loss at epoch (2):  0.16842805\n",
      "Test loss at epoch (2):  0.12536462\n",
      "Train loss at epoch (3):  0.15229183\n",
      "Test loss at epoch (3):  0.11491196\n",
      "Train loss at epoch (4):  0.14213149\n",
      "Test loss at epoch (4):  0.10617054\n",
      "Train loss at epoch (5):  0.1328051\n",
      "Test loss at epoch (5):  0.09836425\n",
      "Train loss at epoch (6):  0.12587073\n",
      "Test loss at epoch (6):  0.091587484\n",
      "Train loss at epoch (7):  0.12022067\n",
      "Test loss at epoch (7):  0.08575912\n",
      "Train loss at epoch (8):  0.11506311\n",
      "Test loss at epoch (8):  0.07992052\n",
      "Train loss at epoch (9):  0.11018028\n",
      "Test loss at epoch (9):  0.07485926\n",
      "Train loss at epoch (10):  0.10636981\n",
      "Test loss at epoch (10):  0.07035898\n",
      "Train loss at epoch (11):  0.10263248\n",
      "Test loss at epoch (11):  0.06823638\n",
      "Train loss at epoch (12):  0.100072995\n",
      "Test loss at epoch (12):  0.06382145\n",
      "Train loss at epoch (13):  0.09809626\n",
      "Test loss at epoch (13):  0.061189733\n",
      "Train loss at epoch (14):  0.09548453\n",
      "Test loss at epoch (14):  0.06095569\n",
      "Train loss at epoch (15):  0.09363727\n",
      "Test loss at epoch (15):  0.058398318\n",
      "Train loss at epoch (16):  0.091819055\n",
      "Test loss at epoch (16):  0.055854272\n",
      "Train loss at epoch (17):  0.09022095\n",
      "Test loss at epoch (17):  0.053867523\n",
      "Train loss at epoch (18):  0.08886981\n",
      "Test loss at epoch (18):  0.053900205\n",
      "Train loss at epoch (19):  0.08772129\n",
      "Test loss at epoch (19):  0.05227397\n",
      "Train loss at epoch (20):  0.08648208\n",
      "Test loss at epoch (20):  0.051220786\n",
      "Train loss at epoch (21):  0.08612543\n",
      "Test loss at epoch (21):  0.050947353\n",
      "Train loss at epoch (22):  0.08468252\n",
      "Test loss at epoch (22):  0.050106954\n",
      "Train loss at epoch (23):  0.08336765\n",
      "Test loss at epoch (23):  0.04933436\n",
      "Train loss at epoch (24):  0.08272428\n",
      "Test loss at epoch (24):  0.04858525\n",
      "Train loss at epoch (25):  0.081758045\n",
      "Test loss at epoch (25):  0.048817743\n",
      "Train loss at epoch (26):  0.08115309\n",
      "Test loss at epoch (26):  0.047065984\n",
      "Train loss at epoch (27):  0.08060513\n",
      "Test loss at epoch (27):  0.04708171\n",
      "Train loss at epoch (28):  0.07961082\n",
      "Test loss at epoch (28):  0.045913115\n",
      "Train loss at epoch (29):  0.0786804\n",
      "Test loss at epoch (29):  0.04520747\n",
      "Train loss at epoch (30):  0.0787149\n",
      "Test loss at epoch (30):  0.045504004\n",
      "Train loss at epoch (31):  0.0777843\n",
      "Test loss at epoch (31):  0.045524653\n",
      "Train loss at epoch (32):  0.07722385\n",
      "Test loss at epoch (32):  0.045721777\n",
      "Train loss at epoch (33):  0.07655944\n",
      "Test loss at epoch (33):  0.044999424\n",
      "Train loss at epoch (34):  0.076445095\n",
      "Test loss at epoch (34):  0.04346303\n",
      "Train loss at epoch (35):  0.07533026\n",
      "Test loss at epoch (35):  0.043526612\n",
      "Train loss at epoch (36):  0.07528765\n",
      "Test loss at epoch (36):  0.042410232\n",
      "Train loss at epoch (37):  0.0745976\n",
      "Test loss at epoch (37):  0.04272191\n",
      "Train loss at epoch (38):  0.07421374\n",
      "Test loss at epoch (38):  0.042650808\n",
      "Train loss at epoch (39):  0.0737561\n",
      "Test loss at epoch (39):  0.04181842\n",
      "Train loss at epoch (40):  0.073156506\n",
      "Test loss at epoch (40):  0.042347074\n",
      "Train loss at epoch (41):  0.07297051\n",
      "Test loss at epoch (41):  0.041486736\n",
      "Train loss at epoch (42):  0.07256577\n",
      "Test loss at epoch (42):  0.041754216\n",
      "Train loss at epoch (43):  0.07214642\n",
      "Test loss at epoch (43):  0.04130584\n",
      "Train loss at epoch (44):  0.0717821\n",
      "Test loss at epoch (44):  0.04052248\n",
      "Train loss at epoch (45):  0.07161602\n",
      "Test loss at epoch (45):  0.04044895\n",
      "Train loss at epoch (46):  0.07101918\n",
      "Test loss at epoch (46):  0.040462133\n",
      "Train loss at epoch (47):  0.07075719\n",
      "Test loss at epoch (47):  0.040382117\n",
      "Train loss at epoch (48):  0.07069473\n",
      "Test loss at epoch (48):  0.0394107\n",
      "Train loss at epoch (49):  0.06993649\n",
      "Test loss at epoch (49):  0.039047226\n",
      "Train loss at epoch (50):  0.06982142\n",
      "Test loss at epoch (50):  0.039804507\n",
      "Train loss at epoch (51):  0.06968099\n",
      "Test loss at epoch (51):  0.038738716\n",
      "Train loss at epoch (52):  0.06910507\n",
      "Test loss at epoch (52):  0.038353838\n",
      "Train loss at epoch (53):  0.06891241\n",
      "Test loss at epoch (53):  0.038981367\n",
      "Train loss at epoch (54):  0.06869253\n",
      "Test loss at epoch (54):  0.038658995\n",
      "Train loss at epoch (55):  0.067871794\n",
      "Test loss at epoch (55):  0.039430108\n",
      "Train loss at epoch (56):  0.06795331\n",
      "Test loss at epoch (56):  0.039123323\n",
      "Train loss at epoch (57):  0.06807236\n",
      "Test loss at epoch (57):  0.03798137\n",
      "Train loss at epoch (58):  0.06755913\n",
      "Test loss at epoch (58):  0.037341356\n",
      "Train loss at epoch (59):  0.06684587\n",
      "Test loss at epoch (59):  0.03919154\n",
      "Train loss at epoch (60):  0.06685788\n",
      "Test loss at epoch (60):  0.037069637\n",
      "Train loss at epoch (61):  0.066854514\n",
      "Test loss at epoch (61):  0.038010467\n",
      "Train loss at epoch (62):  0.066850364\n",
      "Test loss at epoch (62):  0.0373626\n",
      "Train loss at epoch (63):  0.06590737\n",
      "Test loss at epoch (63):  0.037258737\n",
      "Train loss at epoch (64):  0.06660318\n",
      "Test loss at epoch (64):  0.037784703\n",
      "Train loss at epoch (65):  0.066101015\n",
      "Test loss at epoch (65):  0.0368883\n",
      "Train loss at epoch (66):  0.0655187\n",
      "Test loss at epoch (66):  0.035927486\n",
      "Train loss at epoch (67):  0.065125935\n",
      "Test loss at epoch (67):  0.036029443\n",
      "Train loss at epoch (68):  0.06546557\n",
      "Test loss at epoch (68):  0.036564153\n",
      "Train loss at epoch (69):  0.064972386\n",
      "Test loss at epoch (69):  0.036134694\n",
      "Train loss at epoch (70):  0.06496592\n",
      "Test loss at epoch (70):  0.036223188\n",
      "Train loss at epoch (71):  0.064912304\n",
      "Test loss at epoch (71):  0.036642086\n",
      "Train loss at epoch (72):  0.06478355\n",
      "Test loss at epoch (72):  0.03612838\n",
      "Train loss at epoch (73):  0.06415458\n",
      "Test loss at epoch (73):  0.035659593\n",
      "Train loss at epoch (74):  0.06381403\n",
      "Test loss at epoch (74):  0.03547557\n",
      "Train loss at epoch (75):  0.0640499\n",
      "Test loss at epoch (75):  0.035542358\n",
      "Train loss at epoch (76):  0.06370707\n",
      "Test loss at epoch (76):  0.03513038\n",
      "Train loss at epoch (77):  0.06330479\n",
      "Test loss at epoch (77):  0.034982692\n",
      "Train loss at epoch (78):  0.06356009\n",
      "Test loss at epoch (78):  0.03554912\n",
      "Train loss at epoch (79):  0.06301496\n",
      "Test loss at epoch (79):  0.034989387\n",
      "Train loss at epoch (80):  0.06312796\n",
      "Test loss at epoch (80):  0.03448122\n",
      "Train loss at epoch (81):  0.06330739\n",
      "Test loss at epoch (81):  0.03433956\n",
      "Train loss at epoch (82):  0.06269231\n",
      "Test loss at epoch (82):  0.034674913\n",
      "Train loss at epoch (83):  0.06257848\n",
      "Test loss at epoch (83):  0.03467008\n",
      "Train loss at epoch (84):  0.062370658\n",
      "Test loss at epoch (84):  0.034245268\n",
      "Train loss at epoch (85):  0.06235434\n",
      "Test loss at epoch (85):  0.034594987\n",
      "Train loss at epoch (86):  0.062067185\n",
      "Test loss at epoch (86):  0.034216028\n",
      "Train loss at epoch (87):  0.061878864\n",
      "Test loss at epoch (87):  0.034216058\n",
      "Train loss at epoch (88):  0.06194463\n",
      "Test loss at epoch (88):  0.034320388\n",
      "Train loss at epoch (89):  0.061698113\n",
      "Test loss at epoch (89):  0.033878602\n",
      "Train loss at epoch (90):  0.061379828\n",
      "Test loss at epoch (90):  0.03432603\n",
      "Train loss at epoch (91):  0.06166854\n",
      "Test loss at epoch (91):  0.034165997\n",
      "Train loss at epoch (92):  0.06162489\n",
      "Test loss at epoch (92):  0.03402121\n",
      "Train loss at epoch (93):  0.061112672\n",
      "Test loss at epoch (93):  0.033524424\n",
      "Train loss at epoch (94):  0.06085397\n",
      "Test loss at epoch (94):  0.03359678\n",
      "Train loss at epoch (95):  0.061165843\n",
      "Test loss at epoch (95):  0.033391997\n",
      "Train loss at epoch (96):  0.060530994\n",
      "Test loss at epoch (96):  0.033647608\n",
      "Train loss at epoch (97):  0.060741697\n",
      "Test loss at epoch (97):  0.033856075\n",
      "Train loss at epoch (98):  0.060260683\n",
      "Test loss at epoch (98):  0.032912593\n",
      "Train loss at epoch (99):  0.060240995\n",
      "Test loss at epoch (99):  0.032494884\n",
      "Train loss at epoch (100):  0.060201466\n",
      "Test loss at epoch (100):  0.033286955\n",
      "Train loss at epoch (101):  0.05994371\n",
      "Test loss at epoch (101):  0.032514654\n",
      "Train loss at epoch (102):  0.05983466\n",
      "Test loss at epoch (102):  0.033208806\n",
      "Train loss at epoch (103):  0.059862882\n",
      "Test loss at epoch (103):  0.0332024\n",
      "Train loss at epoch (104):  0.059720185\n",
      "Test loss at epoch (104):  0.0320164\n",
      "Train loss at epoch (105):  0.05977844\n",
      "Test loss at epoch (105):  0.032802794\n",
      "Train loss at epoch (106):  0.059278507\n",
      "Test loss at epoch (106):  0.03351076\n",
      "Train loss at epoch (107):  0.05937008\n",
      "Test loss at epoch (107):  0.032361418\n",
      "Train loss at epoch (108):  0.05925597\n",
      "Test loss at epoch (108):  0.03234264\n",
      "Train loss at epoch (109):  0.05932189\n",
      "Test loss at epoch (109):  0.03251222\n",
      "Train loss at epoch (110):  0.058851026\n",
      "Test loss at epoch (110):  0.03276894\n",
      "Train loss at epoch (111):  0.059170038\n",
      "Test loss at epoch (111):  0.03256426\n",
      "Train loss at epoch (112):  0.058798745\n",
      "Test loss at epoch (112):  0.032245073\n",
      "Train loss at epoch (113):  0.058729153\n",
      "Test loss at epoch (113):  0.032214366\n",
      "Train loss at epoch (114):  0.058723267\n",
      "Test loss at epoch (114):  0.032075856\n",
      "Train loss at epoch (115):  0.058794085\n",
      "Test loss at epoch (115):  0.031380318\n",
      "Train loss at epoch (116):  0.058665458\n",
      "Test loss at epoch (116):  0.03186415\n",
      "Train loss at epoch (117):  0.05781387\n",
      "Test loss at epoch (117):  0.03204056\n",
      "Train loss at epoch (118):  0.058198813\n",
      "Test loss at epoch (118):  0.03145827\n",
      "Train loss at epoch (119):  0.058420777\n",
      "Test loss at epoch (119):  0.0318152\n",
      "Train loss at epoch (120):  0.058344994\n",
      "Test loss at epoch (120):  0.031470846\n",
      "Train loss at epoch (121):  0.057968725\n",
      "Test loss at epoch (121):  0.03190695\n",
      "Train loss at epoch (122):  0.057602003\n",
      "Test loss at epoch (122):  0.031432685\n",
      "Train loss at epoch (123):  0.05758855\n",
      "Test loss at epoch (123):  0.031473015\n",
      "Train loss at epoch (124):  0.05760776\n",
      "Test loss at epoch (124):  0.031670015\n",
      "Train loss at epoch (125):  0.057783738\n",
      "Test loss at epoch (125):  0.031260155\n",
      "Train loss at epoch (126):  0.057401307\n",
      "Test loss at epoch (126):  0.0314859\n",
      "Train loss at epoch (127):  0.057569746\n",
      "Test loss at epoch (127):  0.03076766\n",
      "Train loss at epoch (128):  0.05736681\n",
      "Test loss at epoch (128):  0.031025754\n",
      "Train loss at epoch (129):  0.057328053\n",
      "Test loss at epoch (129):  0.03141273\n",
      "Train loss at epoch (130):  0.057323508\n",
      "Test loss at epoch (130):  0.031175151\n",
      "Train loss at epoch (131):  0.05705611\n",
      "Test loss at epoch (131):  0.0311275\n",
      "Train loss at epoch (132):  0.05687799\n",
      "Test loss at epoch (132):  0.03115733\n",
      "Train loss at epoch (133):  0.056948002\n",
      "Test loss at epoch (133):  0.0318135\n",
      "Train loss at epoch (134):  0.05649823\n",
      "Test loss at epoch (134):  0.03050954\n",
      "Train loss at epoch (135):  0.056920845\n",
      "Test loss at epoch (135):  0.030622566\n",
      "Train loss at epoch (136):  0.056461696\n",
      "Test loss at epoch (136):  0.031227667\n",
      "Train loss at epoch (137):  0.05668239\n",
      "Test loss at epoch (137):  0.031545658\n",
      "Train loss at epoch (138):  0.056628633\n",
      "Test loss at epoch (138):  0.031431228\n",
      "Train loss at epoch (139):  0.056632075\n",
      "Test loss at epoch (139):  0.02992232\n",
      "Train loss at epoch (140):  0.05617707\n",
      "Test loss at epoch (140):  0.03066713\n",
      "Train loss at epoch (141):  0.056336995\n",
      "Test loss at epoch (141):  0.030090181\n",
      "Train loss at epoch (142):  0.056042608\n",
      "Test loss at epoch (142):  0.031893883\n",
      "Train loss at epoch (143):  0.05632452\n",
      "Test loss at epoch (143):  0.029697381\n",
      "Train loss at epoch (144):  0.05599638\n",
      "Test loss at epoch (144):  0.031215731\n",
      "Train loss at epoch (145):  0.056095745\n",
      "Test loss at epoch (145):  0.0300887\n",
      "Train loss at epoch (146):  0.055699456\n",
      "Test loss at epoch (146):  0.030258069\n",
      "Train loss at epoch (147):  0.05556022\n",
      "Test loss at epoch (147):  0.02997376\n",
      "Train loss at epoch (148):  0.055699546\n",
      "Test loss at epoch (148):  0.029781973\n",
      "Train loss at epoch (149):  0.05572588\n",
      "Test loss at epoch (149):  0.030979112\n",
      "Train loss at epoch (150):  0.05555167\n",
      "Test loss at epoch (150):  0.030539867\n",
      "Train loss at epoch (151):  0.05501809\n",
      "Test loss at epoch (151):  0.031146746\n",
      "Train loss at epoch (152):  0.055673815\n",
      "Test loss at epoch (152):  0.030528147\n",
      "Train loss at epoch (153):  0.05530468\n",
      "Test loss at epoch (153):  0.029361038\n",
      "Train loss at epoch (154):  0.05510132\n",
      "Test loss at epoch (154):  0.029828375\n",
      "Train loss at epoch (155):  0.055071544\n",
      "Test loss at epoch (155):  0.029622229\n",
      "Train loss at epoch (156):  0.055142097\n",
      "Test loss at epoch (156):  0.029542083\n",
      "Train loss at epoch (157):  0.055004634\n",
      "Test loss at epoch (157):  0.030139653\n",
      "Train loss at epoch (158):  0.055045553\n",
      "Test loss at epoch (158):  0.029231567\n",
      "Train loss at epoch (159):  0.054984104\n",
      "Test loss at epoch (159):  0.029425379\n",
      "Train loss at epoch (160):  0.055017464\n",
      "Test loss at epoch (160):  0.02913534\n",
      "Train loss at epoch (161):  0.0551114\n",
      "Test loss at epoch (161):  0.029789373\n",
      "Train loss at epoch (162):  0.054855052\n",
      "Test loss at epoch (162):  0.02948325\n",
      "Train loss at epoch (163):  0.054796085\n",
      "Test loss at epoch (163):  0.029338896\n",
      "Train loss at epoch (164):  0.054604802\n",
      "Test loss at epoch (164):  0.029293716\n",
      "Train loss at epoch (165):  0.054415308\n",
      "Test loss at epoch (165):  0.029338928\n",
      "Train loss at epoch (166):  0.054600876\n",
      "Test loss at epoch (166):  0.029690934\n",
      "Train loss at epoch (167):  0.05439144\n",
      "Test loss at epoch (167):  0.029219514\n",
      "Train loss at epoch (168):  0.054222208\n",
      "Test loss at epoch (168):  0.029188344\n",
      "Train loss at epoch (169):  0.054305814\n",
      "Test loss at epoch (169):  0.029183852\n",
      "Train loss at epoch (170):  0.054342505\n",
      "Test loss at epoch (170):  0.029067429\n",
      "Train loss at epoch (171):  0.054253947\n",
      "Test loss at epoch (171):  0.029687634\n",
      "Train loss at epoch (172):  0.053936582\n",
      "Test loss at epoch (172):  0.029384837\n",
      "Train loss at epoch (173):  0.054081496\n",
      "Test loss at epoch (173):  0.02973352\n",
      "Train loss at epoch (174):  0.054156296\n",
      "Test loss at epoch (174):  0.028992336\n",
      "Train loss at epoch (175):  0.053609874\n",
      "Test loss at epoch (175):  0.028833728\n",
      "Train loss at epoch (176):  0.05381084\n",
      "Test loss at epoch (176):  0.028719934\n",
      "Train loss at epoch (177):  0.05368068\n",
      "Test loss at epoch (177):  0.029542565\n",
      "Train loss at epoch (178):  0.053641383\n",
      "Test loss at epoch (178):  0.028769735\n",
      "Train loss at epoch (179):  0.053646266\n",
      "Test loss at epoch (179):  0.029062932\n",
      "Train loss at epoch (180):  0.05360722\n",
      "Test loss at epoch (180):  0.028931832\n",
      "Train loss at epoch (181):  0.053382\n",
      "Test loss at epoch (181):  0.028843274\n",
      "Train loss at epoch (182):  0.05341145\n",
      "Test loss at epoch (182):  0.028578551\n",
      "Train loss at epoch (183):  0.053182207\n",
      "Test loss at epoch (183):  0.028893357\n",
      "Train loss at epoch (184):  0.05337071\n",
      "Test loss at epoch (184):  0.028291298\n",
      "Train loss at epoch (185):  0.053389702\n",
      "Test loss at epoch (185):  0.02913811\n",
      "Train loss at epoch (186):  0.05321576\n",
      "Test loss at epoch (186):  0.028860873\n",
      "Train loss at epoch (187):  0.05349427\n",
      "Test loss at epoch (187):  0.028573431\n",
      "Train loss at epoch (188):  0.052931894\n",
      "Test loss at epoch (188):  0.028057355\n",
      "Train loss at epoch (189):  0.053020686\n",
      "Test loss at epoch (189):  0.028343104\n",
      "Train loss at epoch (190):  0.053027153\n",
      "Test loss at epoch (190):  0.028824288\n",
      "Train loss at epoch (191):  0.053022403\n",
      "Test loss at epoch (191):  0.028319368\n",
      "Train loss at epoch (192):  0.05294082\n",
      "Test loss at epoch (192):  0.028286548\n",
      "Train loss at epoch (193):  0.05279082\n",
      "Test loss at epoch (193):  0.028830308\n",
      "Train loss at epoch (194):  0.05271714\n",
      "Test loss at epoch (194):  0.028345004\n",
      "Train loss at epoch (195):  0.052351043\n",
      "Test loss at epoch (195):  0.027919663\n",
      "Train loss at epoch (196):  0.052912228\n",
      "Test loss at epoch (196):  0.028503085\n",
      "Train loss at epoch (197):  0.05255855\n",
      "Test loss at epoch (197):  0.028460769\n",
      "Train loss at epoch (198):  0.052212313\n",
      "Test loss at epoch (198):  0.028098386\n",
      "Train loss at epoch (199):  0.052344188\n",
      "Test loss at epoch (199):  0.028082343\n",
      "Train loss at epoch (200):  0.052063037\n",
      "Test loss at epoch (200):  0.027945776\n",
      "Train loss at epoch (201):  0.05241599\n",
      "Test loss at epoch (201):  0.028318057\n",
      "Train loss at epoch (202):  0.05219301\n",
      "Test loss at epoch (202):  0.028195724\n",
      "Train loss at epoch (203):  0.052285735\n",
      "Test loss at epoch (203):  0.027913209\n",
      "Train loss at epoch (204):  0.05226309\n",
      "Test loss at epoch (204):  0.028655505\n",
      "Train loss at epoch (205):  0.05207937\n",
      "Test loss at epoch (205):  0.027431602\n",
      "Train loss at epoch (206):  0.052217007\n",
      "Test loss at epoch (206):  0.027552312\n",
      "Train loss at epoch (207):  0.05235399\n",
      "Test loss at epoch (207):  0.02783045\n",
      "Train loss at epoch (208):  0.051958386\n",
      "Test loss at epoch (208):  0.02816745\n",
      "Train loss at epoch (209):  0.05190346\n",
      "Test loss at epoch (209):  0.027688516\n",
      "Train loss at epoch (210):  0.051932216\n",
      "Test loss at epoch (210):  0.027960846\n",
      "Train loss at epoch (211):  0.05183515\n",
      "Test loss at epoch (211):  0.027511781\n",
      "Train loss at epoch (212):  0.051712602\n",
      "Test loss at epoch (212):  0.027656144\n",
      "Train loss at epoch (213):  0.051591747\n",
      "Test loss at epoch (213):  0.027637068\n",
      "Train loss at epoch (214):  0.051393334\n",
      "Test loss at epoch (214):  0.027347932\n",
      "Train loss at epoch (215):  0.051780026\n",
      "Test loss at epoch (215):  0.027302833\n",
      "Train loss at epoch (216):  0.05193866\n",
      "Test loss at epoch (216):  0.028385475\n",
      "Train loss at epoch (217):  0.051433764\n",
      "Test loss at epoch (217):  0.02737894\n",
      "Train loss at epoch (218):  0.051421847\n",
      "Test loss at epoch (218):  0.028042426\n",
      "Train loss at epoch (219):  0.051676102\n",
      "Test loss at epoch (219):  0.027800761\n",
      "Train loss at epoch (220):  0.05142813\n",
      "Test loss at epoch (220):  0.027675167\n",
      "Train loss at epoch (221):  0.051227752\n",
      "Test loss at epoch (221):  0.027299477\n",
      "Train loss at epoch (222):  0.05150171\n",
      "Test loss at epoch (222):  0.027438141\n",
      "Train loss at epoch (223):  0.05113022\n",
      "Test loss at epoch (223):  0.027305499\n",
      "Train loss at epoch (224):  0.051381607\n",
      "Test loss at epoch (224):  0.027128452\n",
      "Train loss at epoch (225):  0.051159434\n",
      "Test loss at epoch (225):  0.027372066\n",
      "Train loss at epoch (226):  0.050946504\n",
      "Test loss at epoch (226):  0.027611187\n",
      "Train loss at epoch (227):  0.05109819\n",
      "Test loss at epoch (227):  0.028725803\n",
      "Train loss at epoch (228):  0.05134949\n",
      "Test loss at epoch (228):  0.027185522\n",
      "Train loss at epoch (229):  0.05064597\n",
      "Test loss at epoch (229):  0.026909687\n",
      "Train loss at epoch (230):  0.050568614\n",
      "Test loss at epoch (230):  0.027344054\n",
      "Train loss at epoch (231):  0.050765\n",
      "Test loss at epoch (231):  0.027283289\n",
      "Train loss at epoch (232):  0.05098037\n",
      "Test loss at epoch (232):  0.026697338\n",
      "Train loss at epoch (233):  0.050875667\n",
      "Test loss at epoch (233):  0.02697896\n",
      "Train loss at epoch (234):  0.050755613\n",
      "Test loss at epoch (234):  0.0273079\n",
      "Train loss at epoch (235):  0.05064925\n",
      "Test loss at epoch (235):  0.027349886\n",
      "Train loss at epoch (236):  0.05042782\n",
      "Test loss at epoch (236):  0.027101545\n",
      "Train loss at epoch (237):  0.050473865\n",
      "Test loss at epoch (237):  0.02709934\n",
      "Train loss at epoch (238):  0.050478954\n",
      "Test loss at epoch (238):  0.026891077\n",
      "Train loss at epoch (239):  0.05009359\n",
      "Test loss at epoch (239):  0.027424771\n",
      "Train loss at epoch (240):  0.050653774\n",
      "Test loss at epoch (240):  0.02685381\n",
      "Train loss at epoch (241):  0.050169047\n",
      "Test loss at epoch (241):  0.027072394\n",
      "Train loss at epoch (242):  0.050206263\n",
      "Test loss at epoch (242):  0.026405513\n",
      "Train loss at epoch (243):  0.049998596\n",
      "Test loss at epoch (243):  0.026655028\n",
      "Train loss at epoch (244):  0.05001555\n",
      "Test loss at epoch (244):  0.026872529\n",
      "Train loss at epoch (245):  0.049759816\n",
      "Test loss at epoch (245):  0.026990298\n",
      "Train loss at epoch (246):  0.050233025\n",
      "Test loss at epoch (246):  0.026791042\n",
      "Train loss at epoch (247):  0.050366938\n",
      "Test loss at epoch (247):  0.026950782\n",
      "Train loss at epoch (248):  0.04997904\n",
      "Test loss at epoch (248):  0.02659393\n",
      "Train loss at epoch (249):  0.050003335\n",
      "Test loss at epoch (249):  0.026482528\n",
      "Train loss at epoch (250):  0.049863152\n",
      "Test loss at epoch (250):  0.026659107\n",
      "Train loss at epoch (251):  0.049639218\n",
      "Test loss at epoch (251):  0.026772384\n",
      "Train loss at epoch (252):  0.049883276\n",
      "Test loss at epoch (252):  0.02611406\n",
      "Train loss at epoch (253):  0.049814455\n",
      "Test loss at epoch (253):  0.027046671\n",
      "Train loss at epoch (254):  0.049915418\n",
      "Test loss at epoch (254):  0.026324376\n",
      "Train loss at epoch (255):  0.049605165\n",
      "Test loss at epoch (255):  0.026384436\n",
      "Train loss at epoch (256):  0.049690295\n",
      "Test loss at epoch (256):  0.02637986\n",
      "Train loss at epoch (257):  0.049379002\n",
      "Test loss at epoch (257):  0.026851755\n",
      "Train loss at epoch (258):  0.04942382\n",
      "Test loss at epoch (258):  0.026168907\n",
      "Train loss at epoch (259):  0.049309906\n",
      "Test loss at epoch (259):  0.02675745\n",
      "Train loss at epoch (260):  0.049479235\n",
      "Test loss at epoch (260):  0.02685704\n",
      "Train loss at epoch (261):  0.04945622\n",
      "Test loss at epoch (261):  0.026481986\n",
      "Train loss at epoch (262):  0.04951843\n",
      "Test loss at epoch (262):  0.026885107\n",
      "Train loss at epoch (263):  0.04941822\n",
      "Test loss at epoch (263):  0.026172375\n",
      "Train loss at epoch (264):  0.04925872\n",
      "Test loss at epoch (264):  0.026456576\n",
      "Train loss at epoch (265):  0.049269564\n",
      "Test loss at epoch (265):  0.026577074\n",
      "Train loss at epoch (266):  0.049352568\n",
      "Test loss at epoch (266):  0.026822424\n",
      "Train loss at epoch (267):  0.049176395\n",
      "Test loss at epoch (267):  0.026115077\n",
      "Train loss at epoch (268):  0.048871394\n",
      "Test loss at epoch (268):  0.026259152\n",
      "Train loss at epoch (269):  0.04898281\n",
      "Test loss at epoch (269):  0.027043447\n",
      "Train loss at epoch (270):  0.049064003\n",
      "Test loss at epoch (270):  0.026515493\n",
      "Train loss at epoch (271):  0.049209636\n",
      "Test loss at epoch (271):  0.026719952\n",
      "Train loss at epoch (272):  0.04921862\n",
      "Test loss at epoch (272):  0.025823658\n",
      "Train loss at epoch (273):  0.049158383\n",
      "Test loss at epoch (273):  0.025771413\n",
      "Train loss at epoch (274):  0.048947934\n",
      "Test loss at epoch (274):  0.025732784\n",
      "Train loss at epoch (275):  0.048911165\n",
      "Test loss at epoch (275):  0.025780337\n",
      "Train loss at epoch (276):  0.04876687\n",
      "Test loss at epoch (276):  0.026987204\n",
      "Train loss at epoch (277):  0.048705507\n",
      "Test loss at epoch (277):  0.025751382\n",
      "Train loss at epoch (278):  0.04861137\n",
      "Test loss at epoch (278):  0.025655618\n",
      "Train loss at epoch (279):  0.048765097\n",
      "Test loss at epoch (279):  0.025536733\n",
      "Train loss at epoch (280):  0.04871234\n",
      "Test loss at epoch (280):  0.025564639\n",
      "Train loss at epoch (281):  0.04856262\n",
      "Test loss at epoch (281):  0.025368065\n",
      "Train loss at epoch (282):  0.048676703\n",
      "Test loss at epoch (282):  0.026200475\n",
      "Train loss at epoch (283):  0.04851385\n",
      "Test loss at epoch (283):  0.025513157\n",
      "Train loss at epoch (284):  0.0486242\n",
      "Test loss at epoch (284):  0.025693271\n",
      "Train loss at epoch (285):  0.04843737\n",
      "Test loss at epoch (285):  0.025785899\n",
      "Train loss at epoch (286):  0.048398703\n",
      "Test loss at epoch (286):  0.0256519\n",
      "Train loss at epoch (287):  0.04850211\n",
      "Test loss at epoch (287):  0.026154062\n",
      "Train loss at epoch (288):  0.048387814\n",
      "Test loss at epoch (288):  0.025735864\n",
      "Train loss at epoch (289):  0.048235085\n",
      "Test loss at epoch (289):  0.026770592\n",
      "Train loss at epoch (290):  0.04817999\n",
      "Test loss at epoch (290):  0.025929721\n",
      "Train loss at epoch (291):  0.048274923\n",
      "Test loss at epoch (291):  0.025409564\n",
      "Train loss at epoch (292):  0.04833353\n",
      "Test loss at epoch (292):  0.026149135\n",
      "Train loss at epoch (293):  0.048134375\n",
      "Test loss at epoch (293):  0.025148997\n",
      "Train loss at epoch (294):  0.048069738\n",
      "Test loss at epoch (294):  0.025954632\n",
      "Train loss at epoch (295):  0.047927268\n",
      "Test loss at epoch (295):  0.025466984\n",
      "Train loss at epoch (296):  0.048103068\n",
      "Test loss at epoch (296):  0.02562224\n",
      "Train loss at epoch (297):  0.04816984\n",
      "Test loss at epoch (297):  0.02603647\n",
      "Train loss at epoch (298):  0.048073586\n",
      "Test loss at epoch (298):  0.02580669\n",
      "Train loss at epoch (299):  0.048005253\n",
      "Test loss at epoch (299):  0.02546129\n",
      "Train loss at epoch (300):  0.047830664\n",
      "Test loss at epoch (300):  0.025113933\n",
      "Train loss at epoch (301):  0.047700748\n",
      "Test loss at epoch (301):  0.025189424\n",
      "Train loss at epoch (302):  0.04782648\n",
      "Test loss at epoch (302):  0.025170485\n",
      "Train loss at epoch (303):  0.047807056\n",
      "Test loss at epoch (303):  0.026002083\n",
      "Train loss at epoch (304):  0.047653694\n",
      "Test loss at epoch (304):  0.02596925\n",
      "Train loss at epoch (305):  0.047842026\n",
      "Test loss at epoch (305):  0.02519296\n",
      "Train loss at epoch (306):  0.04782677\n",
      "Test loss at epoch (306):  0.025884222\n",
      "Train loss at epoch (307):  0.04788272\n",
      "Test loss at epoch (307):  0.024900874\n",
      "Train loss at epoch (308):  0.047642775\n",
      "Test loss at epoch (308):  0.025042394\n",
      "Train loss at epoch (309):  0.047639977\n",
      "Test loss at epoch (309):  0.02509503\n",
      "Train loss at epoch (310):  0.04750693\n",
      "Test loss at epoch (310):  0.025633696\n",
      "Train loss at epoch (311):  0.04763636\n",
      "Test loss at epoch (311):  0.024880053\n",
      "Train loss at epoch (312):  0.04734001\n",
      "Test loss at epoch (312):  0.025152102\n",
      "Train loss at epoch (313):  0.047449663\n",
      "Test loss at epoch (313):  0.024589451\n",
      "Train loss at epoch (314):  0.047233887\n",
      "Test loss at epoch (314):  0.025215948\n",
      "Train loss at epoch (315):  0.047432445\n",
      "Test loss at epoch (315):  0.025600476\n",
      "Train loss at epoch (316):  0.047255553\n",
      "Test loss at epoch (316):  0.025243595\n",
      "Train loss at epoch (317):  0.047377992\n",
      "Test loss at epoch (317):  0.024850184\n",
      "Train loss at epoch (318):  0.047484875\n",
      "Test loss at epoch (318):  0.025141556\n",
      "Train loss at epoch (319):  0.047541805\n",
      "Test loss at epoch (319):  0.025349358\n",
      "Train loss at epoch (320):  0.047249544\n",
      "Test loss at epoch (320):  0.025404437\n",
      "Train loss at epoch (321):  0.047277793\n",
      "Test loss at epoch (321):  0.026368985\n",
      "Train loss at epoch (322):  0.047244783\n",
      "Test loss at epoch (322):  0.025884569\n",
      "Train loss at epoch (323):  0.04722692\n",
      "Test loss at epoch (323):  0.024802351\n",
      "Train loss at epoch (324):  0.04696164\n",
      "Test loss at epoch (324):  0.024735518\n",
      "Train loss at epoch (325):  0.04734045\n",
      "Test loss at epoch (325):  0.024336535\n",
      "Train loss at epoch (326):  0.046830937\n",
      "Test loss at epoch (326):  0.024703769\n",
      "Train loss at epoch (327):  0.04710432\n",
      "Test loss at epoch (327):  0.025045136\n",
      "Train loss at epoch (328):  0.046954554\n",
      "Test loss at epoch (328):  0.025357263\n",
      "Train loss at epoch (329):  0.047105495\n",
      "Test loss at epoch (329):  0.024369195\n",
      "Train loss at epoch (330):  0.046844997\n",
      "Test loss at epoch (330):  0.024971224\n",
      "Train loss at epoch (331):  0.047108207\n",
      "Test loss at epoch (331):  0.02466667\n",
      "Train loss at epoch (332):  0.04688149\n",
      "Test loss at epoch (332):  0.024498118\n",
      "Train loss at epoch (333):  0.04684726\n",
      "Test loss at epoch (333):  0.024615876\n",
      "Train loss at epoch (334):  0.04680246\n",
      "Test loss at epoch (334):  0.02445889\n",
      "Train loss at epoch (335):  0.047010362\n",
      "Test loss at epoch (335):  0.024307866\n",
      "Train loss at epoch (336):  0.046863712\n",
      "Test loss at epoch (336):  0.024814377\n",
      "Train loss at epoch (337):  0.04669045\n",
      "Test loss at epoch (337):  0.02528673\n",
      "Train loss at epoch (338):  0.04694185\n",
      "Test loss at epoch (338):  0.02429798\n",
      "Train loss at epoch (339):  0.046672925\n",
      "Test loss at epoch (339):  0.026016025\n",
      "Train loss at epoch (340):  0.046696197\n",
      "Test loss at epoch (340):  0.024526464\n",
      "Train loss at epoch (341):  0.046711825\n",
      "Test loss at epoch (341):  0.024494443\n",
      "Train loss at epoch (342):  0.046581935\n",
      "Test loss at epoch (342):  0.02480879\n",
      "Train loss at epoch (343):  0.04658632\n",
      "Test loss at epoch (343):  0.025051145\n",
      "Train loss at epoch (344):  0.046672188\n",
      "Test loss at epoch (344):  0.024656083\n",
      "Train loss at epoch (345):  0.046425026\n",
      "Test loss at epoch (345):  0.02431334\n",
      "Train loss at epoch (346):  0.04623473\n",
      "Test loss at epoch (346):  0.024328396\n",
      "Train loss at epoch (347):  0.046411376\n",
      "Test loss at epoch (347):  0.023998452\n",
      "Train loss at epoch (348):  0.046469122\n",
      "Test loss at epoch (348):  0.024525966\n",
      "Train loss at epoch (349):  0.04638902\n",
      "Test loss at epoch (349):  0.023951152\n",
      "Train loss at epoch (350):  0.046297174\n",
      "Test loss at epoch (350):  0.024312604\n",
      "Train loss at epoch (351):  0.04643936\n",
      "Test loss at epoch (351):  0.02370613\n",
      "Train loss at epoch (352):  0.046315458\n",
      "Test loss at epoch (352):  0.024421144\n",
      "Train loss at epoch (353):  0.046026967\n",
      "Test loss at epoch (353):  0.024747515\n",
      "Train loss at epoch (354):  0.04609628\n",
      "Test loss at epoch (354):  0.024481537\n",
      "Train loss at epoch (355):  0.04613349\n",
      "Test loss at epoch (355):  0.024139082\n",
      "Train loss at epoch (356):  0.046117328\n",
      "Test loss at epoch (356):  0.023574537\n",
      "Train loss at epoch (357):  0.04626453\n",
      "Test loss at epoch (357):  0.024049861\n",
      "Train loss at epoch (358):  0.046054516\n",
      "Test loss at epoch (358):  0.024487682\n",
      "Train loss at epoch (359):  0.045970105\n",
      "Test loss at epoch (359):  0.024081971\n",
      "Train loss at epoch (360):  0.046162672\n",
      "Test loss at epoch (360):  0.023943126\n",
      "Train loss at epoch (361):  0.045910373\n",
      "Test loss at epoch (361):  0.024029111\n",
      "Train loss at epoch (362):  0.04620607\n",
      "Test loss at epoch (362):  0.02435737\n",
      "Train loss at epoch (363):  0.046083137\n",
      "Test loss at epoch (363):  0.024011523\n",
      "Train loss at epoch (364):  0.045818895\n",
      "Test loss at epoch (364):  0.02449965\n",
      "Train loss at epoch (365):  0.045755394\n",
      "Test loss at epoch (365):  0.024248352\n",
      "Train loss at epoch (366):  0.045737047\n",
      "Test loss at epoch (366):  0.023833055\n",
      "Train loss at epoch (367):  0.045881674\n",
      "Test loss at epoch (367):  0.024483884\n",
      "Train loss at epoch (368):  0.04585163\n",
      "Test loss at epoch (368):  0.024691235\n",
      "Train loss at epoch (369):  0.045854013\n",
      "Test loss at epoch (369):  0.024103515\n",
      "Train loss at epoch (370):  0.04590287\n",
      "Test loss at epoch (370):  0.024898654\n",
      "Train loss at epoch (371):  0.04569991\n",
      "Test loss at epoch (371):  0.023771629\n",
      "Train loss at epoch (372):  0.045630172\n",
      "Test loss at epoch (372):  0.024112092\n",
      "Train loss at epoch (373):  0.045591906\n",
      "Test loss at epoch (373):  0.023639917\n",
      "Train loss at epoch (374):  0.045651544\n",
      "Test loss at epoch (374):  0.023256963\n",
      "Train loss at epoch (375):  0.045655534\n",
      "Test loss at epoch (375):  0.025006194\n",
      "Train loss at epoch (376):  0.045372296\n",
      "Test loss at epoch (376):  0.023989249\n",
      "Train loss at epoch (377):  0.045509145\n",
      "Test loss at epoch (377):  0.023994148\n",
      "Train loss at epoch (378):  0.045666687\n",
      "Test loss at epoch (378):  0.023957103\n",
      "Train loss at epoch (379):  0.045447867\n",
      "Test loss at epoch (379):  0.023665434\n",
      "Train loss at epoch (380):  0.045558862\n",
      "Test loss at epoch (380):  0.024269989\n",
      "Train loss at epoch (381):  0.045385614\n",
      "Test loss at epoch (381):  0.024061276\n",
      "Train loss at epoch (382):  0.04526728\n",
      "Test loss at epoch (382):  0.02441936\n",
      "Train loss at epoch (383):  0.04518179\n",
      "Test loss at epoch (383):  0.023548476\n",
      "Train loss at epoch (384):  0.04535588\n",
      "Test loss at epoch (384):  0.024603063\n",
      "Train loss at epoch (385):  0.045195565\n",
      "Test loss at epoch (385):  0.023550319\n",
      "Train loss at epoch (386):  0.045261543\n",
      "Test loss at epoch (386):  0.023530774\n",
      "Train loss at epoch (387):  0.045294806\n",
      "Test loss at epoch (387):  0.023589397\n",
      "Train loss at epoch (388):  0.045148775\n",
      "Test loss at epoch (388):  0.023790643\n",
      "Train loss at epoch (389):  0.04512862\n",
      "Test loss at epoch (389):  0.023398226\n",
      "Train loss at epoch (390):  0.045252495\n",
      "Test loss at epoch (390):  0.024233777\n",
      "Train loss at epoch (391):  0.04502003\n",
      "Test loss at epoch (391):  0.0231485\n",
      "Train loss at epoch (392):  0.04508345\n",
      "Test loss at epoch (392):  0.023731237\n",
      "Train loss at epoch (393):  0.044936646\n",
      "Test loss at epoch (393):  0.023909923\n",
      "Train loss at epoch (394):  0.045084596\n",
      "Test loss at epoch (394):  0.02414102\n",
      "Train loss at epoch (395):  0.0448539\n",
      "Test loss at epoch (395):  0.023086125\n",
      "Train loss at epoch (396):  0.04489527\n",
      "Test loss at epoch (396):  0.023773368\n",
      "Train loss at epoch (397):  0.044621047\n",
      "Test loss at epoch (397):  0.023023473\n",
      "Train loss at epoch (398):  0.045019913\n",
      "Test loss at epoch (398):  0.02330343\n",
      "Train loss at epoch (399):  0.044703074\n",
      "Test loss at epoch (399):  0.024017334\n",
      "Train loss at epoch (400):  0.044796456\n",
      "Test loss at epoch (400):  0.023287121\n",
      "Train loss at epoch (401):  0.044596743\n",
      "Test loss at epoch (401):  0.023285428\n",
      "Train loss at epoch (402):  0.04491893\n",
      "Test loss at epoch (402):  0.02344732\n",
      "Train loss at epoch (403):  0.044824246\n",
      "Test loss at epoch (403):  0.02301165\n",
      "Train loss at epoch (404):  0.04451785\n",
      "Test loss at epoch (404):  0.024710163\n",
      "Train loss at epoch (405):  0.04468875\n",
      "Test loss at epoch (405):  0.023313608\n",
      "Train loss at epoch (406):  0.044791106\n",
      "Test loss at epoch (406):  0.023004467\n",
      "Train loss at epoch (407):  0.044805817\n",
      "Test loss at epoch (407):  0.022981077\n",
      "Train loss at epoch (408):  0.04469952\n",
      "Test loss at epoch (408):  0.02302206\n",
      "Train loss at epoch (409):  0.044893824\n",
      "Test loss at epoch (409):  0.02332806\n",
      "Train loss at epoch (410):  0.044547677\n",
      "Test loss at epoch (410):  0.023578845\n",
      "Train loss at epoch (411):  0.04431917\n",
      "Test loss at epoch (411):  0.022904921\n",
      "Train loss at epoch (412):  0.04452804\n",
      "Test loss at epoch (412):  0.02451231\n",
      "Train loss at epoch (413):  0.044827018\n",
      "Test loss at epoch (413):  0.023667416\n",
      "Train loss at epoch (414):  0.044452414\n",
      "Test loss at epoch (414):  0.024218226\n",
      "Train loss at epoch (415):  0.044482607\n",
      "Test loss at epoch (415):  0.023410842\n",
      "Train loss at epoch (416):  0.04452164\n",
      "Test loss at epoch (416):  0.023792239\n",
      "Train loss at epoch (417):  0.044200588\n",
      "Test loss at epoch (417):  0.023529762\n",
      "Train loss at epoch (418):  0.04426543\n",
      "Test loss at epoch (418):  0.022854565\n",
      "Train loss at epoch (419):  0.044314377\n",
      "Test loss at epoch (419):  0.02325024\n",
      "Train loss at epoch (420):  0.044254508\n",
      "Test loss at epoch (420):  0.025607267\n",
      "Train loss at epoch (421):  0.044406753\n",
      "Test loss at epoch (421):  0.022689272\n",
      "Train loss at epoch (422):  0.044290584\n",
      "Test loss at epoch (422):  0.023091378\n",
      "Train loss at epoch (423):  0.04440265\n",
      "Test loss at epoch (423):  0.022665538\n",
      "Train loss at epoch (424):  0.04418776\n",
      "Test loss at epoch (424):  0.022747759\n",
      "Train loss at epoch (425):  0.0441855\n",
      "Test loss at epoch (425):  0.023476357\n",
      "Train loss at epoch (426):  0.0440563\n",
      "Test loss at epoch (426):  0.023920707\n",
      "Train loss at epoch (427):  0.044114213\n",
      "Test loss at epoch (427):  0.0228194\n",
      "Train loss at epoch (428):  0.044095334\n",
      "Test loss at epoch (428):  0.022803511\n",
      "Train loss at epoch (429):  0.04396768\n",
      "Test loss at epoch (429):  0.023060307\n",
      "Train loss at epoch (430):  0.044104133\n",
      "Test loss at epoch (430):  0.022639588\n",
      "Train loss at epoch (431):  0.04396305\n",
      "Test loss at epoch (431):  0.022787679\n",
      "Train loss at epoch (432):  0.044020046\n",
      "Test loss at epoch (432):  0.023051059\n",
      "Train loss at epoch (433):  0.043988477\n",
      "Test loss at epoch (433):  0.022553217\n",
      "Train loss at epoch (434):  0.04385426\n",
      "Test loss at epoch (434):  0.023832044\n",
      "Train loss at epoch (435):  0.0437749\n",
      "Test loss at epoch (435):  0.022991523\n",
      "Train loss at epoch (436):  0.044092726\n",
      "Test loss at epoch (436):  0.023009406\n",
      "Train loss at epoch (437):  0.044018134\n",
      "Test loss at epoch (437):  0.022839615\n",
      "Train loss at epoch (438):  0.044122167\n",
      "Test loss at epoch (438):  0.022994427\n",
      "Train loss at epoch (439):  0.04381008\n",
      "Test loss at epoch (439):  0.022896403\n",
      "Train loss at epoch (440):  0.043700032\n",
      "Test loss at epoch (440):  0.023120964\n",
      "Train loss at epoch (441):  0.044049747\n",
      "Test loss at epoch (441):  0.022893911\n",
      "Train loss at epoch (442):  0.043503\n",
      "Test loss at epoch (442):  0.02266451\n",
      "Train loss at epoch (443):  0.04378752\n",
      "Test loss at epoch (443):  0.022973424\n",
      "Train loss at epoch (444):  0.04376534\n",
      "Test loss at epoch (444):  0.02304406\n",
      "Train loss at epoch (445):  0.043503758\n",
      "Test loss at epoch (445):  0.022786103\n",
      "Train loss at epoch (446):  0.043489754\n",
      "Test loss at epoch (446):  0.022643236\n",
      "Train loss at epoch (447):  0.04346944\n",
      "Test loss at epoch (447):  0.023027528\n",
      "Train loss at epoch (448):  0.043371078\n",
      "Test loss at epoch (448):  0.022328343\n",
      "Train loss at epoch (449):  0.043493304\n",
      "Test loss at epoch (449):  0.022933789\n",
      "Train loss at epoch (450):  0.043353405\n",
      "Test loss at epoch (450):  0.022134906\n",
      "Train loss at epoch (451):  0.04348311\n",
      "Test loss at epoch (451):  0.022397945\n",
      "Train loss at epoch (452):  0.04326763\n",
      "Test loss at epoch (452):  0.022471854\n",
      "Train loss at epoch (453):  0.043544896\n",
      "Test loss at epoch (453):  0.022906234\n",
      "Train loss at epoch (454):  0.043357663\n",
      "Test loss at epoch (454):  0.022530412\n",
      "Train loss at epoch (455):  0.043283485\n",
      "Test loss at epoch (455):  0.023642864\n",
      "Train loss at epoch (456):  0.043492872\n",
      "Test loss at epoch (456):  0.023505688\n",
      "Train loss at epoch (457):  0.043220952\n",
      "Test loss at epoch (457):  0.022402793\n",
      "Train loss at epoch (458):  0.04333032\n",
      "Test loss at epoch (458):  0.02354688\n",
      "Train loss at epoch (459):  0.043374397\n",
      "Test loss at epoch (459):  0.02232703\n",
      "Train loss at epoch (460):  0.043361504\n",
      "Test loss at epoch (460):  0.02275594\n",
      "Train loss at epoch (461):  0.043581117\n",
      "Test loss at epoch (461):  0.022593692\n",
      "Train loss at epoch (462):  0.04305081\n",
      "Test loss at epoch (462):  0.022876924\n",
      "Train loss at epoch (463):  0.043426737\n",
      "Test loss at epoch (463):  0.022872355\n",
      "Train loss at epoch (464):  0.043062925\n",
      "Test loss at epoch (464):  0.022771427\n",
      "Train loss at epoch (465):  0.043285903\n",
      "Test loss at epoch (465):  0.022942832\n",
      "Train loss at epoch (466):  0.042839248\n",
      "Test loss at epoch (466):  0.022920128\n",
      "Train loss at epoch (467):  0.042781647\n",
      "Test loss at epoch (467):  0.022729581\n",
      "Train loss at epoch (468):  0.043138184\n",
      "Test loss at epoch (468):  0.022511791\n",
      "Train loss at epoch (469):  0.04288382\n",
      "Test loss at epoch (469):  0.022576263\n",
      "Train loss at epoch (470):  0.043037206\n",
      "Test loss at epoch (470):  0.022356642\n",
      "Train loss at epoch (471):  0.04322454\n",
      "Test loss at epoch (471):  0.022448843\n",
      "Train loss at epoch (472):  0.043179747\n",
      "Test loss at epoch (472):  0.022430934\n",
      "Train loss at epoch (473):  0.042869035\n",
      "Test loss at epoch (473):  0.022862537\n",
      "Train loss at epoch (474):  0.04287858\n",
      "Test loss at epoch (474):  0.022192229\n",
      "Train loss at epoch (475):  0.042811144\n",
      "Test loss at epoch (475):  0.022386836\n",
      "Train loss at epoch (476):  0.04256683\n",
      "Test loss at epoch (476):  0.022722246\n",
      "Train loss at epoch (477):  0.042700328\n",
      "Test loss at epoch (477):  0.022735052\n",
      "Train loss at epoch (478):  0.042596653\n",
      "Test loss at epoch (478):  0.021952068\n",
      "Train loss at epoch (479):  0.042789187\n",
      "Test loss at epoch (479):  0.022484647\n",
      "Train loss at epoch (480):  0.042737722\n",
      "Test loss at epoch (480):  0.022270665\n",
      "Train loss at epoch (481):  0.042908046\n",
      "Test loss at epoch (481):  0.022290083\n",
      "Train loss at epoch (482):  0.042637873\n",
      "Test loss at epoch (482):  0.022930127\n",
      "Train loss at epoch (483):  0.0428924\n",
      "Test loss at epoch (483):  0.022363275\n",
      "Train loss at epoch (484):  0.042651128\n",
      "Test loss at epoch (484):  0.022405088\n",
      "Train loss at epoch (485):  0.042696618\n",
      "Test loss at epoch (485):  0.023246376\n",
      "Train loss at epoch (486):  0.042864997\n",
      "Test loss at epoch (486):  0.022933545\n",
      "Train loss at epoch (487):  0.042628124\n",
      "Test loss at epoch (487):  0.02293994\n",
      "Train loss at epoch (488):  0.042539973\n",
      "Test loss at epoch (488):  0.021793956\n",
      "Train loss at epoch (489):  0.04254568\n",
      "Test loss at epoch (489):  0.022577507\n",
      "Train loss at epoch (490):  0.042564962\n",
      "Test loss at epoch (490):  0.022886327\n",
      "Train loss at epoch (491):  0.04234471\n",
      "Test loss at epoch (491):  0.022172688\n",
      "Train loss at epoch (492):  0.04220762\n",
      "Test loss at epoch (492):  0.022237856\n",
      "Train loss at epoch (493):  0.042731766\n",
      "Test loss at epoch (493):  0.022542447\n",
      "Train loss at epoch (494):  0.042400017\n",
      "Test loss at epoch (494):  0.022091702\n",
      "Train loss at epoch (495):  0.042321287\n",
      "Test loss at epoch (495):  0.022772536\n",
      "Train loss at epoch (496):  0.042246006\n",
      "Test loss at epoch (496):  0.022276605\n",
      "Train loss at epoch (497):  0.042082325\n",
      "Test loss at epoch (497):  0.022137215\n",
      "Train loss at epoch (498):  0.04233667\n",
      "Test loss at epoch (498):  0.022390867\n",
      "Train loss at epoch (499):  0.04232532\n",
      "Test loss at epoch (499):  0.021938259\n",
      "Train loss at epoch (500):  0.042354763\n",
      "Test loss at epoch (500):  0.02289284\n",
      "Train loss at epoch (501):  0.042298004\n",
      "Test loss at epoch (501):  0.02210784\n",
      "Train loss at epoch (502):  0.042065866\n",
      "Test loss at epoch (502):  0.023421964\n",
      "Train loss at epoch (503):  0.042212814\n",
      "Test loss at epoch (503):  0.022092236\n",
      "Train loss at epoch (504):  0.042195998\n",
      "Test loss at epoch (504):  0.021623885\n",
      "Train loss at epoch (505):  0.04187353\n",
      "Test loss at epoch (505):  0.022358336\n",
      "Train loss at epoch (506):  0.04219609\n",
      "Test loss at epoch (506):  0.021903131\n",
      "Train loss at epoch (507):  0.042192277\n",
      "Test loss at epoch (507):  0.02161541\n",
      "Train loss at epoch (508):  0.04204444\n",
      "Test loss at epoch (508):  0.02264147\n",
      "Train loss at epoch (509):  0.041936584\n",
      "Test loss at epoch (509):  0.022021508\n",
      "Train loss at epoch (510):  0.042020854\n",
      "Test loss at epoch (510):  0.021876588\n",
      "Train loss at epoch (511):  0.041921023\n",
      "Test loss at epoch (511):  0.02160486\n",
      "Train loss at epoch (512):  0.041973207\n",
      "Test loss at epoch (512):  0.021636657\n",
      "Train loss at epoch (513):  0.041889418\n",
      "Test loss at epoch (513):  0.022222724\n",
      "Train loss at epoch (514):  0.0418549\n",
      "Test loss at epoch (514):  0.021712448\n",
      "Train loss at epoch (515):  0.042051226\n",
      "Test loss at epoch (515):  0.021653488\n",
      "Train loss at epoch (516):  0.04193688\n",
      "Test loss at epoch (516):  0.022180678\n",
      "Train loss at epoch (517):  0.041857086\n",
      "Test loss at epoch (517):  0.021937972\n",
      "Train loss at epoch (518):  0.04173867\n",
      "Test loss at epoch (518):  0.0219455\n",
      "Train loss at epoch (519):  0.04193433\n",
      "Test loss at epoch (519):  0.022675732\n",
      "Train loss at epoch (520):  0.041725162\n",
      "Test loss at epoch (520):  0.022084057\n",
      "Train loss at epoch (521):  0.041636415\n",
      "Test loss at epoch (521):  0.02129111\n",
      "Train loss at epoch (522):  0.041962434\n",
      "Test loss at epoch (522):  0.021782082\n",
      "Train loss at epoch (523):  0.041856535\n",
      "Test loss at epoch (523):  0.02181419\n",
      "Train loss at epoch (524):  0.041505806\n",
      "Test loss at epoch (524):  0.02244029\n",
      "Train loss at epoch (525):  0.041829724\n",
      "Test loss at epoch (525):  0.02190037\n",
      "Train loss at epoch (526):  0.041628845\n",
      "Test loss at epoch (526):  0.022474453\n",
      "Train loss at epoch (527):  0.0416344\n",
      "Test loss at epoch (527):  0.021957595\n",
      "Train loss at epoch (528):  0.041580103\n",
      "Test loss at epoch (528):  0.021943433\n",
      "Train loss at epoch (529):  0.041681167\n",
      "Test loss at epoch (529):  0.02229701\n",
      "Train loss at epoch (530):  0.04162819\n",
      "Test loss at epoch (530):  0.021887992\n",
      "Train loss at epoch (531):  0.0415526\n",
      "Test loss at epoch (531):  0.02252863\n",
      "Train loss at epoch (532):  0.041515727\n",
      "Test loss at epoch (532):  0.022047814\n",
      "Train loss at epoch (533):  0.04155212\n",
      "Test loss at epoch (533):  0.021571087\n",
      "Train loss at epoch (534):  0.041323766\n",
      "Test loss at epoch (534):  0.021389768\n",
      "Train loss at epoch (535):  0.04160287\n",
      "Test loss at epoch (535):  0.022325322\n",
      "Train loss at epoch (536):  0.04161482\n",
      "Test loss at epoch (536):  0.021641767\n",
      "Train loss at epoch (537):  0.041254643\n",
      "Test loss at epoch (537):  0.021318492\n",
      "Train loss at epoch (538):  0.04121013\n",
      "Test loss at epoch (538):  0.021748075\n",
      "Train loss at epoch (539):  0.041553505\n",
      "Test loss at epoch (539):  0.021571383\n",
      "Train loss at epoch (540):  0.041248243\n",
      "Test loss at epoch (540):  0.022124106\n",
      "Train loss at epoch (541):  0.041061305\n",
      "Test loss at epoch (541):  0.021532113\n",
      "Train loss at epoch (542):  0.041449156\n",
      "Test loss at epoch (542):  0.0220491\n",
      "Train loss at epoch (543):  0.041378945\n",
      "Test loss at epoch (543):  0.02259423\n",
      "Train loss at epoch (544):  0.041274827\n",
      "Test loss at epoch (544):  0.021315103\n",
      "Train loss at epoch (545):  0.041080695\n",
      "Test loss at epoch (545):  0.021500641\n",
      "Train loss at epoch (546):  0.04084043\n",
      "Test loss at epoch (546):  0.02207805\n",
      "Train loss at epoch (547):  0.040927947\n",
      "Test loss at epoch (547):  0.021507043\n",
      "Train loss at epoch (548):  0.041256577\n",
      "Test loss at epoch (548):  0.021383105\n",
      "Train loss at epoch (549):  0.041179817\n",
      "Test loss at epoch (549):  0.021902436\n",
      "Train loss at epoch (550):  0.041245125\n",
      "Test loss at epoch (550):  0.022843393\n",
      "Train loss at epoch (551):  0.041222226\n",
      "Test loss at epoch (551):  0.022574848\n",
      "Train loss at epoch (552):  0.041016288\n",
      "Test loss at epoch (552):  0.022487083\n",
      "Train loss at epoch (553):  0.041252065\n",
      "Test loss at epoch (553):  0.021769691\n",
      "Train loss at epoch (554):  0.04120963\n",
      "Test loss at epoch (554):  0.021267617\n",
      "Train loss at epoch (555):  0.04102584\n",
      "Test loss at epoch (555):  0.021344995\n",
      "Train loss at epoch (556):  0.040951017\n",
      "Test loss at epoch (556):  0.021306539\n",
      "Train loss at epoch (557):  0.041016188\n",
      "Test loss at epoch (557):  0.02171343\n",
      "Train loss at epoch (558):  0.041057423\n",
      "Test loss at epoch (558):  0.021691129\n",
      "Train loss at epoch (559):  0.04091382\n",
      "Test loss at epoch (559):  0.021846656\n",
      "Train loss at epoch (560):  0.041073497\n",
      "Test loss at epoch (560):  0.021981899\n",
      "Train loss at epoch (561):  0.04079011\n",
      "Test loss at epoch (561):  0.021609986\n",
      "Train loss at epoch (562):  0.040737692\n",
      "Test loss at epoch (562):  0.02170726\n",
      "Train loss at epoch (563):  0.04067273\n",
      "Test loss at epoch (563):  0.02124431\n",
      "Train loss at epoch (564):  0.04080209\n",
      "Test loss at epoch (564):  0.021429785\n",
      "Train loss at epoch (565):  0.040821034\n",
      "Test loss at epoch (565):  0.021242443\n",
      "Train loss at epoch (566):  0.040792722\n",
      "Test loss at epoch (566):  0.021623522\n",
      "Train loss at epoch (567):  0.04061892\n",
      "Test loss at epoch (567):  0.021139743\n",
      "Train loss at epoch (568):  0.040677626\n",
      "Test loss at epoch (568):  0.021635583\n",
      "Train loss at epoch (569):  0.040635813\n",
      "Test loss at epoch (569):  0.02141182\n",
      "Train loss at epoch (570):  0.04058018\n",
      "Test loss at epoch (570):  0.021895573\n",
      "Train loss at epoch (571):  0.04043787\n",
      "Test loss at epoch (571):  0.022107786\n",
      "Train loss at epoch (572):  0.040693726\n",
      "Test loss at epoch (572):  0.02105042\n",
      "Train loss at epoch (573):  0.04064033\n",
      "Test loss at epoch (573):  0.021651413\n",
      "Train loss at epoch (574):  0.040723022\n",
      "Test loss at epoch (574):  0.021652991\n",
      "Train loss at epoch (575):  0.04079797\n",
      "Test loss at epoch (575):  0.022184843\n",
      "Train loss at epoch (576):  0.040548146\n",
      "Test loss at epoch (576):  0.021364234\n",
      "Train loss at epoch (577):  0.040538058\n",
      "Test loss at epoch (577):  0.021254672\n",
      "Train loss at epoch (578):  0.040499352\n",
      "Test loss at epoch (578):  0.021166714\n",
      "Train loss at epoch (579):  0.040613845\n",
      "Test loss at epoch (579):  0.021457475\n",
      "Train loss at epoch (580):  0.040625665\n",
      "Test loss at epoch (580):  0.022270655\n",
      "Train loss at epoch (581):  0.040486265\n",
      "Test loss at epoch (581):  0.021700399\n",
      "Train loss at epoch (582):  0.040688396\n",
      "Test loss at epoch (582):  0.02107383\n",
      "Train loss at epoch (583):  0.040264487\n",
      "Test loss at epoch (583):  0.021147987\n",
      "Train loss at epoch (584):  0.040350713\n",
      "Test loss at epoch (584):  0.020893838\n",
      "Train loss at epoch (585):  0.04012657\n",
      "Test loss at epoch (585):  0.020996064\n",
      "Train loss at epoch (586):  0.040266804\n",
      "Test loss at epoch (586):  0.021010792\n",
      "Train loss at epoch (587):  0.04038438\n",
      "Test loss at epoch (587):  0.02203045\n",
      "Train loss at epoch (588):  0.04042378\n",
      "Test loss at epoch (588):  0.022396855\n",
      "Train loss at epoch (589):  0.040241152\n",
      "Test loss at epoch (589):  0.020938829\n",
      "Train loss at epoch (590):  0.040446114\n",
      "Test loss at epoch (590):  0.020943249\n",
      "Train loss at epoch (591):  0.040210288\n",
      "Test loss at epoch (591):  0.02109872\n",
      "Train loss at epoch (592):  0.040308952\n",
      "Test loss at epoch (592):  0.021494966\n",
      "Train loss at epoch (593):  0.040337052\n",
      "Test loss at epoch (593):  0.02153319\n",
      "Train loss at epoch (594):  0.040177464\n",
      "Test loss at epoch (594):  0.021480327\n",
      "Train loss at epoch (595):  0.040313594\n",
      "Test loss at epoch (595):  0.021393012\n",
      "Train loss at epoch (596):  0.04015485\n",
      "Test loss at epoch (596):  0.021251129\n",
      "Train loss at epoch (597):  0.04020307\n",
      "Test loss at epoch (597):  0.021116696\n",
      "Train loss at epoch (598):  0.040118676\n",
      "Test loss at epoch (598):  0.020969879\n",
      "Train loss at epoch (599):  0.040214278\n",
      "Test loss at epoch (599):  0.021375999\n",
      "Train loss at epoch (600):  0.040148918\n",
      "Test loss at epoch (600):  0.021435028\n",
      "Train loss at epoch (601):  0.039968472\n",
      "Test loss at epoch (601):  0.021193897\n",
      "Train loss at epoch (602):  0.040189266\n",
      "Test loss at epoch (602):  0.021441422\n",
      "Train loss at epoch (603):  0.040103935\n",
      "Test loss at epoch (603):  0.02139155\n",
      "Train loss at epoch (604):  0.039957885\n",
      "Test loss at epoch (604):  0.021006452\n",
      "Train loss at epoch (605):  0.04003587\n",
      "Test loss at epoch (605):  0.021190194\n",
      "Train loss at epoch (606):  0.040049683\n",
      "Test loss at epoch (606):  0.021038964\n",
      "Train loss at epoch (607):  0.040100608\n",
      "Test loss at epoch (607):  0.021425908\n",
      "Train loss at epoch (608):  0.039902475\n",
      "Test loss at epoch (608):  0.021271676\n",
      "Train loss at epoch (609):  0.039878476\n",
      "Test loss at epoch (609):  0.021051735\n",
      "Train loss at epoch (610):  0.03999027\n",
      "Test loss at epoch (610):  0.020802336\n",
      "Train loss at epoch (611):  0.03986237\n",
      "Test loss at epoch (611):  0.021448737\n",
      "Train loss at epoch (612):  0.03961109\n",
      "Test loss at epoch (612):  0.021730056\n",
      "Train loss at epoch (613):  0.03996055\n",
      "Test loss at epoch (613):  0.021402694\n",
      "Train loss at epoch (614):  0.039920613\n",
      "Test loss at epoch (614):  0.021384165\n",
      "Train loss at epoch (615):  0.03981103\n",
      "Test loss at epoch (615):  0.02098464\n",
      "Train loss at epoch (616):  0.039932482\n",
      "Test loss at epoch (616):  0.021157647\n",
      "Train loss at epoch (617):  0.039987054\n",
      "Test loss at epoch (617):  0.021567015\n",
      "Train loss at epoch (618):  0.03965768\n",
      "Test loss at epoch (618):  0.021363325\n",
      "Train loss at epoch (619):  0.0396876\n",
      "Test loss at epoch (619):  0.021227414\n",
      "Train loss at epoch (620):  0.03964691\n",
      "Test loss at epoch (620):  0.02163196\n",
      "Train loss at epoch (621):  0.039655045\n",
      "Test loss at epoch (621):  0.021488234\n",
      "Train loss at epoch (622):  0.03962052\n",
      "Test loss at epoch (622):  0.020806426\n",
      "Train loss at epoch (623):  0.039460585\n",
      "Test loss at epoch (623):  0.02099824\n",
      "Train loss at epoch (624):  0.039732855\n",
      "Test loss at epoch (624):  0.020823484\n",
      "Train loss at epoch (625):  0.03970225\n",
      "Test loss at epoch (625):  0.020877168\n",
      "Train loss at epoch (626):  0.039612412\n",
      "Test loss at epoch (626):  0.020887049\n",
      "Train loss at epoch (627):  0.039651398\n",
      "Test loss at epoch (627):  0.021298101\n",
      "Train loss at epoch (628):  0.039429005\n",
      "Test loss at epoch (628):  0.021140875\n",
      "Train loss at epoch (629):  0.03969881\n",
      "Test loss at epoch (629):  0.021254735\n",
      "Train loss at epoch (630):  0.03944184\n",
      "Test loss at epoch (630):  0.021426782\n",
      "Train loss at epoch (631):  0.0396075\n",
      "Test loss at epoch (631):  0.020729141\n",
      "Train loss at epoch (632):  0.03940512\n",
      "Test loss at epoch (632):  0.020703567\n",
      "Train loss at epoch (633):  0.03932175\n",
      "Test loss at epoch (633):  0.020754121\n",
      "Train loss at epoch (634):  0.03945122\n",
      "Test loss at epoch (634):  0.020695139\n",
      "Train loss at epoch (635):  0.039482754\n",
      "Test loss at epoch (635):  0.02076008\n",
      "Train loss at epoch (636):  0.039430108\n",
      "Test loss at epoch (636):  0.02073922\n",
      "Train loss at epoch (637):  0.03939998\n",
      "Test loss at epoch (637):  0.020645607\n",
      "Train loss at epoch (638):  0.039391767\n",
      "Test loss at epoch (638):  0.021658845\n",
      "Train loss at epoch (639):  0.039048593\n",
      "Test loss at epoch (639):  0.021504963\n",
      "Train loss at epoch (640):  0.03947337\n",
      "Test loss at epoch (640):  0.020612776\n",
      "Train loss at epoch (641):  0.039250474\n",
      "Test loss at epoch (641):  0.02084945\n",
      "Train loss at epoch (642):  0.03933305\n",
      "Test loss at epoch (642):  0.021921998\n",
      "Train loss at epoch (643):  0.039092205\n",
      "Test loss at epoch (643):  0.020892696\n",
      "Train loss at epoch (644):  0.039259315\n",
      "Test loss at epoch (644):  0.020399703\n",
      "Train loss at epoch (645):  0.03909071\n",
      "Test loss at epoch (645):  0.020551363\n",
      "Train loss at epoch (646):  0.03920025\n",
      "Test loss at epoch (646):  0.021028444\n",
      "Train loss at epoch (647):  0.039374888\n",
      "Test loss at epoch (647):  0.020353964\n",
      "Train loss at epoch (648):  0.039313413\n",
      "Test loss at epoch (648):  0.020961136\n",
      "Train loss at epoch (649):  0.03916719\n",
      "Test loss at epoch (649):  0.020715952\n",
      "Train loss at epoch (650):  0.039048813\n",
      "Test loss at epoch (650):  0.020616086\n",
      "Train loss at epoch (651):  0.039121184\n",
      "Test loss at epoch (651):  0.020572463\n",
      "Train loss at epoch (652):  0.039105803\n",
      "Test loss at epoch (652):  0.020536253\n",
      "Train loss at epoch (653):  0.039047\n",
      "Test loss at epoch (653):  0.020624587\n",
      "Train loss at epoch (654):  0.03903892\n",
      "Test loss at epoch (654):  0.021131769\n",
      "Train loss at epoch (655):  0.039144125\n",
      "Test loss at epoch (655):  0.020473754\n",
      "Train loss at epoch (656):  0.039049532\n",
      "Test loss at epoch (656):  0.021129977\n",
      "Train loss at epoch (657):  0.039065216\n",
      "Test loss at epoch (657):  0.021648161\n",
      "Train loss at epoch (658):  0.038974926\n",
      "Test loss at epoch (658):  0.021192148\n",
      "Train loss at epoch (659):  0.039091643\n",
      "Test loss at epoch (659):  0.021192573\n",
      "Train loss at epoch (660):  0.03903897\n",
      "Test loss at epoch (660):  0.021513507\n",
      "Train loss at epoch (661):  0.038876045\n",
      "Test loss at epoch (661):  0.020931533\n",
      "Train loss at epoch (662):  0.039152373\n",
      "Test loss at epoch (662):  0.02123884\n",
      "Train loss at epoch (663):  0.039001368\n",
      "Test loss at epoch (663):  0.020338265\n",
      "Train loss at epoch (664):  0.038728844\n",
      "Test loss at epoch (664):  0.02059491\n",
      "Train loss at epoch (665):  0.038764272\n",
      "Test loss at epoch (665):  0.020441884\n",
      "Train loss at epoch (666):  0.038802527\n",
      "Test loss at epoch (666):  0.020428406\n",
      "Train loss at epoch (667):  0.038808227\n",
      "Test loss at epoch (667):  0.02044217\n",
      "Train loss at epoch (668):  0.038906585\n",
      "Test loss at epoch (668):  0.020168666\n",
      "Train loss at epoch (669):  0.03881522\n",
      "Test loss at epoch (669):  0.020621303\n",
      "Train loss at epoch (670):  0.038847525\n",
      "Test loss at epoch (670):  0.020372005\n",
      "Train loss at epoch (671):  0.03888808\n",
      "Test loss at epoch (671):  0.020124603\n",
      "Train loss at epoch (672):  0.038832244\n",
      "Test loss at epoch (672):  0.020925336\n",
      "Train loss at epoch (673):  0.038451646\n",
      "Test loss at epoch (673):  0.020709267\n",
      "Train loss at epoch (674):  0.038974795\n",
      "Test loss at epoch (674):  0.021482274\n",
      "Train loss at epoch (675):  0.039078303\n",
      "Test loss at epoch (675):  0.020694962\n",
      "Train loss at epoch (676):  0.038577188\n",
      "Test loss at epoch (676):  0.020519638\n",
      "Train loss at epoch (677):  0.038767524\n",
      "Test loss at epoch (677):  0.020542046\n",
      "Train loss at epoch (678):  0.038791765\n",
      "Test loss at epoch (678):  0.022345703\n",
      "Train loss at epoch (679):  0.038567442\n",
      "Test loss at epoch (679):  0.019872526\n",
      "Train loss at epoch (680):  0.038512655\n",
      "Test loss at epoch (680):  0.02063693\n",
      "Train loss at epoch (681):  0.038650487\n",
      "Test loss at epoch (681):  0.020430135\n",
      "Train loss at epoch (682):  0.038568415\n",
      "Test loss at epoch (682):  0.021004278\n",
      "Train loss at epoch (683):  0.038730785\n",
      "Test loss at epoch (683):  0.02124366\n",
      "Train loss at epoch (684):  0.038594693\n",
      "Test loss at epoch (684):  0.020290274\n",
      "Train loss at epoch (685):  0.038311217\n",
      "Test loss at epoch (685):  0.02029602\n",
      "Train loss at epoch (686):  0.038443476\n",
      "Test loss at epoch (686):  0.020398071\n",
      "Train loss at epoch (687):  0.038510773\n",
      "Test loss at epoch (687):  0.020856932\n",
      "Train loss at epoch (688):  0.038654316\n",
      "Test loss at epoch (688):  0.020461908\n",
      "Train loss at epoch (689):  0.038694307\n",
      "Test loss at epoch (689):  0.020184\n",
      "Train loss at epoch (690):  0.038383994\n",
      "Test loss at epoch (690):  0.020018848\n",
      "Train loss at epoch (691):  0.038435817\n",
      "Test loss at epoch (691):  0.020305539\n",
      "Train loss at epoch (692):  0.038392562\n",
      "Test loss at epoch (692):  0.021014288\n",
      "Train loss at epoch (693):  0.03843343\n",
      "Test loss at epoch (693):  0.02039132\n",
      "Train loss at epoch (694):  0.03855665\n",
      "Test loss at epoch (694):  0.020598158\n",
      "Train loss at epoch (695):  0.038501445\n",
      "Test loss at epoch (695):  0.020534948\n",
      "Train loss at epoch (696):  0.038411308\n",
      "Test loss at epoch (696):  0.020169213\n",
      "Train loss at epoch (697):  0.03818237\n",
      "Test loss at epoch (697):  0.020372948\n",
      "Train loss at epoch (698):  0.038184166\n",
      "Test loss at epoch (698):  0.020029228\n",
      "Train loss at epoch (699):  0.03800129\n",
      "Test loss at epoch (699):  0.020427093\n",
      "Train loss at epoch (700):  0.03850095\n",
      "Test loss at epoch (700):  0.020237729\n",
      "Train loss at epoch (701):  0.03818034\n",
      "Test loss at epoch (701):  0.020914441\n",
      "Train loss at epoch (702):  0.03818316\n",
      "Test loss at epoch (702):  0.019943215\n",
      "Train loss at epoch (703):  0.038032196\n",
      "Test loss at epoch (703):  0.02068369\n",
      "Train loss at epoch (704):  0.038365062\n",
      "Test loss at epoch (704):  0.020357208\n",
      "Train loss at epoch (705):  0.038266174\n",
      "Test loss at epoch (705):  0.01998243\n",
      "Train loss at epoch (706):  0.03808064\n",
      "Test loss at epoch (706):  0.019849097\n",
      "Train loss at epoch (707):  0.037929278\n",
      "Test loss at epoch (707):  0.02104858\n",
      "Train loss at epoch (708):  0.038078196\n",
      "Test loss at epoch (708):  0.020277096\n",
      "Train loss at epoch (709):  0.038211484\n",
      "Test loss at epoch (709):  0.019852363\n",
      "Train loss at epoch (710):  0.038116213\n",
      "Test loss at epoch (710):  0.020117285\n",
      "Train loss at epoch (711):  0.038110998\n",
      "Test loss at epoch (711):  0.020091245\n",
      "Train loss at epoch (712):  0.03780981\n",
      "Test loss at epoch (712):  0.020782778\n",
      "Train loss at epoch (713):  0.037942965\n",
      "Test loss at epoch (713):  0.01978615\n",
      "Train loss at epoch (714):  0.037913784\n",
      "Test loss at epoch (714):  0.021247983\n",
      "Train loss at epoch (715):  0.038061995\n",
      "Test loss at epoch (715):  0.02033141\n",
      "Train loss at epoch (716):  0.037908915\n",
      "Test loss at epoch (716):  0.019925566\n",
      "Train loss at epoch (717):  0.038056575\n",
      "Test loss at epoch (717):  0.019983398\n",
      "Train loss at epoch (718):  0.038024224\n",
      "Test loss at epoch (718):  0.021640155\n",
      "Train loss at epoch (719):  0.03797761\n",
      "Test loss at epoch (719):  0.01980107\n",
      "Train loss at epoch (720):  0.0378811\n",
      "Test loss at epoch (720):  0.020117745\n",
      "Train loss at epoch (721):  0.037662793\n",
      "Test loss at epoch (721):  0.02025767\n",
      "Train loss at epoch (722):  0.037930522\n",
      "Test loss at epoch (722):  0.019862399\n",
      "Train loss at epoch (723):  0.03766305\n",
      "Test loss at epoch (723):  0.020311315\n",
      "Train loss at epoch (724):  0.037852094\n",
      "Test loss at epoch (724):  0.019791374\n",
      "Train loss at epoch (725):  0.037933983\n",
      "Test loss at epoch (725):  0.019911008\n",
      "Train loss at epoch (726):  0.037979294\n",
      "Test loss at epoch (726):  0.019926323\n",
      "Train loss at epoch (727):  0.037803173\n",
      "Test loss at epoch (727):  0.01975581\n",
      "Train loss at epoch (728):  0.037685905\n",
      "Test loss at epoch (728):  0.019909194\n",
      "Train loss at epoch (729):  0.03770888\n",
      "Test loss at epoch (729):  0.021644097\n",
      "Train loss at epoch (730):  0.03775195\n",
      "Test loss at epoch (730):  0.01982489\n",
      "Train loss at epoch (731):  0.037724584\n",
      "Test loss at epoch (731):  0.019659432\n",
      "Train loss at epoch (732):  0.037563514\n",
      "Test loss at epoch (732):  0.019841215\n",
      "Train loss at epoch (733):  0.037780315\n",
      "Test loss at epoch (733):  0.020241095\n",
      "Train loss at epoch (734):  0.037395183\n",
      "Test loss at epoch (734):  0.019829895\n",
      "Train loss at epoch (735):  0.03745566\n",
      "Test loss at epoch (735):  0.019917868\n",
      "Train loss at epoch (736):  0.037788894\n",
      "Test loss at epoch (736):  0.01972092\n",
      "Train loss at epoch (737):  0.03756271\n",
      "Test loss at epoch (737):  0.019916669\n",
      "Train loss at epoch (738):  0.037482705\n",
      "Test loss at epoch (738):  0.019990413\n",
      "Train loss at epoch (739):  0.037412677\n",
      "Test loss at epoch (739):  0.020063614\n",
      "Train loss at epoch (740):  0.03757464\n",
      "Test loss at epoch (740):  0.019929156\n",
      "Train loss at epoch (741):  0.037644442\n",
      "Test loss at epoch (741):  0.019748665\n",
      "Train loss at epoch (742):  0.037588436\n",
      "Test loss at epoch (742):  0.019986669\n",
      "Train loss at epoch (743):  0.037439875\n",
      "Test loss at epoch (743):  0.02038685\n",
      "Train loss at epoch (744):  0.037517358\n",
      "Test loss at epoch (744):  0.0203523\n",
      "Train loss at epoch (745):  0.0374596\n",
      "Test loss at epoch (745):  0.020027662\n",
      "Train loss at epoch (746):  0.037635766\n",
      "Test loss at epoch (746):  0.01988399\n",
      "Train loss at epoch (747):  0.037605993\n",
      "Test loss at epoch (747):  0.020192344\n",
      "Train loss at epoch (748):  0.037643936\n",
      "Test loss at epoch (748):  0.01962884\n",
      "Train loss at epoch (749):  0.03722474\n",
      "Test loss at epoch (749):  0.020115206\n",
      "Train loss at epoch (750):  0.037398715\n",
      "Test loss at epoch (750):  0.01961653\n",
      "Train loss at epoch (751):  0.037548333\n",
      "Test loss at epoch (751):  0.019851897\n",
      "Train loss at epoch (752):  0.037112907\n",
      "Test loss at epoch (752):  0.020308498\n",
      "Train loss at epoch (753):  0.037310652\n",
      "Test loss at epoch (753):  0.019839758\n",
      "Train loss at epoch (754):  0.037350245\n",
      "Test loss at epoch (754):  0.020099351\n",
      "Train loss at epoch (755):  0.03711101\n",
      "Test loss at epoch (755):  0.019694325\n",
      "Train loss at epoch (756):  0.03702541\n",
      "Test loss at epoch (756):  0.019563414\n",
      "Train loss at epoch (757):  0.037209023\n",
      "Test loss at epoch (757):  0.020121543\n",
      "Train loss at epoch (758):  0.03733024\n",
      "Test loss at epoch (758):  0.019956106\n",
      "Train loss at epoch (759):  0.037472505\n",
      "Test loss at epoch (759):  0.01981145\n",
      "Train loss at epoch (760):  0.037209854\n",
      "Test loss at epoch (760):  0.020472215\n",
      "Train loss at epoch (761):  0.03731918\n",
      "Test loss at epoch (761):  0.019637747\n",
      "Train loss at epoch (762):  0.037286606\n",
      "Test loss at epoch (762):  0.019644419\n",
      "Train loss at epoch (763):  0.037065405\n",
      "Test loss at epoch (763):  0.019953849\n",
      "Train loss at epoch (764):  0.037194002\n",
      "Test loss at epoch (764):  0.020191843\n",
      "Train loss at epoch (765):  0.037213653\n",
      "Test loss at epoch (765):  0.019717816\n",
      "Train loss at epoch (766):  0.037198696\n",
      "Test loss at epoch (766):  0.02012205\n",
      "Train loss at epoch (767):  0.03710631\n",
      "Test loss at epoch (767):  0.019463649\n",
      "Train loss at epoch (768):  0.036965534\n",
      "Test loss at epoch (768):  0.02039214\n",
      "Train loss at epoch (769):  0.03707901\n",
      "Test loss at epoch (769):  0.019789115\n",
      "Train loss at epoch (770):  0.037374258\n",
      "Test loss at epoch (770):  0.019583657\n",
      "Train loss at epoch (771):  0.036956426\n",
      "Test loss at epoch (771):  0.020198999\n",
      "Train loss at epoch (772):  0.037032146\n",
      "Test loss at epoch (772):  0.01943007\n",
      "Train loss at epoch (773):  0.03708862\n",
      "Test loss at epoch (773):  0.019440629\n",
      "Train loss at epoch (774):  0.036970958\n",
      "Test loss at epoch (774):  0.020353928\n",
      "Train loss at epoch (775):  0.037066992\n",
      "Test loss at epoch (775):  0.019857867\n",
      "Train loss at epoch (776):  0.036735058\n",
      "Test loss at epoch (776):  0.020181106\n",
      "Train loss at epoch (777):  0.037112582\n",
      "Test loss at epoch (777):  0.020862965\n",
      "Train loss at epoch (778):  0.036867835\n",
      "Test loss at epoch (778):  0.020211399\n",
      "Train loss at epoch (779):  0.03704735\n",
      "Test loss at epoch (779):  0.020046426\n",
      "Train loss at epoch (780):  0.037072465\n",
      "Test loss at epoch (780):  0.01955003\n",
      "Train loss at epoch (781):  0.037174813\n",
      "Test loss at epoch (781):  0.01958603\n",
      "Train loss at epoch (782):  0.037057873\n",
      "Test loss at epoch (782):  0.019572236\n",
      "Train loss at epoch (783):  0.036739532\n",
      "Test loss at epoch (783):  0.019385125\n",
      "Train loss at epoch (784):  0.036822136\n",
      "Test loss at epoch (784):  0.019928932\n",
      "Train loss at epoch (785):  0.036872637\n",
      "Test loss at epoch (785):  0.019708065\n",
      "Train loss at epoch (786):  0.03683499\n",
      "Test loss at epoch (786):  0.020339275\n",
      "Train loss at epoch (787):  0.03694209\n",
      "Test loss at epoch (787):  0.01935428\n",
      "Train loss at epoch (788):  0.03700686\n",
      "Test loss at epoch (788):  0.020024933\n",
      "Train loss at epoch (789):  0.036830794\n",
      "Test loss at epoch (789):  0.019768449\n",
      "Train loss at epoch (790):  0.036826883\n",
      "Test loss at epoch (790):  0.020286635\n",
      "Train loss at epoch (791):  0.03681676\n",
      "Test loss at epoch (791):  0.019665062\n",
      "Train loss at epoch (792):  0.036972664\n",
      "Test loss at epoch (792):  0.019421773\n",
      "Train loss at epoch (793):  0.036764305\n",
      "Test loss at epoch (793):  0.020336794\n",
      "Train loss at epoch (794):  0.036546282\n",
      "Test loss at epoch (794):  0.02001424\n",
      "Train loss at epoch (795):  0.036724895\n",
      "Test loss at epoch (795):  0.020125728\n",
      "Train loss at epoch (796):  0.036682878\n",
      "Test loss at epoch (796):  0.019470008\n",
      "Train loss at epoch (797):  0.03652501\n",
      "Test loss at epoch (797):  0.01988664\n",
      "Train loss at epoch (798):  0.03662569\n",
      "Test loss at epoch (798):  0.019296939\n",
      "Train loss at epoch (799):  0.036546517\n",
      "Test loss at epoch (799):  0.019408034\n",
      "Train loss at epoch (800):  0.03649585\n",
      "Test loss at epoch (800):  0.019816553\n",
      "Train loss at epoch (801):  0.03646234\n",
      "Test loss at epoch (801):  0.019848064\n",
      "Train loss at epoch (802):  0.036503278\n",
      "Test loss at epoch (802):  0.019180238\n",
      "Train loss at epoch (803):  0.036549453\n",
      "Test loss at epoch (803):  0.01956766\n",
      "Train loss at epoch (804):  0.036650747\n",
      "Test loss at epoch (804):  0.019830897\n",
      "Train loss at epoch (805):  0.036602337\n",
      "Test loss at epoch (805):  0.019246321\n",
      "Train loss at epoch (806):  0.036568046\n",
      "Test loss at epoch (806):  0.019551272\n",
      "Train loss at epoch (807):  0.03644256\n",
      "Test loss at epoch (807):  0.019463724\n",
      "Train loss at epoch (808):  0.036638733\n",
      "Test loss at epoch (808):  0.019554907\n",
      "Train loss at epoch (809):  0.036527086\n",
      "Test loss at epoch (809):  0.019431358\n",
      "Train loss at epoch (810):  0.036356386\n",
      "Test loss at epoch (810):  0.019935047\n",
      "Train loss at epoch (811):  0.03652978\n",
      "Test loss at epoch (811):  0.019540358\n",
      "Train loss at epoch (812):  0.036219276\n",
      "Test loss at epoch (812):  0.019344324\n",
      "Train loss at epoch (813):  0.036497265\n",
      "Test loss at epoch (813):  0.019474573\n",
      "Train loss at epoch (814):  0.03665693\n",
      "Test loss at epoch (814):  0.019574743\n",
      "Train loss at epoch (815):  0.036489252\n",
      "Test loss at epoch (815):  0.019843824\n",
      "Train loss at epoch (816):  0.03657019\n",
      "Test loss at epoch (816):  0.019288396\n",
      "Train loss at epoch (817):  0.03641468\n",
      "Test loss at epoch (817):  0.019525899\n",
      "Train loss at epoch (818):  0.036277015\n",
      "Test loss at epoch (818):  0.019180067\n",
      "Train loss at epoch (819):  0.036353834\n",
      "Test loss at epoch (819):  0.01960361\n",
      "Train loss at epoch (820):  0.036349744\n",
      "Test loss at epoch (820):  0.019609356\n",
      "Train loss at epoch (821):  0.036519196\n",
      "Test loss at epoch (821):  0.019419799\n",
      "Train loss at epoch (822):  0.03619275\n",
      "Test loss at epoch (822):  0.019717446\n",
      "Train loss at epoch (823):  0.036341134\n",
      "Test loss at epoch (823):  0.01934148\n",
      "Train loss at epoch (824):  0.036222424\n",
      "Test loss at epoch (824):  0.019042598\n",
      "Train loss at epoch (825):  0.036267996\n",
      "Test loss at epoch (825):  0.019340187\n",
      "Train loss at epoch (826):  0.036239278\n",
      "Test loss at epoch (826):  0.01946286\n",
      "Train loss at epoch (827):  0.036230735\n",
      "Test loss at epoch (827):  0.019490248\n",
      "Train loss at epoch (828):  0.036306392\n",
      "Test loss at epoch (828):  0.01905875\n",
      "Train loss at epoch (829):  0.036134504\n",
      "Test loss at epoch (829):  0.01929049\n",
      "Train loss at epoch (830):  0.036041968\n",
      "Test loss at epoch (830):  0.01956363\n",
      "Train loss at epoch (831):  0.036241554\n",
      "Test loss at epoch (831):  0.019475901\n",
      "Train loss at epoch (832):  0.036325928\n",
      "Test loss at epoch (832):  0.019276291\n",
      "Train loss at epoch (833):  0.03616214\n",
      "Test loss at epoch (833):  0.019535279\n",
      "Train loss at epoch (834):  0.03623612\n",
      "Test loss at epoch (834):  0.019170403\n",
      "Train loss at epoch (835):  0.035988417\n",
      "Test loss at epoch (835):  0.019350583\n",
      "Train loss at epoch (836):  0.035988167\n",
      "Test loss at epoch (836):  0.01914054\n",
      "Train loss at epoch (837):  0.036081713\n",
      "Test loss at epoch (837):  0.01966617\n",
      "Train loss at epoch (838):  0.036142815\n",
      "Test loss at epoch (838):  0.019483376\n",
      "Train loss at epoch (839):  0.035924\n",
      "Test loss at epoch (839):  0.020090241\n",
      "Train loss at epoch (840):  0.036048032\n",
      "Test loss at epoch (840):  0.019254478\n",
      "Train loss at epoch (841):  0.036206584\n",
      "Test loss at epoch (841):  0.019398486\n",
      "Train loss at epoch (842):  0.036129035\n",
      "Test loss at epoch (842):  0.019149093\n",
      "Train loss at epoch (843):  0.03601387\n",
      "Test loss at epoch (843):  0.019204525\n",
      "Train loss at epoch (844):  0.035854485\n",
      "Test loss at epoch (844):  0.019277358\n",
      "Train loss at epoch (845):  0.036060315\n",
      "Test loss at epoch (845):  0.019079816\n",
      "Train loss at epoch (846):  0.035789307\n",
      "Test loss at epoch (846):  0.019646063\n",
      "Train loss at epoch (847):  0.035840306\n",
      "Test loss at epoch (847):  0.019219648\n",
      "Train loss at epoch (848):  0.03601305\n",
      "Test loss at epoch (848):  0.019182697\n",
      "Train loss at epoch (849):  0.03602266\n",
      "Test loss at epoch (849):  0.01916453\n",
      "Train loss at epoch (850):  0.03605661\n",
      "Test loss at epoch (850):  0.019083207\n",
      "Train loss at epoch (851):  0.035680156\n",
      "Test loss at epoch (851):  0.0191078\n",
      "Train loss at epoch (852):  0.035799254\n",
      "Test loss at epoch (852):  0.018998494\n",
      "Train loss at epoch (853):  0.035947632\n",
      "Test loss at epoch (853):  0.019209012\n",
      "Train loss at epoch (854):  0.036223713\n",
      "Test loss at epoch (854):  0.019193258\n",
      "Train loss at epoch (855):  0.03592974\n",
      "Test loss at epoch (855):  0.01915391\n",
      "Train loss at epoch (856):  0.035777666\n",
      "Test loss at epoch (856):  0.019349828\n",
      "Train loss at epoch (857):  0.036022447\n",
      "Test loss at epoch (857):  0.019012941\n",
      "Train loss at epoch (858):  0.035838816\n",
      "Test loss at epoch (858):  0.019580768\n",
      "Train loss at epoch (859):  0.03607539\n",
      "Test loss at epoch (859):  0.019484723\n",
      "Train loss at epoch (860):  0.035567638\n",
      "Test loss at epoch (860):  0.01939603\n",
      "Train loss at epoch (861):  0.035676293\n",
      "Test loss at epoch (861):  0.018951906\n",
      "Train loss at epoch (862):  0.03575675\n",
      "Test loss at epoch (862):  0.019491632\n",
      "Train loss at epoch (863):  0.035572756\n",
      "Test loss at epoch (863):  0.019393254\n",
      "Train loss at epoch (864):  0.03575622\n",
      "Test loss at epoch (864):  0.019203572\n",
      "Train loss at epoch (865):  0.03562551\n",
      "Test loss at epoch (865):  0.019848006\n",
      "Train loss at epoch (866):  0.035705075\n",
      "Test loss at epoch (866):  0.018853277\n",
      "Train loss at epoch (867):  0.035691414\n",
      "Test loss at epoch (867):  0.019091332\n",
      "Train loss at epoch (868):  0.035629317\n",
      "Test loss at epoch (868):  0.01986816\n",
      "Train loss at epoch (869):  0.03576334\n",
      "Test loss at epoch (869):  0.0193636\n",
      "Train loss at epoch (870):  0.035780367\n",
      "Test loss at epoch (870):  0.019631559\n",
      "Train loss at epoch (871):  0.035637986\n",
      "Test loss at epoch (871):  0.019220946\n",
      "Train loss at epoch (872):  0.03585962\n",
      "Test loss at epoch (872):  0.019333169\n",
      "Train loss at epoch (873):  0.035654847\n",
      "Test loss at epoch (873):  0.01896054\n",
      "Train loss at epoch (874):  0.035646297\n",
      "Test loss at epoch (874):  0.019817984\n",
      "Train loss at epoch (875):  0.03550779\n",
      "Test loss at epoch (875):  0.01949585\n",
      "Train loss at epoch (876):  0.0357\n",
      "Test loss at epoch (876):  0.018948562\n",
      "Train loss at epoch (877):  0.03563731\n",
      "Test loss at epoch (877):  0.019105611\n",
      "Train loss at epoch (878):  0.03565696\n",
      "Test loss at epoch (878):  0.01911244\n",
      "Train loss at epoch (879):  0.035601463\n",
      "Test loss at epoch (879):  0.019251645\n",
      "Train loss at epoch (880):  0.0356004\n",
      "Test loss at epoch (880):  0.019288182\n",
      "Train loss at epoch (881):  0.03549309\n",
      "Test loss at epoch (881):  0.0193423\n",
      "Train loss at epoch (882):  0.035382155\n",
      "Test loss at epoch (882):  0.019346341\n",
      "Train loss at epoch (883):  0.035564136\n",
      "Test loss at epoch (883):  0.019372474\n",
      "Train loss at epoch (884):  0.035510916\n",
      "Test loss at epoch (884):  0.018700685\n",
      "Train loss at epoch (885):  0.035283227\n",
      "Test loss at epoch (885):  0.018840296\n",
      "Train loss at epoch (886):  0.035531934\n",
      "Test loss at epoch (886):  0.019323137\n",
      "Train loss at epoch (887):  0.035520323\n",
      "Test loss at epoch (887):  0.018963153\n",
      "Train loss at epoch (888):  0.035512414\n",
      "Test loss at epoch (888):  0.018918898\n",
      "Train loss at epoch (889):  0.03537446\n",
      "Test loss at epoch (889):  0.01891112\n",
      "Train loss at epoch (890):  0.035520718\n",
      "Test loss at epoch (890):  0.01887035\n",
      "Train loss at epoch (891):  0.035507128\n",
      "Test loss at epoch (891):  0.01887632\n",
      "Train loss at epoch (892):  0.03524623\n",
      "Test loss at epoch (892):  0.01877313\n",
      "Train loss at epoch (893):  0.035412055\n",
      "Test loss at epoch (893):  0.019136198\n",
      "Train loss at epoch (894):  0.035417147\n",
      "Test loss at epoch (894):  0.018786307\n",
      "Train loss at epoch (895):  0.035320435\n",
      "Test loss at epoch (895):  0.019049278\n",
      "Train loss at epoch (896):  0.035164457\n",
      "Test loss at epoch (896):  0.018617924\n",
      "Train loss at epoch (897):  0.035308227\n",
      "Test loss at epoch (897):  0.019485798\n",
      "Train loss at epoch (898):  0.03527288\n",
      "Test loss at epoch (898):  0.018840227\n",
      "Train loss at epoch (899):  0.035322223\n",
      "Test loss at epoch (899):  0.018747792\n",
      "Train loss at epoch (900):  0.035304055\n",
      "Test loss at epoch (900):  0.018660428\n",
      "Train loss at epoch (901):  0.03525666\n",
      "Test loss at epoch (901):  0.019025331\n",
      "Train loss at epoch (902):  0.035292808\n",
      "Test loss at epoch (902):  0.019115737\n",
      "Train loss at epoch (903):  0.03522371\n",
      "Test loss at epoch (903):  0.018659092\n",
      "Train loss at epoch (904):  0.03515015\n",
      "Test loss at epoch (904):  0.01894409\n",
      "Train loss at epoch (905):  0.035259366\n",
      "Test loss at epoch (905):  0.0190218\n",
      "Train loss at epoch (906):  0.035129525\n",
      "Test loss at epoch (906):  0.018906424\n",
      "Train loss at epoch (907):  0.03522173\n",
      "Test loss at epoch (907):  0.01865164\n",
      "Train loss at epoch (908):  0.03533372\n",
      "Test loss at epoch (908):  0.018931987\n",
      "Train loss at epoch (909):  0.035142586\n",
      "Test loss at epoch (909):  0.018841133\n",
      "Train loss at epoch (910):  0.035066027\n",
      "Test loss at epoch (910):  0.01889998\n",
      "Train loss at epoch (911):  0.034980237\n",
      "Test loss at epoch (911):  0.018899366\n",
      "Train loss at epoch (912):  0.035042558\n",
      "Test loss at epoch (912):  0.01892035\n",
      "Train loss at epoch (913):  0.035205487\n",
      "Test loss at epoch (913):  0.019006943\n",
      "Train loss at epoch (914):  0.034985725\n",
      "Test loss at epoch (914):  0.01872387\n",
      "Train loss at epoch (915):  0.03525735\n",
      "Test loss at epoch (915):  0.018742647\n",
      "Train loss at epoch (916):  0.035132635\n",
      "Test loss at epoch (916):  0.018826256\n",
      "Train loss at epoch (917):  0.03479678\n",
      "Test loss at epoch (917):  0.018669879\n",
      "Train loss at epoch (918):  0.035121538\n",
      "Test loss at epoch (918):  0.01929629\n",
      "Train loss at epoch (919):  0.034922592\n",
      "Test loss at epoch (919):  0.018546082\n",
      "Train loss at epoch (920):  0.03498926\n",
      "Test loss at epoch (920):  0.018613929\n",
      "Train loss at epoch (921):  0.03495508\n",
      "Test loss at epoch (921):  0.01863183\n",
      "Train loss at epoch (922):  0.03489168\n",
      "Test loss at epoch (922):  0.018578934\n",
      "Train loss at epoch (923):  0.03510907\n",
      "Test loss at epoch (923):  0.018614715\n",
      "Train loss at epoch (924):  0.03485967\n",
      "Test loss at epoch (924):  0.018624976\n",
      "Train loss at epoch (925):  0.035243094\n",
      "Test loss at epoch (925):  0.019401997\n",
      "Train loss at epoch (926):  0.035176348\n",
      "Test loss at epoch (926):  0.018963436\n",
      "Train loss at epoch (927):  0.03528876\n",
      "Test loss at epoch (927):  0.018745095\n",
      "Train loss at epoch (928):  0.03480113\n",
      "Test loss at epoch (928):  0.018905073\n",
      "Train loss at epoch (929):  0.034858953\n",
      "Test loss at epoch (929):  0.018826738\n",
      "Train loss at epoch (930):  0.034914304\n",
      "Test loss at epoch (930):  0.018999748\n",
      "Train loss at epoch (931):  0.03477855\n",
      "Test loss at epoch (931):  0.018950561\n",
      "Train loss at epoch (932):  0.03488034\n",
      "Test loss at epoch (932):  0.018822733\n",
      "Train loss at epoch (933):  0.035014447\n",
      "Test loss at epoch (933):  0.018772695\n",
      "Train loss at epoch (934):  0.034883667\n",
      "Test loss at epoch (934):  0.018572414\n",
      "Train loss at epoch (935):  0.03513639\n",
      "Test loss at epoch (935):  0.018641645\n",
      "Train loss at epoch (936):  0.034801893\n",
      "Test loss at epoch (936):  0.018484846\n",
      "Train loss at epoch (937):  0.034617778\n",
      "Test loss at epoch (937):  0.018610293\n",
      "Train loss at epoch (938):  0.03480342\n",
      "Test loss at epoch (938):  0.018982114\n",
      "Train loss at epoch (939):  0.034786407\n",
      "Test loss at epoch (939):  0.018608335\n",
      "Train loss at epoch (940):  0.034696374\n",
      "Test loss at epoch (940):  0.018697781\n",
      "Train loss at epoch (941):  0.03463343\n",
      "Test loss at epoch (941):  0.0186066\n",
      "Train loss at epoch (942):  0.034597803\n",
      "Test loss at epoch (942):  0.01891556\n",
      "Train loss at epoch (943):  0.034833618\n",
      "Test loss at epoch (943):  0.018609298\n",
      "Train loss at epoch (944):  0.034635887\n",
      "Test loss at epoch (944):  0.018696642\n",
      "Train loss at epoch (945):  0.03481599\n",
      "Test loss at epoch (945):  0.01837312\n",
      "Train loss at epoch (946):  0.03468088\n",
      "Test loss at epoch (946):  0.01885605\n",
      "Train loss at epoch (947):  0.034681283\n",
      "Test loss at epoch (947):  0.018735027\n",
      "Train loss at epoch (948):  0.03478519\n",
      "Test loss at epoch (948):  0.01857561\n",
      "Train loss at epoch (949):  0.034712404\n",
      "Test loss at epoch (949):  0.01839615\n",
      "Train loss at epoch (950):  0.03454616\n",
      "Test loss at epoch (950):  0.019057976\n",
      "Train loss at epoch (951):  0.03465295\n",
      "Test loss at epoch (951):  0.01857743\n",
      "Train loss at epoch (952):  0.034575183\n",
      "Test loss at epoch (952):  0.018325254\n",
      "Train loss at epoch (953):  0.034688134\n",
      "Test loss at epoch (953):  0.01849915\n",
      "Train loss at epoch (954):  0.034648474\n",
      "Test loss at epoch (954):  0.018679094\n",
      "Train loss at epoch (955):  0.034521896\n",
      "Test loss at epoch (955):  0.018449238\n",
      "Train loss at epoch (956):  0.034622353\n",
      "Test loss at epoch (956):  0.018636875\n",
      "Train loss at epoch (957):  0.034570795\n",
      "Test loss at epoch (957):  0.018809011\n",
      "Train loss at epoch (958):  0.034384705\n",
      "Test loss at epoch (958):  0.019123202\n",
      "Train loss at epoch (959):  0.03459856\n",
      "Test loss at epoch (959):  0.018894332\n",
      "Train loss at epoch (960):  0.034602284\n",
      "Test loss at epoch (960):  0.018891249\n",
      "Train loss at epoch (961):  0.03452991\n",
      "Test loss at epoch (961):  0.018760184\n",
      "Train loss at epoch (962):  0.034390017\n",
      "Test loss at epoch (962):  0.018356182\n",
      "Train loss at epoch (963):  0.03435499\n",
      "Test loss at epoch (963):  0.018433979\n",
      "Train loss at epoch (964):  0.03447089\n",
      "Test loss at epoch (964):  0.018867606\n",
      "Train loss at epoch (965):  0.03457374\n",
      "Test loss at epoch (965):  0.018648215\n",
      "Train loss at epoch (966):  0.034406196\n",
      "Test loss at epoch (966):  0.018581802\n",
      "Train loss at epoch (967):  0.03424691\n",
      "Test loss at epoch (967):  0.01879455\n",
      "Train loss at epoch (968):  0.034357376\n",
      "Test loss at epoch (968):  0.019063586\n",
      "Train loss at epoch (969):  0.03459213\n",
      "Test loss at epoch (969):  0.01876011\n",
      "Train loss at epoch (970):  0.034361094\n",
      "Test loss at epoch (970):  0.018581457\n",
      "Train loss at epoch (971):  0.03420212\n",
      "Test loss at epoch (971):  0.018585956\n",
      "Train loss at epoch (972):  0.034170743\n",
      "Test loss at epoch (972):  0.018573886\n",
      "Train loss at epoch (973):  0.03449486\n",
      "Test loss at epoch (973):  0.018436592\n",
      "Train loss at epoch (974):  0.03433483\n",
      "Test loss at epoch (974):  0.018394427\n",
      "Train loss at epoch (975):  0.034358896\n",
      "Test loss at epoch (975):  0.0186586\n",
      "Train loss at epoch (976):  0.03455992\n",
      "Test loss at epoch (976):  0.01898927\n",
      "Train loss at epoch (977):  0.034277305\n",
      "Test loss at epoch (977):  0.018431608\n",
      "Train loss at epoch (978):  0.03428588\n",
      "Test loss at epoch (978):  0.018617572\n",
      "Train loss at epoch (979):  0.034274038\n",
      "Test loss at epoch (979):  0.018341158\n",
      "Train loss at epoch (980):  0.0341932\n",
      "Test loss at epoch (980):  0.018423267\n",
      "Train loss at epoch (981):  0.034167282\n",
      "Test loss at epoch (981):  0.01838206\n",
      "Train loss at epoch (982):  0.034280352\n",
      "Test loss at epoch (982):  0.018542362\n",
      "Train loss at epoch (983):  0.03422814\n",
      "Test loss at epoch (983):  0.018370356\n",
      "Train loss at epoch (984):  0.034394164\n",
      "Test loss at epoch (984):  0.018444385\n",
      "Train loss at epoch (985):  0.034042206\n",
      "Test loss at epoch (985):  0.018435117\n",
      "Train loss at epoch (986):  0.034271196\n",
      "Test loss at epoch (986):  0.018814847\n",
      "Train loss at epoch (987):  0.03430816\n",
      "Test loss at epoch (987):  0.01856653\n",
      "Train loss at epoch (988):  0.03436148\n",
      "Test loss at epoch (988):  0.018326078\n",
      "Train loss at epoch (989):  0.034214552\n",
      "Test loss at epoch (989):  0.018625781\n",
      "Train loss at epoch (990):  0.034004375\n",
      "Test loss at epoch (990):  0.018715585\n",
      "Train loss at epoch (991):  0.034286644\n",
      "Test loss at epoch (991):  0.0185278\n",
      "Train loss at epoch (992):  0.03411776\n",
      "Test loss at epoch (992):  0.018298782\n",
      "Train loss at epoch (993):  0.034088634\n",
      "Test loss at epoch (993):  0.0184613\n",
      "Train loss at epoch (994):  0.034046296\n",
      "Test loss at epoch (994):  0.018293569\n",
      "Train loss at epoch (995):  0.034064006\n",
      "Test loss at epoch (995):  0.018386059\n",
      "Train loss at epoch (996):  0.034196187\n",
      "Test loss at epoch (996):  0.01830466\n",
      "Train loss at epoch (997):  0.034101132\n",
      "Test loss at epoch (997):  0.018340752\n",
      "Train loss at epoch (998):  0.034205217\n",
      "Test loss at epoch (998):  0.01880328\n",
      "Train loss at epoch (999):  0.0340526\n",
      "Test loss at epoch (999):  0.018312069\n",
      "Train loss at epoch (1000):  0.034045912\n",
      "Test loss at epoch (1000):  0.018692132\n",
      "Train loss at epoch (1001):  0.034140985\n",
      "Test loss at epoch (1001):  0.018550359\n",
      "Train loss at epoch (1002):  0.034233972\n",
      "Test loss at epoch (1002):  0.018488372\n",
      "Train loss at epoch (1003):  0.033945587\n",
      "Test loss at epoch (1003):  0.018421132\n",
      "Train loss at epoch (1004):  0.034161933\n",
      "Test loss at epoch (1004):  0.018292882\n",
      "Train loss at epoch (1005):  0.034114398\n",
      "Test loss at epoch (1005):  0.018593658\n",
      "Train loss at epoch (1006):  0.03421368\n",
      "Test loss at epoch (1006):  0.018512717\n",
      "Train loss at epoch (1007):  0.03404614\n",
      "Test loss at epoch (1007):  0.019080985\n",
      "Train loss at epoch (1008):  0.03403164\n",
      "Test loss at epoch (1008):  0.01869075\n",
      "Train loss at epoch (1009):  0.033773594\n",
      "Test loss at epoch (1009):  0.018327517\n",
      "Train loss at epoch (1010):  0.03399562\n",
      "Test loss at epoch (1010):  0.01816038\n",
      "Train loss at epoch (1011):  0.03374562\n",
      "Test loss at epoch (1011):  0.018311994\n",
      "Train loss at epoch (1012):  0.034023188\n",
      "Test loss at epoch (1012):  0.018576786\n",
      "Train loss at epoch (1013):  0.034067538\n",
      "Test loss at epoch (1013):  0.018268123\n",
      "Train loss at epoch (1014):  0.033864617\n",
      "Test loss at epoch (1014):  0.018475085\n",
      "Train loss at epoch (1015):  0.033842497\n",
      "Test loss at epoch (1015):  0.018183328\n",
      "Train loss at epoch (1016):  0.033918347\n",
      "Test loss at epoch (1016):  0.018519975\n",
      "Train loss at epoch (1017):  0.03386532\n",
      "Test loss at epoch (1017):  0.01826651\n",
      "Train loss at epoch (1018):  0.033716574\n",
      "Test loss at epoch (1018):  0.018176002\n",
      "Train loss at epoch (1019):  0.033814553\n",
      "Test loss at epoch (1019):  0.018301371\n",
      "Train loss at epoch (1020):  0.03380507\n",
      "Test loss at epoch (1020):  0.018297449\n",
      "Train loss at epoch (1021):  0.03386899\n",
      "Test loss at epoch (1021):  0.01846897\n",
      "Train loss at epoch (1022):  0.033781927\n",
      "Test loss at epoch (1022):  0.018007902\n",
      "Train loss at epoch (1023):  0.033716347\n",
      "Test loss at epoch (1023):  0.01820904\n",
      "Train loss at epoch (1024):  0.033585075\n",
      "Test loss at epoch (1024):  0.018442424\n",
      "Train loss at epoch (1025):  0.03366955\n",
      "Test loss at epoch (1025):  0.018994652\n",
      "Train loss at epoch (1026):  0.03386324\n",
      "Test loss at epoch (1026):  0.018653575\n",
      "Train loss at epoch (1027):  0.033808604\n",
      "Test loss at epoch (1027):  0.018152898\n",
      "Train loss at epoch (1028):  0.033714645\n",
      "Test loss at epoch (1028):  0.018158855\n",
      "Train loss at epoch (1029):  0.03359891\n",
      "Test loss at epoch (1029):  0.018167557\n",
      "Train loss at epoch (1030):  0.03352917\n",
      "Test loss at epoch (1030):  0.018188661\n",
      "Train loss at epoch (1031):  0.033778556\n",
      "Test loss at epoch (1031):  0.018424496\n",
      "Train loss at epoch (1032):  0.033726715\n",
      "Test loss at epoch (1032):  0.01803257\n",
      "Train loss at epoch (1033):  0.03378028\n",
      "Test loss at epoch (1033):  0.018089997\n",
      "Train loss at epoch (1034):  0.033710748\n",
      "Test loss at epoch (1034):  0.018544221\n",
      "Train loss at epoch (1035):  0.03377826\n",
      "Test loss at epoch (1035):  0.018510561\n",
      "Train loss at epoch (1036):  0.033640653\n",
      "Test loss at epoch (1036):  0.018759795\n",
      "Train loss at epoch (1037):  0.033530924\n",
      "Test loss at epoch (1037):  0.01848153\n",
      "Train loss at epoch (1038):  0.033754624\n",
      "Test loss at epoch (1038):  0.018149192\n",
      "Train loss at epoch (1039):  0.033548288\n",
      "Test loss at epoch (1039):  0.018038401\n",
      "Train loss at epoch (1040):  0.03364181\n",
      "Test loss at epoch (1040):  0.018054465\n",
      "Train loss at epoch (1041):  0.033513613\n",
      "Test loss at epoch (1041):  0.018195871\n",
      "Train loss at epoch (1042):  0.033610452\n",
      "Test loss at epoch (1042):  0.018272437\n",
      "Train loss at epoch (1043):  0.033528663\n",
      "Test loss at epoch (1043):  0.018528184\n",
      "Train loss at epoch (1044):  0.033584356\n",
      "Test loss at epoch (1044):  0.018455794\n",
      "Train loss at epoch (1045):  0.033681057\n",
      "Test loss at epoch (1045):  0.018068397\n",
      "Train loss at epoch (1046):  0.033415984\n",
      "Test loss at epoch (1046):  0.018291753\n",
      "Train loss at epoch (1047):  0.033497445\n",
      "Test loss at epoch (1047):  0.018160457\n",
      "Train loss at epoch (1048):  0.03364518\n",
      "Test loss at epoch (1048):  0.01804966\n",
      "Train loss at epoch (1049):  0.03355766\n",
      "Test loss at epoch (1049):  0.018137125\n",
      "Train loss at epoch (1050):  0.033630177\n",
      "Test loss at epoch (1050):  0.018140633\n",
      "Train loss at epoch (1051):  0.033582553\n",
      "Test loss at epoch (1051):  0.018776549\n",
      "Train loss at epoch (1052):  0.033469334\n",
      "Test loss at epoch (1052):  0.018227864\n",
      "Train loss at epoch (1053):  0.033377495\n",
      "Test loss at epoch (1053):  0.018077994\n",
      "Train loss at epoch (1054):  0.03351212\n",
      "Test loss at epoch (1054):  0.018114787\n",
      "Train loss at epoch (1055):  0.03330604\n",
      "Test loss at epoch (1055):  0.0180625\n",
      "Train loss at epoch (1056):  0.033587553\n",
      "Test loss at epoch (1056):  0.018334245\n",
      "Train loss at epoch (1057):  0.033459276\n",
      "Test loss at epoch (1057):  0.018061602\n",
      "Train loss at epoch (1058):  0.033398334\n",
      "Test loss at epoch (1058):  0.018536726\n",
      "Train loss at epoch (1059):  0.03345231\n",
      "Test loss at epoch (1059):  0.01797548\n",
      "Train loss at epoch (1060):  0.03336651\n",
      "Test loss at epoch (1060):  0.018155077\n",
      "Train loss at epoch (1061):  0.033476274\n",
      "Test loss at epoch (1061):  0.01821944\n",
      "Train loss at epoch (1062):  0.03337238\n",
      "Test loss at epoch (1062):  0.018400475\n",
      "Train loss at epoch (1063):  0.033512753\n",
      "Test loss at epoch (1063):  0.018201752\n",
      "Train loss at epoch (1064):  0.033129856\n",
      "Test loss at epoch (1064):  0.018113937\n",
      "Train loss at epoch (1065):  0.033382803\n",
      "Test loss at epoch (1065):  0.018151497\n",
      "Train loss at epoch (1066):  0.03319197\n",
      "Test loss at epoch (1066):  0.018215915\n",
      "Train loss at epoch (1067):  0.033404317\n",
      "Test loss at epoch (1067):  0.018114273\n",
      "Train loss at epoch (1068):  0.03352639\n",
      "Test loss at epoch (1068):  0.018021222\n",
      "Train loss at epoch (1069):  0.033278987\n",
      "Test loss at epoch (1069):  0.018052632\n",
      "Train loss at epoch (1070):  0.033315822\n",
      "Test loss at epoch (1070):  0.01834641\n",
      "Train loss at epoch (1071):  0.0331857\n",
      "Test loss at epoch (1071):  0.018227294\n",
      "Train loss at epoch (1072):  0.03305876\n",
      "Test loss at epoch (1072):  0.017926684\n",
      "Train loss at epoch (1073):  0.0330411\n",
      "Test loss at epoch (1073):  0.018316371\n",
      "Train loss at epoch (1074):  0.03328541\n",
      "Test loss at epoch (1074):  0.018150311\n",
      "Train loss at epoch (1075):  0.033307195\n",
      "Test loss at epoch (1075):  0.018358335\n",
      "Train loss at epoch (1076):  0.03328772\n",
      "Test loss at epoch (1076):  0.018111037\n",
      "Train loss at epoch (1077):  0.033288658\n",
      "Test loss at epoch (1077):  0.018083679\n",
      "Train loss at epoch (1078):  0.033303466\n",
      "Test loss at epoch (1078):  0.018011723\n",
      "Train loss at epoch (1079):  0.0330608\n",
      "Test loss at epoch (1079):  0.018236957\n",
      "Train loss at epoch (1080):  0.03313349\n",
      "Test loss at epoch (1080):  0.018269068\n",
      "Train loss at epoch (1081):  0.03320974\n",
      "Test loss at epoch (1081):  0.018002445\n",
      "Train loss at epoch (1082):  0.033211555\n",
      "Test loss at epoch (1082):  0.018184528\n",
      "Train loss at epoch (1083):  0.033327445\n",
      "Test loss at epoch (1083):  0.017962495\n",
      "Train loss at epoch (1084):  0.033035766\n",
      "Test loss at epoch (1084):  0.018221928\n",
      "Train loss at epoch (1085):  0.03300946\n",
      "Test loss at epoch (1085):  0.018593088\n",
      "Train loss at epoch (1086):  0.03307353\n",
      "Test loss at epoch (1086):  0.01787307\n",
      "Train loss at epoch (1087):  0.03300627\n",
      "Test loss at epoch (1087):  0.017951429\n",
      "Train loss at epoch (1088):  0.03293036\n",
      "Test loss at epoch (1088):  0.018023288\n",
      "Train loss at epoch (1089):  0.032993842\n",
      "Test loss at epoch (1089):  0.01792443\n",
      "Train loss at epoch (1090):  0.033214428\n",
      "Test loss at epoch (1090):  0.018129861\n",
      "Train loss at epoch (1091):  0.033190485\n",
      "Test loss at epoch (1091):  0.017940843\n",
      "Train loss at epoch (1092):  0.033016935\n",
      "Test loss at epoch (1092):  0.017993893\n",
      "Train loss at epoch (1093):  0.03298474\n",
      "Test loss at epoch (1093):  0.01796491\n",
      "Train loss at epoch (1094):  0.032952115\n",
      "Test loss at epoch (1094):  0.01807142\n",
      "Train loss at epoch (1095):  0.033214916\n",
      "Test loss at epoch (1095):  0.018240761\n",
      "Train loss at epoch (1096):  0.033081047\n",
      "Test loss at epoch (1096):  0.017976722\n",
      "Train loss at epoch (1097):  0.0329691\n",
      "Test loss at epoch (1097):  0.017950738\n",
      "Train loss at epoch (1098):  0.032919265\n",
      "Test loss at epoch (1098):  0.01838815\n",
      "Train loss at epoch (1099):  0.033226352\n",
      "Test loss at epoch (1099):  0.017925171\n",
      "Train loss at epoch (1100):  0.033119053\n",
      "Test loss at epoch (1100):  0.018299904\n",
      "Train loss at epoch (1101):  0.033000533\n",
      "Test loss at epoch (1101):  0.017999442\n",
      "Train loss at epoch (1102):  0.03312888\n",
      "Test loss at epoch (1102):  0.01818093\n",
      "Train loss at epoch (1103):  0.03284659\n",
      "Test loss at epoch (1103):  0.018249167\n",
      "Train loss at epoch (1104):  0.03282848\n",
      "Test loss at epoch (1104):  0.018304408\n",
      "Train loss at epoch (1105):  0.033082537\n",
      "Test loss at epoch (1105):  0.017846365\n",
      "Train loss at epoch (1106):  0.03288231\n",
      "Test loss at epoch (1106):  0.017956547\n",
      "Train loss at epoch (1107):  0.03310885\n",
      "Test loss at epoch (1107):  0.01799173\n",
      "Train loss at epoch (1108):  0.033201285\n",
      "Test loss at epoch (1108):  0.017992703\n",
      "Train loss at epoch (1109):  0.032904625\n",
      "Test loss at epoch (1109):  0.017865771\n",
      "Train loss at epoch (1110):  0.032959927\n",
      "Test loss at epoch (1110):  0.018099217\n",
      "Train loss at epoch (1111):  0.03295738\n",
      "Test loss at epoch (1111):  0.017807366\n",
      "Train loss at epoch (1112):  0.032887828\n",
      "Test loss at epoch (1112):  0.017808836\n",
      "Train loss at epoch (1113):  0.033038583\n",
      "Test loss at epoch (1113):  0.017984876\n",
      "Train loss at epoch (1114):  0.032985073\n",
      "Test loss at epoch (1114):  0.017987268\n",
      "Train loss at epoch (1115):  0.032897785\n",
      "Test loss at epoch (1115):  0.017876917\n",
      "Train loss at epoch (1116):  0.032740634\n",
      "Test loss at epoch (1116):  0.017930975\n",
      "Train loss at epoch (1117):  0.0329871\n",
      "Test loss at epoch (1117):  0.01787884\n",
      "Train loss at epoch (1118):  0.033167526\n",
      "Test loss at epoch (1118):  0.017942121\n",
      "Train loss at epoch (1119):  0.033074245\n",
      "Test loss at epoch (1119):  0.0179371\n",
      "Train loss at epoch (1120):  0.032797623\n",
      "Test loss at epoch (1120):  0.018176036\n",
      "Train loss at epoch (1121):  0.032774333\n",
      "Test loss at epoch (1121):  0.01782238\n",
      "Train loss at epoch (1122):  0.032963768\n",
      "Test loss at epoch (1122):  0.017885763\n",
      "Train loss at epoch (1123):  0.032870032\n",
      "Test loss at epoch (1123):  0.018161194\n",
      "Train loss at epoch (1124):  0.033034194\n",
      "Test loss at epoch (1124):  0.018126462\n",
      "Train loss at epoch (1125):  0.032717764\n",
      "Test loss at epoch (1125):  0.017996313\n",
      "Train loss at epoch (1126):  0.032940764\n",
      "Test loss at epoch (1126):  0.017888045\n",
      "Train loss at epoch (1127):  0.032828666\n",
      "Test loss at epoch (1127):  0.018260173\n",
      "Train loss at epoch (1128):  0.03283586\n",
      "Test loss at epoch (1128):  0.017909767\n",
      "Train loss at epoch (1129):  0.03297893\n",
      "Test loss at epoch (1129):  0.018245038\n",
      "Train loss at epoch (1130):  0.032587603\n",
      "Test loss at epoch (1130):  0.01788681\n",
      "Train loss at epoch (1131):  0.032698534\n",
      "Test loss at epoch (1131):  0.018166298\n",
      "Train loss at epoch (1132):  0.03262512\n",
      "Test loss at epoch (1132):  0.018232476\n",
      "Train loss at epoch (1133):  0.032848522\n",
      "Test loss at epoch (1133):  0.017946156\n",
      "Train loss at epoch (1134):  0.03260678\n",
      "Test loss at epoch (1134):  0.017765984\n",
      "Train loss at epoch (1135):  0.032921765\n",
      "Test loss at epoch (1135):  0.018703595\n",
      "Train loss at epoch (1136):  0.032949865\n",
      "Test loss at epoch (1136):  0.017855303\n",
      "Train loss at epoch (1137):  0.03283192\n",
      "Test loss at epoch (1137):  0.01783196\n",
      "Train loss at epoch (1138):  0.03254576\n",
      "Test loss at epoch (1138):  0.017808918\n",
      "Train loss at epoch (1139):  0.032630857\n",
      "Test loss at epoch (1139):  0.017790545\n",
      "Train loss at epoch (1140):  0.032512113\n",
      "Test loss at epoch (1140):  0.018281141\n",
      "Train loss at epoch (1141):  0.03292384\n",
      "Test loss at epoch (1141):  0.017872958\n",
      "Train loss at epoch (1142):  0.03281374\n",
      "Test loss at epoch (1142):  0.018051442\n",
      "Train loss at epoch (1143):  0.032824524\n",
      "Test loss at epoch (1143):  0.017969146\n",
      "Train loss at epoch (1144):  0.032533858\n",
      "Test loss at epoch (1144):  0.017993657\n",
      "Train loss at epoch (1145):  0.03262566\n",
      "Test loss at epoch (1145):  0.017766554\n",
      "Train loss at epoch (1146):  0.03252748\n",
      "Test loss at epoch (1146):  0.0180505\n",
      "Train loss at epoch (1147):  0.032783505\n",
      "Test loss at epoch (1147):  0.017890941\n",
      "Train loss at epoch (1148):  0.032668255\n",
      "Test loss at epoch (1148):  0.017688215\n",
      "Train loss at epoch (1149):  0.032617614\n",
      "Test loss at epoch (1149):  0.017802771\n",
      "Train loss at epoch (1150):  0.03252941\n",
      "Test loss at epoch (1150):  0.018234706\n",
      "Train loss at epoch (1151):  0.032719813\n",
      "Test loss at epoch (1151):  0.018584317\n",
      "Train loss at epoch (1152):  0.03260455\n",
      "Test loss at epoch (1152):  0.018086024\n",
      "Train loss at epoch (1153):  0.032614898\n",
      "Test loss at epoch (1153):  0.017874138\n",
      "Train loss at epoch (1154):  0.032561377\n",
      "Test loss at epoch (1154):  0.017818782\n",
      "Train loss at epoch (1155):  0.03278934\n",
      "Test loss at epoch (1155):  0.017770909\n",
      "Train loss at epoch (1156):  0.032559276\n",
      "Test loss at epoch (1156):  0.017687146\n",
      "Train loss at epoch (1157):  0.032679606\n",
      "Test loss at epoch (1157):  0.017926171\n",
      "Train loss at epoch (1158):  0.032322634\n",
      "Test loss at epoch (1158):  0.017815234\n",
      "Train loss at epoch (1159):  0.032681495\n",
      "Test loss at epoch (1159):  0.017970681\n",
      "Train loss at epoch (1160):  0.032564417\n",
      "Test loss at epoch (1160):  0.017786108\n",
      "Train loss at epoch (1161):  0.03260431\n",
      "Test loss at epoch (1161):  0.017878274\n",
      "Train loss at epoch (1162):  0.032593045\n",
      "Test loss at epoch (1162):  0.018328933\n",
      "Train loss at epoch (1163):  0.03235914\n",
      "Test loss at epoch (1163):  0.017895965\n",
      "Train loss at epoch (1164):  0.0326533\n",
      "Test loss at epoch (1164):  0.017713482\n",
      "Train loss at epoch (1165):  0.03246469\n",
      "Test loss at epoch (1165):  0.017859804\n",
      "Train loss at epoch (1166):  0.032319687\n",
      "Test loss at epoch (1166):  0.017928487\n",
      "Train loss at epoch (1167):  0.032362662\n",
      "Test loss at epoch (1167):  0.017677201\n",
      "Train loss at epoch (1168):  0.03242273\n",
      "Test loss at epoch (1168):  0.017846081\n",
      "Train loss at epoch (1169):  0.03224441\n",
      "Test loss at epoch (1169):  0.018128078\n",
      "Train loss at epoch (1170):  0.032379314\n",
      "Test loss at epoch (1170):  0.017783875\n",
      "Train loss at epoch (1171):  0.032416627\n",
      "Test loss at epoch (1171):  0.017705275\n",
      "Train loss at epoch (1172):  0.032225683\n",
      "Test loss at epoch (1172):  0.01773193\n",
      "Train loss at epoch (1173):  0.03260272\n",
      "Test loss at epoch (1173):  0.018124217\n",
      "Train loss at epoch (1174):  0.03240466\n",
      "Test loss at epoch (1174):  0.01783888\n",
      "Train loss at epoch (1175):  0.032217182\n",
      "Test loss at epoch (1175):  0.017797923\n",
      "Train loss at epoch (1176):  0.03236741\n",
      "Test loss at epoch (1176):  0.017745467\n",
      "Train loss at epoch (1177):  0.0322013\n",
      "Test loss at epoch (1177):  0.017804407\n",
      "Train loss at epoch (1178):  0.032501448\n",
      "Test loss at epoch (1178):  0.017866064\n",
      "Train loss at epoch (1179):  0.032162353\n",
      "Test loss at epoch (1179):  0.017709516\n",
      "Train loss at epoch (1180):  0.032226354\n",
      "Test loss at epoch (1180):  0.017865146\n",
      "Train loss at epoch (1181):  0.03233118\n",
      "Test loss at epoch (1181):  0.017714635\n",
      "Train loss at epoch (1182):  0.032222398\n",
      "Test loss at epoch (1182):  0.017786326\n",
      "Train loss at epoch (1183):  0.032297608\n",
      "Test loss at epoch (1183):  0.017880648\n",
      "Train loss at epoch (1184):  0.032419704\n",
      "Test loss at epoch (1184):  0.017673397\n",
      "Train loss at epoch (1185):  0.032347098\n",
      "Test loss at epoch (1185):  0.017741801\n",
      "Train loss at epoch (1186):  0.032170527\n",
      "Test loss at epoch (1186):  0.017889835\n",
      "Train loss at epoch (1187):  0.032171693\n",
      "Test loss at epoch (1187):  0.017808614\n",
      "Train loss at epoch (1188):  0.032279555\n",
      "Test loss at epoch (1188):  0.017858349\n",
      "Train loss at epoch (1189):  0.032079194\n",
      "Test loss at epoch (1189):  0.017918343\n",
      "Train loss at epoch (1190):  0.03226858\n",
      "Test loss at epoch (1190):  0.018069975\n",
      "Train loss at epoch (1191):  0.03224563\n",
      "Test loss at epoch (1191):  0.017763106\n",
      "Train loss at epoch (1192):  0.03204092\n",
      "Test loss at epoch (1192):  0.01779672\n",
      "Train loss at epoch (1193):  0.03227586\n",
      "Test loss at epoch (1193):  0.017795393\n",
      "Train loss at epoch (1194):  0.032066554\n",
      "Test loss at epoch (1194):  0.01776256\n",
      "Train loss at epoch (1195):  0.032343306\n",
      "Test loss at epoch (1195):  0.017952273\n",
      "Train loss at epoch (1196):  0.03220378\n",
      "Test loss at epoch (1196):  0.017771265\n",
      "Train loss at epoch (1197):  0.032316495\n",
      "Test loss at epoch (1197):  0.01772416\n",
      "Train loss at epoch (1198):  0.03208264\n",
      "Test loss at epoch (1198):  0.017690746\n",
      "Train loss at epoch (1199):  0.03204156\n",
      "Test loss at epoch (1199):  0.017826963\n",
      "Train loss at epoch (1200):  0.03208084\n",
      "Test loss at epoch (1200):  0.017915033\n",
      "Train loss at epoch (1201):  0.0322673\n",
      "Test loss at epoch (1201):  0.017824547\n",
      "Train loss at epoch (1202):  0.031947285\n",
      "Test loss at epoch (1202):  0.017893288\n",
      "Train loss at epoch (1203):  0.032001518\n",
      "Test loss at epoch (1203):  0.017779812\n",
      "Train loss at epoch (1204):  0.031899434\n",
      "Test loss at epoch (1204):  0.017725144\n",
      "Train loss at epoch (1205):  0.03207991\n",
      "Test loss at epoch (1205):  0.017709138\n",
      "Train loss at epoch (1206):  0.0319337\n",
      "Test loss at epoch (1206):  0.018353416\n",
      "Train loss at epoch (1207):  0.032312516\n",
      "Test loss at epoch (1207):  0.01784783\n",
      "Train loss at epoch (1208):  0.032074247\n",
      "Test loss at epoch (1208):  0.017822651\n",
      "Train loss at epoch (1209):  0.03211157\n",
      "Test loss at epoch (1209):  0.018157879\n",
      "Train loss at epoch (1210):  0.032013927\n",
      "Test loss at epoch (1210):  0.01772857\n",
      "Train loss at epoch (1211):  0.032062773\n",
      "Test loss at epoch (1211):  0.01766276\n",
      "Train loss at epoch (1212):  0.03187889\n",
      "Test loss at epoch (1212):  0.01783745\n",
      "Train loss at epoch (1213):  0.03203574\n",
      "Test loss at epoch (1213):  0.017584085\n",
      "Train loss at epoch (1214):  0.032084834\n",
      "Test loss at epoch (1214):  0.01783452\n",
      "Train loss at epoch (1215):  0.03194446\n",
      "Test loss at epoch (1215):  0.017803682\n",
      "Train loss at epoch (1216):  0.03210355\n",
      "Test loss at epoch (1216):  0.017887386\n",
      "Train loss at epoch (1217):  0.031965896\n",
      "Test loss at epoch (1217):  0.017701227\n",
      "Train loss at epoch (1218):  0.031961642\n",
      "Test loss at epoch (1218):  0.017758794\n",
      "Train loss at epoch (1219):  0.031851087\n",
      "Test loss at epoch (1219):  0.01772724\n",
      "Train loss at epoch (1220):  0.03187612\n",
      "Test loss at epoch (1220):  0.017859925\n",
      "Train loss at epoch (1221):  0.0317637\n",
      "Test loss at epoch (1221):  0.017744921\n",
      "Train loss at epoch (1222):  0.031876683\n",
      "Test loss at epoch (1222):  0.017694935\n",
      "Train loss at epoch (1223):  0.031942163\n",
      "Test loss at epoch (1223):  0.017935341\n",
      "Train loss at epoch (1224):  0.031764466\n",
      "Test loss at epoch (1224):  0.01777407\n",
      "Train loss at epoch (1225):  0.03188722\n",
      "Test loss at epoch (1225):  0.017642245\n",
      "Train loss at epoch (1226):  0.032127723\n",
      "Test loss at epoch (1226):  0.01763049\n",
      "Train loss at epoch (1227):  0.031841937\n",
      "Test loss at epoch (1227):  0.01776701\n",
      "Train loss at epoch (1228):  0.031750876\n",
      "Test loss at epoch (1228):  0.017673941\n",
      "Train loss at epoch (1229):  0.0319186\n",
      "Test loss at epoch (1229):  0.017790772\n",
      "Train loss at epoch (1230):  0.03189136\n",
      "Test loss at epoch (1230):  0.017788138\n",
      "Train loss at epoch (1231):  0.031826302\n",
      "Test loss at epoch (1231):  0.017748889\n",
      "Train loss at epoch (1232):  0.031650584\n",
      "Test loss at epoch (1232):  0.017956924\n",
      "Train loss at epoch (1233):  0.031773925\n",
      "Test loss at epoch (1233):  0.017715776\n",
      "Train loss at epoch (1234):  0.031827886\n",
      "Test loss at epoch (1234):  0.017753372\n",
      "Train loss at epoch (1235):  0.03168287\n",
      "Test loss at epoch (1235):  0.017855281\n",
      "Train loss at epoch (1236):  0.031740703\n",
      "Test loss at epoch (1236):  0.017768817\n",
      "Train loss at epoch (1237):  0.031838182\n",
      "Test loss at epoch (1237):  0.017803872\n",
      "Train loss at epoch (1238):  0.031819783\n",
      "Test loss at epoch (1238):  0.017643813\n",
      "Train loss at epoch (1239):  0.031764705\n",
      "Test loss at epoch (1239):  0.01788271\n",
      "Train loss at epoch (1240):  0.031780783\n",
      "Test loss at epoch (1240):  0.01777859\n",
      "Train loss at epoch (1241):  0.03185796\n",
      "Test loss at epoch (1241):  0.017860267\n",
      "Train loss at epoch (1242):  0.031847928\n",
      "Test loss at epoch (1242):  0.01803958\n",
      "Train loss at epoch (1243):  0.031920236\n",
      "Test loss at epoch (1243):  0.017625773\n",
      "Train loss at epoch (1244):  0.031659134\n",
      "Test loss at epoch (1244):  0.0178011\n",
      "Train loss at epoch (1245):  0.031705964\n",
      "Test loss at epoch (1245):  0.017622668\n",
      "Train loss at epoch (1246):  0.031813\n",
      "Test loss at epoch (1246):  0.01760462\n",
      "Train loss at epoch (1247):  0.031685427\n",
      "Test loss at epoch (1247):  0.017672056\n",
      "Train loss at epoch (1248):  0.031853385\n",
      "Test loss at epoch (1248):  0.017586306\n",
      "Train loss at epoch (1249):  0.03199089\n",
      "Test loss at epoch (1249):  0.017864013\n",
      "Train loss at epoch (1250):  0.031677738\n",
      "Test loss at epoch (1250):  0.017903166\n",
      "Train loss at epoch (1251):  0.03185457\n",
      "Test loss at epoch (1251):  0.017670529\n",
      "Train loss at epoch (1252):  0.03176555\n",
      "Test loss at epoch (1252):  0.01766507\n",
      "Train loss at epoch (1253):  0.031873383\n",
      "Test loss at epoch (1253):  0.017699555\n",
      "Train loss at epoch (1254):  0.03182547\n",
      "Test loss at epoch (1254):  0.017746305\n",
      "Train loss at epoch (1255):  0.03175691\n",
      "Test loss at epoch (1255):  0.01766762\n",
      "Train loss at epoch (1256):  0.031610224\n",
      "Test loss at epoch (1256):  0.017739521\n",
      "Train loss at epoch (1257):  0.031706583\n",
      "Test loss at epoch (1257):  0.017705742\n",
      "Train loss at epoch (1258):  0.031738196\n",
      "Test loss at epoch (1258):  0.017643703\n",
      "Train loss at epoch (1259):  0.03156804\n",
      "Test loss at epoch (1259):  0.017710408\n",
      "Train loss at epoch (1260):  0.031595226\n",
      "Test loss at epoch (1260):  0.017697878\n",
      "Train loss at epoch (1261):  0.031685542\n",
      "Test loss at epoch (1261):  0.017670715\n",
      "Train loss at epoch (1262):  0.031773455\n",
      "Test loss at epoch (1262):  0.017662514\n",
      "Train loss at epoch (1263):  0.031569347\n",
      "Test loss at epoch (1263):  0.017644428\n",
      "Train loss at epoch (1264):  0.03171854\n",
      "Test loss at epoch (1264):  0.01763665\n",
      "Train loss at epoch (1265):  0.031613093\n",
      "Test loss at epoch (1265):  0.017895103\n",
      "Train loss at epoch (1266):  0.03173164\n",
      "Test loss at epoch (1266):  0.017589008\n",
      "Train loss at epoch (1267):  0.031501\n",
      "Test loss at epoch (1267):  0.017613348\n",
      "Train loss at epoch (1268):  0.031522617\n",
      "Test loss at epoch (1268):  0.017801538\n",
      "Train loss at epoch (1269):  0.031465538\n",
      "Test loss at epoch (1269):  0.017943932\n",
      "Train loss at epoch (1270):  0.03161367\n",
      "Test loss at epoch (1270):  0.017918305\n",
      "Train loss at epoch (1271):  0.03158467\n",
      "Test loss at epoch (1271):  0.017741272\n",
      "Train loss at epoch (1272):  0.031735957\n",
      "Test loss at epoch (1272):  0.018015958\n",
      "Train loss at epoch (1273):  0.031622056\n",
      "Test loss at epoch (1273):  0.017746441\n",
      "Train loss at epoch (1274):  0.031630896\n",
      "Test loss at epoch (1274):  0.017990574\n",
      "Train loss at epoch (1275):  0.031543303\n",
      "Test loss at epoch (1275):  0.017714353\n",
      "Train loss at epoch (1276):  0.031530492\n",
      "Test loss at epoch (1276):  0.01752418\n",
      "Train loss at epoch (1277):  0.0316005\n",
      "Test loss at epoch (1277):  0.018102977\n",
      "Train loss at epoch (1278):  0.031560834\n",
      "Test loss at epoch (1278):  0.017659832\n",
      "Train loss at epoch (1279):  0.031682182\n",
      "Test loss at epoch (1279):  0.017630724\n",
      "Train loss at epoch (1280):  0.03143709\n",
      "Test loss at epoch (1280):  0.01757685\n",
      "Train loss at epoch (1281):  0.031393822\n",
      "Test loss at epoch (1281):  0.017752538\n",
      "Train loss at epoch (1282):  0.031406704\n",
      "Test loss at epoch (1282):  0.01790634\n",
      "Train loss at epoch (1283):  0.031385787\n",
      "Test loss at epoch (1283):  0.017627418\n",
      "Train loss at epoch (1284):  0.031470913\n",
      "Test loss at epoch (1284):  0.017634794\n",
      "Train loss at epoch (1285):  0.031521205\n",
      "Test loss at epoch (1285):  0.017724684\n",
      "Train loss at epoch (1286):  0.03138866\n",
      "Test loss at epoch (1286):  0.017603505\n",
      "Train loss at epoch (1287):  0.03161406\n",
      "Test loss at epoch (1287):  0.01770029\n",
      "Train loss at epoch (1288):  0.03149596\n",
      "Test loss at epoch (1288):  0.017905578\n",
      "Train loss at epoch (1289):  0.031455673\n",
      "Test loss at epoch (1289):  0.017676663\n",
      "Train loss at epoch (1290):  0.031138366\n",
      "Test loss at epoch (1290):  0.017716803\n",
      "Train loss at epoch (1291):  0.031482406\n",
      "Test loss at epoch (1291):  0.017464146\n",
      "Train loss at epoch (1292):  0.031432033\n",
      "Test loss at epoch (1292):  0.017639205\n",
      "Train loss at epoch (1293):  0.03146905\n",
      "Test loss at epoch (1293):  0.017565975\n",
      "Train loss at epoch (1294):  0.031479165\n",
      "Test loss at epoch (1294):  0.017542902\n",
      "Train loss at epoch (1295):  0.03128612\n",
      "Test loss at epoch (1295):  0.017580863\n",
      "Train loss at epoch (1296):  0.031464726\n",
      "Test loss at epoch (1296):  0.017753107\n",
      "Train loss at epoch (1297):  0.0315406\n",
      "Test loss at epoch (1297):  0.017564746\n",
      "Train loss at epoch (1298):  0.03127257\n",
      "Test loss at epoch (1298):  0.017571684\n",
      "Train loss at epoch (1299):  0.0314294\n",
      "Test loss at epoch (1299):  0.017610293\n",
      "Train loss at epoch (1300):  0.031237911\n",
      "Test loss at epoch (1300):  0.017939057\n",
      "Train loss at epoch (1301):  0.031163396\n",
      "Test loss at epoch (1301):  0.017607361\n",
      "Train loss at epoch (1302):  0.031420033\n",
      "Test loss at epoch (1302):  0.017561588\n",
      "Train loss at epoch (1303):  0.03144034\n",
      "Test loss at epoch (1303):  0.017554073\n",
      "Train loss at epoch (1304):  0.031372316\n",
      "Test loss at epoch (1304):  0.017833995\n",
      "Train loss at epoch (1305):  0.031428892\n",
      "Test loss at epoch (1305):  0.017710045\n",
      "Train loss at epoch (1306):  0.031273253\n",
      "Test loss at epoch (1306):  0.017637813\n",
      "Train loss at epoch (1307):  0.0313742\n",
      "Test loss at epoch (1307):  0.017668987\n",
      "Train loss at epoch (1308):  0.03129837\n",
      "Test loss at epoch (1308):  0.017633514\n",
      "Train loss at epoch (1309):  0.03125045\n",
      "Test loss at epoch (1309):  0.017713135\n",
      "Train loss at epoch (1310):  0.031242479\n",
      "Test loss at epoch (1310):  0.017936815\n",
      "Train loss at epoch (1311):  0.03133777\n",
      "Test loss at epoch (1311):  0.01749962\n",
      "Train loss at epoch (1312):  0.031352326\n",
      "Test loss at epoch (1312):  0.01746405\n",
      "Train loss at epoch (1313):  0.03135766\n",
      "Test loss at epoch (1313):  0.017494882\n",
      "Train loss at epoch (1314):  0.031378236\n",
      "Test loss at epoch (1314):  0.017816132\n",
      "Train loss at epoch (1315):  0.031375315\n",
      "Test loss at epoch (1315):  0.017475838\n",
      "Train loss at epoch (1316):  0.031239895\n",
      "Test loss at epoch (1316):  0.017902564\n",
      "Train loss at epoch (1317):  0.03133001\n",
      "Test loss at epoch (1317):  0.017564112\n",
      "Train loss at epoch (1318):  0.031190975\n",
      "Test loss at epoch (1318):  0.017556181\n",
      "Train loss at epoch (1319):  0.031086646\n",
      "Test loss at epoch (1319):  0.017620238\n",
      "Train loss at epoch (1320):  0.03136326\n",
      "Test loss at epoch (1320):  0.017615108\n",
      "Train loss at epoch (1321):  0.031191278\n",
      "Test loss at epoch (1321):  0.017612949\n",
      "Train loss at epoch (1322):  0.031193737\n",
      "Test loss at epoch (1322):  0.017545592\n",
      "Train loss at epoch (1323):  0.031255066\n",
      "Test loss at epoch (1323):  0.017839586\n",
      "Train loss at epoch (1324):  0.031116765\n",
      "Test loss at epoch (1324):  0.017725157\n",
      "Train loss at epoch (1325):  0.031281445\n",
      "Test loss at epoch (1325):  0.01748639\n",
      "Train loss at epoch (1326):  0.031357545\n",
      "Test loss at epoch (1326):  0.017733479\n",
      "Train loss at epoch (1327):  0.031346846\n",
      "Test loss at epoch (1327):  0.017466912\n",
      "Train loss at epoch (1328):  0.031191116\n",
      "Test loss at epoch (1328):  0.0176564\n",
      "Train loss at epoch (1329):  0.031297065\n",
      "Test loss at epoch (1329):  0.017602613\n",
      "Train loss at epoch (1330):  0.031244129\n",
      "Test loss at epoch (1330):  0.01765302\n",
      "Train loss at epoch (1331):  0.031172562\n",
      "Test loss at epoch (1331):  0.01765687\n",
      "Train loss at epoch (1332):  0.031226056\n",
      "Test loss at epoch (1332):  0.017550217\n",
      "Train loss at epoch (1333):  0.03127743\n",
      "Test loss at epoch (1333):  0.01776795\n",
      "Train loss at epoch (1334):  0.03126677\n",
      "Test loss at epoch (1334):  0.017508818\n",
      "Train loss at epoch (1335):  0.03124718\n",
      "Test loss at epoch (1335):  0.01775001\n",
      "Train loss at epoch (1336):  0.031107431\n",
      "Test loss at epoch (1336):  0.017574843\n",
      "Train loss at epoch (1337):  0.03140452\n",
      "Test loss at epoch (1337):  0.017562138\n",
      "Train loss at epoch (1338):  0.03102164\n",
      "Test loss at epoch (1338):  0.017552074\n",
      "Train loss at epoch (1339):  0.031094497\n",
      "Test loss at epoch (1339):  0.017461767\n",
      "Train loss at epoch (1340):  0.031092787\n",
      "Test loss at epoch (1340):  0.017547421\n",
      "Train loss at epoch (1341):  0.031075938\n",
      "Test loss at epoch (1341):  0.01751968\n",
      "Train loss at epoch (1342):  0.03125226\n",
      "Test loss at epoch (1342):  0.017839238\n",
      "Train loss at epoch (1343):  0.031092351\n",
      "Test loss at epoch (1343):  0.0176602\n",
      "Train loss at epoch (1344):  0.031204125\n",
      "Test loss at epoch (1344):  0.017691836\n",
      "Train loss at epoch (1345):  0.031122688\n",
      "Test loss at epoch (1345):  0.017686145\n",
      "Train loss at epoch (1346):  0.03109745\n",
      "Test loss at epoch (1346):  0.01756663\n",
      "Train loss at epoch (1347):  0.030998887\n",
      "Test loss at epoch (1347):  0.017439883\n",
      "Train loss at epoch (1348):  0.031118918\n",
      "Test loss at epoch (1348):  0.017525664\n",
      "Train loss at epoch (1349):  0.031185133\n",
      "Test loss at epoch (1349):  0.017719906\n",
      "Train loss at epoch (1350):  0.031016875\n",
      "Test loss at epoch (1350):  0.017471481\n",
      "Train loss at epoch (1351):  0.031171646\n",
      "Test loss at epoch (1351):  0.017606432\n",
      "Train loss at epoch (1352):  0.031101136\n",
      "Test loss at epoch (1352):  0.017461484\n",
      "Train loss at epoch (1353):  0.031197052\n",
      "Test loss at epoch (1353):  0.017506445\n",
      "Train loss at epoch (1354):  0.031115351\n",
      "Test loss at epoch (1354):  0.017759105\n",
      "Train loss at epoch (1355):  0.031094698\n",
      "Test loss at epoch (1355):  0.01762951\n",
      "Train loss at epoch (1356):  0.031204399\n",
      "Test loss at epoch (1356):  0.017936815\n",
      "Train loss at epoch (1357):  0.031148456\n",
      "Test loss at epoch (1357):  0.017771931\n",
      "Train loss at epoch (1358):  0.03106953\n",
      "Test loss at epoch (1358):  0.017456891\n",
      "Train loss at epoch (1359):  0.031128393\n",
      "Test loss at epoch (1359):  0.017415833\n",
      "Train loss at epoch (1360):  0.030997502\n",
      "Test loss at epoch (1360):  0.017523956\n",
      "Train loss at epoch (1361):  0.030836403\n",
      "Test loss at epoch (1361):  0.01755966\n",
      "Train loss at epoch (1362):  0.030969251\n",
      "Test loss at epoch (1362):  0.017502459\n",
      "Train loss at epoch (1363):  0.030995118\n",
      "Test loss at epoch (1363):  0.017444713\n",
      "Train loss at epoch (1364):  0.03129826\n",
      "Test loss at epoch (1364):  0.017445521\n",
      "Train loss at epoch (1365):  0.031005723\n",
      "Test loss at epoch (1365):  0.017721593\n",
      "Train loss at epoch (1366):  0.031102523\n",
      "Test loss at epoch (1366):  0.017600179\n",
      "Train loss at epoch (1367):  0.031176433\n",
      "Test loss at epoch (1367):  0.017464386\n",
      "Train loss at epoch (1368):  0.031149512\n",
      "Test loss at epoch (1368):  0.01750625\n",
      "Train loss at epoch (1369):  0.030830994\n",
      "Test loss at epoch (1369):  0.017659368\n",
      "Train loss at epoch (1370):  0.031109193\n",
      "Test loss at epoch (1370):  0.01759226\n",
      "Train loss at epoch (1371):  0.031118304\n",
      "Test loss at epoch (1371):  0.017558204\n",
      "Train loss at epoch (1372):  0.031055896\n",
      "Test loss at epoch (1372):  0.017900458\n",
      "Train loss at epoch (1373):  0.031084452\n",
      "Test loss at epoch (1373):  0.017632948\n",
      "Train loss at epoch (1374):  0.03101899\n",
      "Test loss at epoch (1374):  0.017477013\n",
      "Train loss at epoch (1375):  0.030827278\n",
      "Test loss at epoch (1375):  0.01763886\n",
      "Train loss at epoch (1376):  0.031260636\n",
      "Test loss at epoch (1376):  0.017589996\n",
      "Train loss at epoch (1377):  0.03081544\n",
      "Test loss at epoch (1377):  0.017481826\n",
      "Train loss at epoch (1378):  0.03113986\n",
      "Test loss at epoch (1378):  0.017505746\n",
      "Train loss at epoch (1379):  0.03117068\n",
      "Test loss at epoch (1379):  0.017494567\n",
      "Train loss at epoch (1380):  0.03108793\n",
      "Test loss at epoch (1380):  0.017538914\n",
      "Train loss at epoch (1381):  0.030987052\n",
      "Test loss at epoch (1381):  0.017476115\n",
      "Train loss at epoch (1382):  0.03099939\n",
      "Test loss at epoch (1382):  0.017524343\n",
      "Train loss at epoch (1383):  0.031188536\n",
      "Test loss at epoch (1383):  0.017618934\n",
      "Train loss at epoch (1384):  0.030962499\n",
      "Test loss at epoch (1384):  0.017564824\n",
      "Train loss at epoch (1385):  0.031041618\n",
      "Test loss at epoch (1385):  0.017486848\n",
      "Train loss at epoch (1386):  0.031091077\n",
      "Test loss at epoch (1386):  0.017463118\n",
      "Train loss at epoch (1387):  0.030983187\n",
      "Test loss at epoch (1387):  0.017660424\n",
      "Train loss at epoch (1388):  0.031081988\n",
      "Test loss at epoch (1388):  0.01760993\n",
      "Train loss at epoch (1389):  0.03120361\n",
      "Test loss at epoch (1389):  0.017435806\n",
      "Train loss at epoch (1390):  0.031016821\n",
      "Test loss at epoch (1390):  0.01753328\n",
      "Train loss at epoch (1391):  0.03093351\n",
      "Test loss at epoch (1391):  0.01752841\n",
      "Train loss at epoch (1392):  0.030943627\n",
      "Test loss at epoch (1392):  0.017478328\n",
      "Train loss at epoch (1393):  0.0307557\n",
      "Test loss at epoch (1393):  0.017565576\n",
      "Train loss at epoch (1394):  0.030933848\n",
      "Test loss at epoch (1394):  0.017451782\n",
      "Train loss at epoch (1395):  0.030916058\n",
      "Test loss at epoch (1395):  0.01783482\n",
      "Train loss at epoch (1396):  0.031028315\n",
      "Test loss at epoch (1396):  0.017470783\n",
      "Train loss at epoch (1397):  0.030892098\n",
      "Test loss at epoch (1397):  0.017419586\n",
      "Train loss at epoch (1398):  0.03103687\n",
      "Test loss at epoch (1398):  0.017479563\n",
      "Train loss at epoch (1399):  0.03084692\n",
      "Test loss at epoch (1399):  0.017420629\n",
      "Train loss at epoch (1400):  0.030909542\n",
      "Test loss at epoch (1400):  0.01738316\n",
      "Train loss at epoch (1401):  0.03074138\n",
      "Test loss at epoch (1401):  0.017655388\n",
      "Train loss at epoch (1402):  0.030737437\n",
      "Test loss at epoch (1402):  0.017476015\n",
      "Train loss at epoch (1403):  0.030961942\n",
      "Test loss at epoch (1403):  0.017618084\n",
      "Train loss at epoch (1404):  0.030904798\n",
      "Test loss at epoch (1404):  0.017482745\n",
      "Train loss at epoch (1405):  0.031079985\n",
      "Test loss at epoch (1405):  0.01742682\n",
      "Train loss at epoch (1406):  0.03086977\n",
      "Test loss at epoch (1406):  0.017381078\n",
      "Train loss at epoch (1407):  0.030938553\n",
      "Test loss at epoch (1407):  0.01747604\n",
      "Train loss at epoch (1408):  0.030841624\n",
      "Test loss at epoch (1408):  0.017427478\n",
      "Train loss at epoch (1409):  0.030722147\n",
      "Test loss at epoch (1409):  0.017715272\n",
      "Train loss at epoch (1410):  0.030957356\n",
      "Test loss at epoch (1410):  0.017530123\n",
      "Train loss at epoch (1411):  0.030865062\n",
      "Test loss at epoch (1411):  0.017539615\n",
      "Train loss at epoch (1412):  0.031026805\n",
      "Test loss at epoch (1412):  0.017583994\n",
      "Train loss at epoch (1413):  0.031022716\n",
      "Test loss at epoch (1413):  0.01744105\n",
      "Train loss at epoch (1414):  0.030950213\n",
      "Test loss at epoch (1414):  0.017463898\n",
      "Train loss at epoch (1415):  0.030958166\n",
      "Test loss at epoch (1415):  0.017377192\n",
      "Train loss at epoch (1416):  0.03086496\n",
      "Test loss at epoch (1416):  0.017489286\n",
      "Train loss at epoch (1417):  0.031009404\n",
      "Test loss at epoch (1417):  0.017514322\n",
      "Train loss at epoch (1418):  0.031006832\n",
      "Test loss at epoch (1418):  0.017454404\n",
      "Train loss at epoch (1419):  0.030983366\n",
      "Test loss at epoch (1419):  0.01739565\n",
      "Train loss at epoch (1420):  0.030555833\n",
      "Test loss at epoch (1420):  0.017560292\n",
      "Train loss at epoch (1421):  0.030720374\n",
      "Test loss at epoch (1421):  0.017528884\n",
      "Train loss at epoch (1422):  0.030989971\n",
      "Test loss at epoch (1422):  0.017406091\n",
      "Train loss at epoch (1423):  0.03068291\n",
      "Test loss at epoch (1423):  0.017477052\n",
      "Train loss at epoch (1424):  0.030876732\n",
      "Test loss at epoch (1424):  0.017499615\n",
      "Train loss at epoch (1425):  0.030890636\n",
      "Test loss at epoch (1425):  0.01748053\n",
      "Train loss at epoch (1426):  0.03086732\n",
      "Test loss at epoch (1426):  0.017487848\n",
      "Train loss at epoch (1427):  0.03080696\n",
      "Test loss at epoch (1427):  0.017431488\n",
      "Train loss at epoch (1428):  0.03089454\n",
      "Test loss at epoch (1428):  0.017426075\n",
      "Train loss at epoch (1429):  0.030660048\n",
      "Test loss at epoch (1429):  0.017436404\n",
      "Train loss at epoch (1430):  0.030846767\n",
      "Test loss at epoch (1430):  0.017530207\n",
      "Train loss at epoch (1431):  0.030808834\n",
      "Test loss at epoch (1431):  0.017437218\n",
      "Train loss at epoch (1432):  0.030917173\n",
      "Test loss at epoch (1432):  0.017480966\n",
      "Train loss at epoch (1433):  0.03076154\n",
      "Test loss at epoch (1433):  0.017580781\n",
      "Train loss at epoch (1434):  0.030580737\n",
      "Test loss at epoch (1434):  0.017300371\n",
      "Train loss at epoch (1435):  0.030771933\n",
      "Test loss at epoch (1435):  0.017371707\n",
      "Train loss at epoch (1436):  0.030899763\n",
      "Test loss at epoch (1436):  0.017627867\n",
      "Train loss at epoch (1437):  0.030878192\n",
      "Test loss at epoch (1437):  0.017409578\n",
      "Train loss at epoch (1438):  0.03067793\n",
      "Test loss at epoch (1438):  0.017359143\n",
      "Train loss at epoch (1439):  0.030776666\n",
      "Test loss at epoch (1439):  0.01746466\n",
      "Train loss at epoch (1440):  0.030763568\n",
      "Test loss at epoch (1440):  0.017423907\n",
      "Train loss at epoch (1441):  0.030793438\n",
      "Test loss at epoch (1441):  0.017353874\n",
      "Train loss at epoch (1442):  0.030806528\n",
      "Test loss at epoch (1442):  0.017407423\n",
      "Train loss at epoch (1443):  0.030755164\n",
      "Test loss at epoch (1443):  0.017337209\n",
      "Train loss at epoch (1444):  0.0308977\n",
      "Test loss at epoch (1444):  0.017408267\n",
      "Train loss at epoch (1445):  0.030908804\n",
      "Test loss at epoch (1445):  0.017590655\n",
      "Train loss at epoch (1446):  0.030798098\n",
      "Test loss at epoch (1446):  0.017476177\n",
      "Train loss at epoch (1447):  0.03079785\n",
      "Test loss at epoch (1447):  0.017513793\n",
      "Train loss at epoch (1448):  0.030924689\n",
      "Test loss at epoch (1448):  0.017378341\n",
      "Train loss at epoch (1449):  0.030755354\n",
      "Test loss at epoch (1449):  0.017450059\n",
      "Train loss at epoch (1450):  0.030722614\n",
      "Test loss at epoch (1450):  0.017417882\n",
      "Train loss at epoch (1451):  0.030728314\n",
      "Test loss at epoch (1451):  0.017631784\n",
      "Train loss at epoch (1452):  0.030700328\n",
      "Test loss at epoch (1452):  0.017395232\n",
      "Train loss at epoch (1453):  0.030852782\n",
      "Test loss at epoch (1453):  0.017371582\n",
      "Train loss at epoch (1454):  0.030617466\n",
      "Test loss at epoch (1454):  0.017474271\n",
      "Train loss at epoch (1455):  0.030759262\n",
      "Test loss at epoch (1455):  0.017560968\n",
      "Train loss at epoch (1456):  0.030756302\n",
      "Test loss at epoch (1456):  0.017414441\n",
      "Train loss at epoch (1457):  0.030779488\n",
      "Test loss at epoch (1457):  0.017428458\n",
      "Train loss at epoch (1458):  0.030440034\n",
      "Test loss at epoch (1458):  0.017381748\n",
      "Train loss at epoch (1459):  0.030775614\n",
      "Test loss at epoch (1459):  0.017447265\n",
      "Train loss at epoch (1460):  0.03062708\n",
      "Test loss at epoch (1460):  0.017477764\n",
      "Train loss at epoch (1461):  0.030749626\n",
      "Test loss at epoch (1461):  0.017605633\n",
      "Train loss at epoch (1462):  0.030900946\n",
      "Test loss at epoch (1462):  0.017367069\n",
      "Train loss at epoch (1463):  0.030798525\n",
      "Test loss at epoch (1463):  0.01740317\n",
      "Train loss at epoch (1464):  0.03071641\n",
      "Test loss at epoch (1464):  0.01734444\n",
      "Train loss at epoch (1465):  0.03073851\n",
      "Test loss at epoch (1465):  0.017572908\n",
      "Train loss at epoch (1466):  0.030680675\n",
      "Test loss at epoch (1466):  0.017392939\n",
      "Train loss at epoch (1467):  0.030518977\n",
      "Test loss at epoch (1467):  0.017368592\n",
      "Train loss at epoch (1468):  0.030920547\n",
      "Test loss at epoch (1468):  0.017328626\n",
      "Train loss at epoch (1469):  0.030834109\n",
      "Test loss at epoch (1469):  0.017380571\n",
      "Train loss at epoch (1470):  0.030651975\n",
      "Test loss at epoch (1470):  0.017367063\n",
      "Train loss at epoch (1471):  0.030679487\n",
      "Test loss at epoch (1471):  0.017371183\n",
      "Train loss at epoch (1472):  0.030780261\n",
      "Test loss at epoch (1472):  0.017362619\n",
      "Train loss at epoch (1473):  0.030729208\n",
      "Test loss at epoch (1473):  0.017551763\n",
      "Train loss at epoch (1474):  0.030962143\n",
      "Test loss at epoch (1474):  0.017476613\n",
      "Train loss at epoch (1475):  0.03070797\n",
      "Test loss at epoch (1475):  0.017340433\n",
      "Train loss at epoch (1476):  0.030838178\n",
      "Test loss at epoch (1476):  0.017575616\n",
      "Train loss at epoch (1477):  0.030567497\n",
      "Test loss at epoch (1477):  0.017361848\n",
      "Train loss at epoch (1478):  0.030662537\n",
      "Test loss at epoch (1478):  0.017333206\n",
      "Train loss at epoch (1479):  0.030562744\n",
      "Test loss at epoch (1479):  0.01739901\n",
      "Train loss at epoch (1480):  0.03087341\n",
      "Test loss at epoch (1480):  0.017382823\n",
      "Train loss at epoch (1481):  0.030778304\n",
      "Test loss at epoch (1481):  0.017364854\n",
      "Train loss at epoch (1482):  0.030567206\n",
      "Test loss at epoch (1482):  0.017477129\n",
      "Train loss at epoch (1483):  0.030653296\n",
      "Test loss at epoch (1483):  0.01748134\n",
      "Train loss at epoch (1484):  0.03093813\n",
      "Test loss at epoch (1484):  0.017376602\n",
      "Train loss at epoch (1485):  0.030504154\n",
      "Test loss at epoch (1485):  0.017332213\n",
      "Train loss at epoch (1486):  0.030790009\n",
      "Test loss at epoch (1486):  0.017365295\n",
      "Train loss at epoch (1487):  0.030723793\n",
      "Test loss at epoch (1487):  0.01746788\n",
      "Train loss at epoch (1488):  0.030657643\n",
      "Test loss at epoch (1488):  0.017297631\n",
      "Train loss at epoch (1489):  0.030549306\n",
      "Test loss at epoch (1489):  0.017374313\n",
      "Train loss at epoch (1490):  0.030622667\n",
      "Test loss at epoch (1490):  0.017406106\n",
      "Train loss at epoch (1491):  0.030721584\n",
      "Test loss at epoch (1491):  0.017390663\n",
      "Train loss at epoch (1492):  0.030674864\n",
      "Test loss at epoch (1492):  0.017370353\n",
      "Train loss at epoch (1493):  0.030485028\n",
      "Test loss at epoch (1493):  0.017395282\n",
      "Train loss at epoch (1494):  0.030605303\n",
      "Test loss at epoch (1494):  0.017360961\n",
      "Train loss at epoch (1495):  0.030608358\n",
      "Test loss at epoch (1495):  0.017360302\n",
      "Train loss at epoch (1496):  0.030810172\n",
      "Test loss at epoch (1496):  0.017362233\n",
      "Train loss at epoch (1497):  0.030419085\n",
      "Test loss at epoch (1497):  0.01735151\n",
      "Train loss at epoch (1498):  0.03070117\n",
      "Test loss at epoch (1498):  0.01730219\n",
      "Train loss at epoch (1499):  0.030411545\n",
      "Test loss at epoch (1499):  0.017601069\n",
      "Train loss at epoch (1500):  0.03067817\n",
      "Test loss at epoch (1500):  0.017417856\n",
      "Train loss at epoch (1501):  0.03051388\n",
      "Test loss at epoch (1501):  0.017487273\n",
      "Train loss at epoch (1502):  0.030668573\n",
      "Test loss at epoch (1502):  0.01732798\n",
      "Train loss at epoch (1503):  0.030468568\n",
      "Test loss at epoch (1503):  0.017355906\n",
      "Train loss at epoch (1504):  0.030682882\n",
      "Test loss at epoch (1504):  0.017353728\n",
      "Train loss at epoch (1505):  0.03056837\n",
      "Test loss at epoch (1505):  0.017345183\n",
      "Train loss at epoch (1506):  0.030340698\n",
      "Test loss at epoch (1506):  0.017330654\n",
      "Train loss at epoch (1507):  0.030615758\n",
      "Test loss at epoch (1507):  0.017478952\n",
      "Train loss at epoch (1508):  0.030587506\n",
      "Test loss at epoch (1508):  0.01743103\n",
      "Train loss at epoch (1509):  0.030545045\n",
      "Test loss at epoch (1509):  0.017332902\n",
      "Train loss at epoch (1510):  0.030768894\n",
      "Test loss at epoch (1510):  0.017319584\n",
      "Train loss at epoch (1511):  0.030663729\n",
      "Test loss at epoch (1511):  0.017379351\n",
      "Train loss at epoch (1512):  0.030446677\n",
      "Test loss at epoch (1512):  0.01732603\n",
      "Train loss at epoch (1513):  0.030534754\n",
      "Test loss at epoch (1513):  0.017340181\n",
      "Train loss at epoch (1514):  0.030658241\n",
      "Test loss at epoch (1514):  0.017228952\n",
      "Train loss at epoch (1515):  0.030544395\n",
      "Test loss at epoch (1515):  0.01742204\n",
      "Train loss at epoch (1516):  0.030890644\n",
      "Test loss at epoch (1516):  0.017227484\n",
      "Train loss at epoch (1517):  0.030893214\n",
      "Test loss at epoch (1517):  0.017335596\n",
      "Train loss at epoch (1518):  0.030607326\n",
      "Test loss at epoch (1518):  0.01748549\n",
      "Train loss at epoch (1519):  0.030682258\n",
      "Test loss at epoch (1519):  0.017276034\n",
      "Train loss at epoch (1520):  0.030434726\n",
      "Test loss at epoch (1520):  0.017316466\n",
      "Train loss at epoch (1521):  0.030572062\n",
      "Test loss at epoch (1521):  0.017559491\n",
      "Train loss at epoch (1522):  0.030714985\n",
      "Test loss at epoch (1522):  0.017386196\n",
      "Train loss at epoch (1523):  0.03063653\n",
      "Test loss at epoch (1523):  0.017443458\n",
      "Train loss at epoch (1524):  0.030593552\n",
      "Test loss at epoch (1524):  0.017332051\n",
      "Train loss at epoch (1525):  0.030479206\n",
      "Test loss at epoch (1525):  0.01737135\n",
      "Train loss at epoch (1526):  0.030459963\n",
      "Test loss at epoch (1526):  0.017358087\n",
      "Train loss at epoch (1527):  0.030550709\n",
      "Test loss at epoch (1527):  0.017357871\n",
      "Train loss at epoch (1528):  0.03052941\n",
      "Test loss at epoch (1528):  0.017335411\n",
      "Train loss at epoch (1529):  0.030517785\n",
      "Test loss at epoch (1529):  0.017353484\n",
      "Train loss at epoch (1530):  0.030468734\n",
      "Test loss at epoch (1530):  0.017278813\n",
      "Train loss at epoch (1531):  0.030591825\n",
      "Test loss at epoch (1531):  0.017377289\n",
      "Train loss at epoch (1532):  0.030550003\n",
      "Test loss at epoch (1532):  0.017438605\n",
      "Train loss at epoch (1533):  0.030473229\n",
      "Test loss at epoch (1533):  0.017272659\n",
      "Train loss at epoch (1534):  0.030435512\n",
      "Test loss at epoch (1534):  0.017338546\n",
      "Train loss at epoch (1535):  0.030576615\n",
      "Test loss at epoch (1535):  0.017394671\n",
      "Train loss at epoch (1536):  0.030418135\n",
      "Test loss at epoch (1536):  0.017434116\n",
      "Train loss at epoch (1537):  0.0306451\n",
      "Test loss at epoch (1537):  0.01739074\n",
      "Train loss at epoch (1538):  0.030508429\n",
      "Test loss at epoch (1538):  0.017337717\n",
      "Train loss at epoch (1539):  0.030552728\n",
      "Test loss at epoch (1539):  0.01731035\n",
      "Train loss at epoch (1540):  0.030638937\n",
      "Test loss at epoch (1540):  0.017432837\n",
      "Train loss at epoch (1541):  0.030636778\n",
      "Test loss at epoch (1541):  0.01728733\n",
      "Train loss at epoch (1542):  0.030353002\n",
      "Test loss at epoch (1542):  0.017254755\n",
      "Train loss at epoch (1543):  0.030496234\n",
      "Test loss at epoch (1543):  0.017298363\n",
      "Train loss at epoch (1544):  0.030559089\n",
      "Test loss at epoch (1544):  0.017448805\n",
      "Train loss at epoch (1545):  0.03039065\n",
      "Test loss at epoch (1545):  0.017279875\n",
      "Train loss at epoch (1546):  0.030628068\n",
      "Test loss at epoch (1546):  0.017253464\n",
      "Train loss at epoch (1547):  0.030483555\n",
      "Test loss at epoch (1547):  0.017215138\n",
      "Train loss at epoch (1548):  0.030513488\n",
      "Test loss at epoch (1548):  0.017302347\n",
      "Train loss at epoch (1549):  0.030492608\n",
      "Test loss at epoch (1549):  0.017289419\n",
      "Train loss at epoch (1550):  0.03058479\n",
      "Test loss at epoch (1550):  0.017281815\n",
      "Train loss at epoch (1551):  0.030477788\n",
      "Test loss at epoch (1551):  0.017290942\n",
      "Train loss at epoch (1552):  0.030370817\n",
      "Test loss at epoch (1552):  0.017339407\n",
      "Train loss at epoch (1553):  0.030558093\n",
      "Test loss at epoch (1553):  0.01732886\n",
      "Train loss at epoch (1554):  0.030666567\n",
      "Test loss at epoch (1554):  0.017263895\n",
      "Train loss at epoch (1555):  0.030370316\n",
      "Test loss at epoch (1555):  0.017334813\n",
      "Train loss at epoch (1556):  0.030581709\n",
      "Test loss at epoch (1556):  0.017307784\n",
      "Train loss at epoch (1557):  0.030478776\n",
      "Test loss at epoch (1557):  0.017329436\n",
      "Train loss at epoch (1558):  0.03047523\n",
      "Test loss at epoch (1558):  0.017368244\n",
      "Train loss at epoch (1559):  0.030451208\n",
      "Test loss at epoch (1559):  0.017275393\n",
      "Train loss at epoch (1560):  0.030445518\n",
      "Test loss at epoch (1560):  0.017330058\n",
      "Train loss at epoch (1561):  0.03049626\n",
      "Test loss at epoch (1561):  0.017341837\n",
      "Train loss at epoch (1562):  0.030445013\n",
      "Test loss at epoch (1562):  0.017291574\n",
      "Train loss at epoch (1563):  0.030544452\n",
      "Test loss at epoch (1563):  0.017340932\n",
      "Train loss at epoch (1564):  0.030266806\n",
      "Test loss at epoch (1564):  0.017323432\n",
      "Train loss at epoch (1565):  0.030203432\n",
      "Test loss at epoch (1565):  0.017295558\n",
      "Train loss at epoch (1566):  0.03043613\n",
      "Test loss at epoch (1566):  0.01736797\n",
      "Train loss at epoch (1567):  0.03028623\n",
      "Test loss at epoch (1567):  0.017324125\n",
      "Train loss at epoch (1568):  0.030429643\n",
      "Test loss at epoch (1568):  0.017246792\n",
      "Train loss at epoch (1569):  0.030495172\n",
      "Test loss at epoch (1569):  0.017360112\n",
      "Train loss at epoch (1570):  0.03042729\n",
      "Test loss at epoch (1570):  0.01729083\n",
      "Train loss at epoch (1571):  0.030285709\n",
      "Test loss at epoch (1571):  0.0173136\n",
      "Train loss at epoch (1572):  0.030437136\n",
      "Test loss at epoch (1572):  0.017231727\n",
      "Train loss at epoch (1573):  0.030437253\n",
      "Test loss at epoch (1573):  0.017381476\n",
      "Train loss at epoch (1574):  0.030232506\n",
      "Test loss at epoch (1574):  0.01727009\n",
      "Train loss at epoch (1575):  0.030170988\n",
      "Test loss at epoch (1575):  0.017320964\n",
      "Train loss at epoch (1576):  0.030562375\n",
      "Test loss at epoch (1576):  0.017228032\n",
      "Train loss at epoch (1577):  0.030544199\n",
      "Test loss at epoch (1577):  0.017295852\n",
      "Train loss at epoch (1578):  0.03054887\n",
      "Test loss at epoch (1578):  0.017327333\n",
      "Train loss at epoch (1579):  0.03060254\n",
      "Test loss at epoch (1579):  0.017241152\n",
      "Train loss at epoch (1580):  0.030468501\n",
      "Test loss at epoch (1580):  0.017365472\n",
      "Train loss at epoch (1581):  0.03043205\n",
      "Test loss at epoch (1581):  0.017264973\n",
      "Train loss at epoch (1582):  0.030415306\n",
      "Test loss at epoch (1582):  0.017264461\n",
      "Train loss at epoch (1583):  0.030498533\n",
      "Test loss at epoch (1583):  0.01727926\n",
      "Train loss at epoch (1584):  0.030279987\n",
      "Test loss at epoch (1584):  0.017295951\n",
      "Train loss at epoch (1585):  0.030314518\n",
      "Test loss at epoch (1585):  0.017281624\n",
      "Train loss at epoch (1586):  0.030372009\n",
      "Test loss at epoch (1586):  0.017309839\n",
      "Train loss at epoch (1587):  0.030493839\n",
      "Test loss at epoch (1587):  0.017249683\n",
      "Train loss at epoch (1588):  0.030375035\n",
      "Test loss at epoch (1588):  0.017463084\n",
      "Train loss at epoch (1589):  0.030469602\n",
      "Test loss at epoch (1589):  0.017219681\n",
      "Train loss at epoch (1590):  0.03042297\n",
      "Test loss at epoch (1590):  0.01723432\n",
      "Train loss at epoch (1591):  0.030499827\n",
      "Test loss at epoch (1591):  0.017277861\n",
      "Train loss at epoch (1592):  0.030470155\n",
      "Test loss at epoch (1592):  0.017258817\n",
      "Train loss at epoch (1593):  0.030592203\n",
      "Test loss at epoch (1593):  0.017238464\n",
      "Train loss at epoch (1594):  0.030534554\n",
      "Test loss at epoch (1594):  0.0172557\n",
      "Train loss at epoch (1595):  0.03031078\n",
      "Test loss at epoch (1595):  0.017438317\n",
      "Train loss at epoch (1596):  0.030281983\n",
      "Test loss at epoch (1596):  0.017328767\n",
      "Train loss at epoch (1597):  0.030196803\n",
      "Test loss at epoch (1597):  0.017382128\n",
      "Train loss at epoch (1598):  0.030464096\n",
      "Test loss at epoch (1598):  0.017246986\n",
      "Train loss at epoch (1599):  0.030197935\n",
      "Test loss at epoch (1599):  0.017266857\n",
      "Train loss at epoch (1600):  0.030255187\n",
      "Test loss at epoch (1600):  0.017221823\n",
      "Train loss at epoch (1601):  0.030332657\n",
      "Test loss at epoch (1601):  0.017226448\n",
      "Train loss at epoch (1602):  0.030202627\n",
      "Test loss at epoch (1602):  0.017206104\n",
      "Train loss at epoch (1603):  0.030392269\n",
      "Test loss at epoch (1603):  0.017232323\n",
      "Train loss at epoch (1604):  0.030248076\n",
      "Test loss at epoch (1604):  0.017431438\n",
      "Train loss at epoch (1605):  0.030376691\n",
      "Test loss at epoch (1605):  0.01725941\n",
      "Train loss at epoch (1606):  0.03035457\n",
      "Test loss at epoch (1606):  0.017419778\n",
      "Train loss at epoch (1607):  0.03034906\n",
      "Test loss at epoch (1607):  0.017259374\n",
      "Train loss at epoch (1608):  0.030286731\n",
      "Test loss at epoch (1608):  0.017218988\n",
      "Train loss at epoch (1609):  0.030168213\n",
      "Test loss at epoch (1609):  0.017253308\n",
      "Train loss at epoch (1610):  0.03032562\n",
      "Test loss at epoch (1610):  0.017223466\n",
      "Train loss at epoch (1611):  0.030344864\n",
      "Test loss at epoch (1611):  0.017266842\n",
      "Train loss at epoch (1612):  0.030461805\n",
      "Test loss at epoch (1612):  0.017411022\n",
      "Train loss at epoch (1613):  0.03046032\n",
      "Test loss at epoch (1613):  0.017273122\n",
      "Train loss at epoch (1614):  0.030436598\n",
      "Test loss at epoch (1614):  0.017281847\n",
      "Train loss at epoch (1615):  0.03039018\n",
      "Test loss at epoch (1615):  0.017251154\n",
      "Train loss at epoch (1616):  0.030211281\n",
      "Test loss at epoch (1616):  0.017316032\n",
      "Train loss at epoch (1617):  0.030305669\n",
      "Test loss at epoch (1617):  0.017286493\n",
      "Train loss at epoch (1618):  0.03040665\n",
      "Test loss at epoch (1618):  0.017272284\n",
      "Train loss at epoch (1619):  0.030484663\n",
      "Test loss at epoch (1619):  0.01738156\n",
      "Train loss at epoch (1620):  0.030285237\n",
      "Test loss at epoch (1620):  0.017266503\n",
      "Train loss at epoch (1621):  0.030149136\n",
      "Test loss at epoch (1621):  0.017257534\n",
      "Train loss at epoch (1622):  0.030309336\n",
      "Test loss at epoch (1622):  0.017355349\n",
      "Train loss at epoch (1623):  0.030246649\n",
      "Test loss at epoch (1623):  0.01724413\n",
      "Train loss at epoch (1624):  0.030383322\n",
      "Test loss at epoch (1624):  0.017221395\n",
      "Train loss at epoch (1625):  0.030465517\n",
      "Test loss at epoch (1625):  0.017229132\n",
      "Train loss at epoch (1626):  0.030277181\n",
      "Test loss at epoch (1626):  0.017236494\n",
      "Train loss at epoch (1627):  0.030239578\n",
      "Test loss at epoch (1627):  0.01726015\n",
      "Train loss at epoch (1628):  0.030366026\n",
      "Test loss at epoch (1628):  0.017421663\n",
      "Train loss at epoch (1629):  0.030125268\n",
      "Test loss at epoch (1629):  0.017272905\n",
      "Train loss at epoch (1630):  0.030463826\n",
      "Test loss at epoch (1630):  0.01727665\n",
      "Train loss at epoch (1631):  0.030276889\n",
      "Test loss at epoch (1631):  0.017263502\n",
      "Train loss at epoch (1632):  0.030187504\n",
      "Test loss at epoch (1632):  0.017253997\n",
      "Train loss at epoch (1633):  0.030371148\n",
      "Test loss at epoch (1633):  0.017205589\n",
      "Train loss at epoch (1634):  0.030415643\n",
      "Test loss at epoch (1634):  0.017239397\n",
      "Train loss at epoch (1635):  0.030448148\n",
      "Test loss at epoch (1635):  0.017337998\n",
      "Train loss at epoch (1636):  0.030151453\n",
      "Test loss at epoch (1636):  0.017313177\n",
      "Train loss at epoch (1637):  0.030371683\n",
      "Test loss at epoch (1637):  0.017175267\n",
      "Train loss at epoch (1638):  0.030177787\n",
      "Test loss at epoch (1638):  0.017247872\n",
      "Train loss at epoch (1639):  0.030340675\n",
      "Test loss at epoch (1639):  0.017224139\n",
      "Train loss at epoch (1640):  0.030298214\n",
      "Test loss at epoch (1640):  0.017440572\n",
      "Train loss at epoch (1641):  0.030113373\n",
      "Test loss at epoch (1641):  0.017322147\n",
      "Train loss at epoch (1642):  0.030504623\n",
      "Test loss at epoch (1642):  0.017252792\n",
      "Train loss at epoch (1643):  0.030354843\n",
      "Test loss at epoch (1643):  0.017210044\n",
      "Train loss at epoch (1644):  0.030272705\n",
      "Test loss at epoch (1644):  0.017244548\n",
      "Train loss at epoch (1645):  0.030358732\n",
      "Test loss at epoch (1645):  0.0174189\n",
      "Train loss at epoch (1646):  0.03036437\n",
      "Test loss at epoch (1646):  0.017262045\n",
      "Train loss at epoch (1647):  0.03027548\n",
      "Test loss at epoch (1647):  0.017243221\n",
      "Train loss at epoch (1648):  0.030397205\n",
      "Test loss at epoch (1648):  0.017329866\n",
      "Train loss at epoch (1649):  0.030218076\n",
      "Test loss at epoch (1649):  0.017261969\n",
      "Train loss at epoch (1650):  0.030308925\n",
      "Test loss at epoch (1650):  0.017332222\n",
      "Train loss at epoch (1651):  0.030421829\n",
      "Test loss at epoch (1651):  0.017229006\n",
      "Train loss at epoch (1652):  0.03035391\n",
      "Test loss at epoch (1652):  0.017238967\n",
      "Train loss at epoch (1653):  0.030330038\n",
      "Test loss at epoch (1653):  0.017202757\n",
      "Train loss at epoch (1654):  0.030264381\n",
      "Test loss at epoch (1654):  0.017309045\n",
      "Train loss at epoch (1655):  0.030293154\n",
      "Test loss at epoch (1655):  0.01742262\n",
      "Train loss at epoch (1656):  0.030270163\n",
      "Test loss at epoch (1656):  0.017410077\n",
      "Train loss at epoch (1657):  0.030115338\n",
      "Test loss at epoch (1657):  0.017243486\n",
      "Train loss at epoch (1658):  0.030227134\n",
      "Test loss at epoch (1658):  0.01724921\n",
      "Train loss at epoch (1659):  0.030121906\n",
      "Test loss at epoch (1659):  0.01723734\n",
      "Train loss at epoch (1660):  0.030378714\n",
      "Test loss at epoch (1660):  0.017223237\n",
      "Train loss at epoch (1661):  0.030292314\n",
      "Test loss at epoch (1661):  0.017230665\n",
      "Train loss at epoch (1662):  0.030326853\n",
      "Test loss at epoch (1662):  0.017281944\n",
      "Train loss at epoch (1663):  0.030182086\n",
      "Test loss at epoch (1663):  0.017231822\n",
      "Train loss at epoch (1664):  0.030309655\n",
      "Test loss at epoch (1664):  0.01723212\n",
      "Train loss at epoch (1665):  0.030236965\n",
      "Test loss at epoch (1665):  0.017213421\n",
      "Train loss at epoch (1666):  0.030217705\n",
      "Test loss at epoch (1666):  0.017225107\n",
      "Train loss at epoch (1667):  0.030085538\n",
      "Test loss at epoch (1667):  0.01731536\n",
      "Train loss at epoch (1668):  0.030331694\n",
      "Test loss at epoch (1668):  0.017201677\n",
      "Train loss at epoch (1669):  0.030331211\n",
      "Test loss at epoch (1669):  0.017335102\n",
      "Train loss at epoch (1670):  0.03020982\n",
      "Test loss at epoch (1670):  0.017223105\n",
      "Train loss at epoch (1671):  0.030013058\n",
      "Test loss at epoch (1671):  0.01720763\n",
      "Train loss at epoch (1672):  0.03011802\n",
      "Test loss at epoch (1672):  0.017239269\n",
      "Train loss at epoch (1673):  0.030289223\n",
      "Test loss at epoch (1673):  0.017329682\n",
      "Train loss at epoch (1674):  0.030280523\n",
      "Test loss at epoch (1674):  0.017293526\n",
      "Train loss at epoch (1675):  0.030442255\n",
      "Test loss at epoch (1675):  0.017249225\n",
      "Train loss at epoch (1676):  0.029984172\n",
      "Test loss at epoch (1676):  0.017184136\n",
      "Train loss at epoch (1677):  0.030300299\n",
      "Test loss at epoch (1677):  0.017364366\n",
      "Train loss at epoch (1678):  0.030318988\n",
      "Test loss at epoch (1678):  0.017194178\n",
      "Train loss at epoch (1679):  0.03033647\n",
      "Test loss at epoch (1679):  0.01727331\n",
      "Train loss at epoch (1680):  0.030263027\n",
      "Test loss at epoch (1680):  0.017219305\n",
      "Train loss at epoch (1681):  0.030275596\n",
      "Test loss at epoch (1681):  0.017239165\n",
      "Train loss at epoch (1682):  0.030117769\n",
      "Test loss at epoch (1682):  0.017212901\n",
      "Train loss at epoch (1683):  0.030311981\n",
      "Test loss at epoch (1683):  0.01728126\n",
      "Train loss at epoch (1684):  0.030031987\n",
      "Test loss at epoch (1684):  0.017260445\n",
      "Train loss at epoch (1685):  0.030303461\n",
      "Test loss at epoch (1685):  0.017283179\n",
      "Train loss at epoch (1686):  0.030308466\n",
      "Test loss at epoch (1686):  0.017222583\n",
      "Train loss at epoch (1687):  0.030192172\n",
      "Test loss at epoch (1687):  0.0172911\n",
      "Train loss at epoch (1688):  0.03018075\n",
      "Test loss at epoch (1688):  0.017236669\n",
      "Train loss at epoch (1689):  0.030166317\n",
      "Test loss at epoch (1689):  0.01721837\n",
      "Train loss at epoch (1690):  0.030260546\n",
      "Test loss at epoch (1690):  0.017200332\n",
      "Train loss at epoch (1691):  0.030121699\n",
      "Test loss at epoch (1691):  0.017240232\n",
      "Train loss at epoch (1692):  0.030246891\n",
      "Test loss at epoch (1692):  0.01722669\n",
      "Train loss at epoch (1693):  0.030178213\n",
      "Test loss at epoch (1693):  0.01724948\n",
      "Train loss at epoch (1694):  0.030266415\n",
      "Test loss at epoch (1694):  0.01726923\n",
      "Train loss at epoch (1695):  0.029930543\n",
      "Test loss at epoch (1695):  0.017172469\n",
      "Train loss at epoch (1696):  0.03021394\n",
      "Test loss at epoch (1696):  0.017230362\n",
      "Train loss at epoch (1697):  0.03020757\n",
      "Test loss at epoch (1697):  0.017240772\n",
      "Train loss at epoch (1698):  0.029989004\n",
      "Test loss at epoch (1698):  0.017173681\n",
      "Train loss at epoch (1699):  0.030162951\n",
      "Test loss at epoch (1699):  0.017214756\n",
      "Train loss at epoch (1700):  0.030222924\n",
      "Test loss at epoch (1700):  0.017245142\n",
      "Train loss at epoch (1701):  0.029991519\n",
      "Test loss at epoch (1701):  0.017163647\n",
      "Train loss at epoch (1702):  0.029924873\n",
      "Test loss at epoch (1702):  0.017251417\n",
      "Train loss at epoch (1703):  0.030181179\n",
      "Test loss at epoch (1703):  0.01720464\n",
      "Train loss at epoch (1704):  0.030264715\n",
      "Test loss at epoch (1704):  0.017236728\n",
      "Train loss at epoch (1705):  0.030249376\n",
      "Test loss at epoch (1705):  0.017185537\n",
      "Train loss at epoch (1706):  0.030257305\n",
      "Test loss at epoch (1706):  0.017234715\n",
      "Train loss at epoch (1707):  0.03045357\n",
      "Test loss at epoch (1707):  0.017249672\n",
      "Train loss at epoch (1708):  0.030158147\n",
      "Test loss at epoch (1708):  0.017240014\n",
      "Train loss at epoch (1709):  0.030318493\n",
      "Test loss at epoch (1709):  0.017246302\n",
      "Train loss at epoch (1710):  0.030185133\n",
      "Test loss at epoch (1710):  0.017203832\n",
      "Train loss at epoch (1711):  0.030380271\n",
      "Test loss at epoch (1711):  0.01721172\n",
      "Train loss at epoch (1712):  0.03022072\n",
      "Test loss at epoch (1712):  0.017270079\n",
      "Train loss at epoch (1713):  0.030142866\n",
      "Test loss at epoch (1713):  0.01724994\n",
      "Train loss at epoch (1714):  0.030110246\n",
      "Test loss at epoch (1714):  0.017171012\n",
      "Train loss at epoch (1715):  0.030294431\n",
      "Test loss at epoch (1715):  0.017190322\n",
      "Train loss at epoch (1716):  0.030207558\n",
      "Test loss at epoch (1716):  0.017203188\n",
      "Train loss at epoch (1717):  0.029969886\n",
      "Test loss at epoch (1717):  0.01727643\n",
      "Train loss at epoch (1718):  0.03011404\n",
      "Test loss at epoch (1718):  0.017222883\n",
      "Train loss at epoch (1719):  0.030398067\n",
      "Test loss at epoch (1719):  0.017292932\n",
      "Train loss at epoch (1720):  0.030286925\n",
      "Test loss at epoch (1720):  0.017237082\n",
      "Train loss at epoch (1721):  0.03014745\n",
      "Test loss at epoch (1721):  0.01720129\n",
      "Train loss at epoch (1722):  0.030179258\n",
      "Test loss at epoch (1722):  0.017196495\n",
      "Train loss at epoch (1723):  0.030038271\n",
      "Test loss at epoch (1723):  0.017215163\n",
      "Train loss at epoch (1724):  0.03019523\n",
      "Test loss at epoch (1724):  0.0172892\n",
      "Train loss at epoch (1725):  0.03007809\n",
      "Test loss at epoch (1725):  0.017229263\n",
      "Train loss at epoch (1726):  0.030203985\n",
      "Test loss at epoch (1726):  0.017167702\n",
      "Train loss at epoch (1727):  0.030402547\n",
      "Test loss at epoch (1727):  0.01717558\n",
      "Train loss at epoch (1728):  0.03013187\n",
      "Test loss at epoch (1728):  0.01720144\n",
      "Train loss at epoch (1729):  0.030048164\n",
      "Test loss at epoch (1729):  0.01718766\n",
      "Train loss at epoch (1730):  0.030234998\n",
      "Test loss at epoch (1730):  0.017202081\n",
      "Train loss at epoch (1731):  0.030226734\n",
      "Test loss at epoch (1731):  0.017216697\n",
      "Train loss at epoch (1732):  0.03020221\n",
      "Test loss at epoch (1732):  0.017210836\n",
      "Train loss at epoch (1733):  0.030256141\n",
      "Test loss at epoch (1733):  0.01722174\n",
      "Train loss at epoch (1734):  0.0302205\n",
      "Test loss at epoch (1734):  0.017212076\n",
      "Train loss at epoch (1735):  0.030126018\n",
      "Test loss at epoch (1735):  0.017229818\n",
      "Train loss at epoch (1736):  0.030216765\n",
      "Test loss at epoch (1736):  0.01719718\n",
      "Train loss at epoch (1737):  0.030163316\n",
      "Test loss at epoch (1737):  0.017220432\n",
      "Train loss at epoch (1738):  0.03009968\n",
      "Test loss at epoch (1738):  0.017169245\n",
      "Train loss at epoch (1739):  0.030208439\n",
      "Test loss at epoch (1739):  0.017194543\n",
      "Train loss at epoch (1740):  0.030214326\n",
      "Test loss at epoch (1740):  0.017178478\n",
      "Train loss at epoch (1741):  0.030200629\n",
      "Test loss at epoch (1741):  0.017174497\n",
      "Train loss at epoch (1742):  0.030058542\n",
      "Test loss at epoch (1742):  0.017139703\n",
      "Train loss at epoch (1743):  0.030341638\n",
      "Test loss at epoch (1743):  0.01721637\n",
      "Train loss at epoch (1744):  0.030415086\n",
      "Test loss at epoch (1744):  0.017132476\n",
      "Train loss at epoch (1745):  0.03011111\n",
      "Test loss at epoch (1745):  0.017226838\n",
      "Train loss at epoch (1746):  0.030191345\n",
      "Test loss at epoch (1746):  0.017246945\n",
      "Train loss at epoch (1747):  0.030079095\n",
      "Test loss at epoch (1747):  0.017201139\n",
      "Train loss at epoch (1748):  0.030122865\n",
      "Test loss at epoch (1748):  0.017180933\n",
      "Train loss at epoch (1749):  0.030098902\n",
      "Test loss at epoch (1749):  0.01715595\n",
      "Train loss at epoch (1750):  0.03016098\n",
      "Test loss at epoch (1750):  0.017190358\n",
      "Train loss at epoch (1751):  0.03008343\n",
      "Test loss at epoch (1751):  0.017211461\n",
      "Train loss at epoch (1752):  0.030211378\n",
      "Test loss at epoch (1752):  0.017187279\n",
      "Train loss at epoch (1753):  0.030044924\n",
      "Test loss at epoch (1753):  0.01716419\n",
      "Train loss at epoch (1754):  0.03026481\n",
      "Test loss at epoch (1754):  0.017162865\n",
      "Train loss at epoch (1755):  0.030308561\n",
      "Test loss at epoch (1755):  0.01725508\n",
      "Train loss at epoch (1756):  0.030105457\n",
      "Test loss at epoch (1756):  0.017200608\n",
      "Train loss at epoch (1757):  0.03009438\n",
      "Test loss at epoch (1757):  0.017176827\n",
      "Train loss at epoch (1758):  0.030196648\n",
      "Test loss at epoch (1758):  0.017173149\n",
      "Train loss at epoch (1759):  0.030151354\n",
      "Test loss at epoch (1759):  0.017174194\n",
      "Train loss at epoch (1760):  0.030210206\n",
      "Test loss at epoch (1760):  0.017203262\n",
      "Train loss at epoch (1761):  0.0302865\n",
      "Test loss at epoch (1761):  0.017223159\n",
      "Train loss at epoch (1762):  0.030208196\n",
      "Test loss at epoch (1762):  0.017204124\n",
      "Train loss at epoch (1763):  0.03010951\n",
      "Test loss at epoch (1763):  0.017209291\n",
      "Train loss at epoch (1764):  0.030269457\n",
      "Test loss at epoch (1764):  0.017244129\n",
      "Train loss at epoch (1765):  0.0304624\n",
      "Test loss at epoch (1765):  0.017158108\n",
      "Train loss at epoch (1766):  0.030083494\n",
      "Test loss at epoch (1766):  0.017224701\n",
      "Train loss at epoch (1767):  0.03014596\n",
      "Test loss at epoch (1767):  0.017178282\n",
      "Train loss at epoch (1768):  0.030284287\n",
      "Test loss at epoch (1768):  0.017266575\n",
      "Train loss at epoch (1769):  0.030159067\n",
      "Test loss at epoch (1769):  0.01720397\n",
      "Train loss at epoch (1770):  0.030144624\n",
      "Test loss at epoch (1770):  0.017244866\n",
      "Train loss at epoch (1771):  0.030223735\n",
      "Test loss at epoch (1771):  0.017191926\n",
      "Train loss at epoch (1772):  0.030070804\n",
      "Test loss at epoch (1772):  0.017211992\n",
      "Train loss at epoch (1773):  0.030244166\n",
      "Test loss at epoch (1773):  0.017185237\n",
      "Train loss at epoch (1774):  0.030084219\n",
      "Test loss at epoch (1774):  0.017202327\n",
      "Train loss at epoch (1775):  0.03028829\n",
      "Test loss at epoch (1775):  0.017174277\n",
      "Train loss at epoch (1776):  0.03012132\n",
      "Test loss at epoch (1776):  0.017227847\n",
      "Train loss at epoch (1777):  0.030155458\n",
      "Test loss at epoch (1777):  0.017227396\n",
      "Train loss at epoch (1778):  0.030211957\n",
      "Test loss at epoch (1778):  0.017210526\n",
      "Train loss at epoch (1779):  0.030355448\n",
      "Test loss at epoch (1779):  0.017223865\n",
      "Train loss at epoch (1780):  0.029996853\n",
      "Test loss at epoch (1780):  0.017185485\n",
      "Train loss at epoch (1781):  0.030188587\n",
      "Test loss at epoch (1781):  0.017206524\n",
      "Train loss at epoch (1782):  0.030203855\n",
      "Test loss at epoch (1782):  0.017179392\n",
      "Train loss at epoch (1783):  0.030122783\n",
      "Test loss at epoch (1783):  0.017212505\n",
      "Train loss at epoch (1784):  0.030106993\n",
      "Test loss at epoch (1784):  0.017194653\n",
      "Train loss at epoch (1785):  0.030171426\n",
      "Test loss at epoch (1785):  0.017209694\n",
      "Train loss at epoch (1786):  0.030278848\n",
      "Test loss at epoch (1786):  0.017187344\n",
      "Train loss at epoch (1787):  0.030082896\n",
      "Test loss at epoch (1787):  0.01723279\n",
      "Train loss at epoch (1788):  0.030094529\n",
      "Test loss at epoch (1788):  0.01718246\n",
      "Train loss at epoch (1789):  0.030167287\n",
      "Test loss at epoch (1789):  0.017231174\n",
      "Train loss at epoch (1790):  0.0302513\n",
      "Test loss at epoch (1790):  0.017210435\n",
      "Train loss at epoch (1791):  0.030069329\n",
      "Test loss at epoch (1791):  0.01721644\n",
      "Train loss at epoch (1792):  0.029975027\n",
      "Test loss at epoch (1792):  0.01719185\n",
      "Train loss at epoch (1793):  0.030113716\n",
      "Test loss at epoch (1793):  0.017193964\n",
      "Train loss at epoch (1794):  0.030160535\n",
      "Test loss at epoch (1794):  0.01722894\n",
      "Train loss at epoch (1795):  0.030307258\n",
      "Test loss at epoch (1795):  0.017211368\n",
      "Train loss at epoch (1796):  0.030240119\n",
      "Test loss at epoch (1796):  0.017206073\n",
      "Train loss at epoch (1797):  0.030134575\n",
      "Test loss at epoch (1797):  0.017170971\n",
      "Train loss at epoch (1798):  0.029930132\n",
      "Test loss at epoch (1798):  0.017180527\n",
      "Train loss at epoch (1799):  0.030130219\n",
      "Test loss at epoch (1799):  0.01725134\n",
      "Train loss at epoch (1800):  0.030183928\n",
      "Test loss at epoch (1800):  0.017184291\n",
      "Train loss at epoch (1801):  0.030144902\n",
      "Test loss at epoch (1801):  0.017170323\n",
      "Train loss at epoch (1802):  0.030185604\n",
      "Test loss at epoch (1802):  0.017193392\n",
      "Train loss at epoch (1803):  0.030113954\n",
      "Test loss at epoch (1803):  0.01721789\n",
      "Train loss at epoch (1804):  0.030094204\n",
      "Test loss at epoch (1804):  0.017204037\n",
      "Train loss at epoch (1805):  0.029982923\n",
      "Test loss at epoch (1805):  0.017152531\n",
      "Train loss at epoch (1806):  0.030080331\n",
      "Test loss at epoch (1806):  0.017165435\n",
      "Train loss at epoch (1807):  0.030100036\n",
      "Test loss at epoch (1807):  0.01717739\n",
      "Train loss at epoch (1808):  0.030063216\n",
      "Test loss at epoch (1808):  0.01716012\n",
      "Train loss at epoch (1809):  0.030029818\n",
      "Test loss at epoch (1809):  0.017167622\n",
      "Train loss at epoch (1810):  0.029981073\n",
      "Test loss at epoch (1810):  0.017167518\n",
      "Train loss at epoch (1811):  0.030104714\n",
      "Test loss at epoch (1811):  0.017166212\n",
      "Train loss at epoch (1812):  0.02994872\n",
      "Test loss at epoch (1812):  0.017205013\n",
      "Train loss at epoch (1813):  0.030166464\n",
      "Test loss at epoch (1813):  0.017217338\n",
      "Train loss at epoch (1814):  0.030052824\n",
      "Test loss at epoch (1814):  0.017137308\n",
      "Train loss at epoch (1815):  0.030053008\n",
      "Test loss at epoch (1815):  0.017210273\n",
      "Train loss at epoch (1816):  0.030042896\n",
      "Test loss at epoch (1816):  0.017168693\n",
      "Train loss at epoch (1817):  0.030168258\n",
      "Test loss at epoch (1817):  0.017176138\n",
      "Train loss at epoch (1818):  0.030018155\n",
      "Test loss at epoch (1818):  0.017151734\n",
      "Train loss at epoch (1819):  0.030185055\n",
      "Test loss at epoch (1819):  0.017179145\n",
      "Train loss at epoch (1820):  0.030010996\n",
      "Test loss at epoch (1820):  0.017226482\n",
      "Train loss at epoch (1821):  0.030173283\n",
      "Test loss at epoch (1821):  0.01718867\n",
      "Train loss at epoch (1822):  0.03005608\n",
      "Test loss at epoch (1822):  0.017190631\n",
      "Train loss at epoch (1823):  0.030363474\n",
      "Test loss at epoch (1823):  0.017187878\n",
      "Train loss at epoch (1824):  0.030170768\n",
      "Test loss at epoch (1824):  0.017186297\n",
      "Train loss at epoch (1825):  0.030039378\n",
      "Test loss at epoch (1825):  0.01715953\n",
      "Train loss at epoch (1826):  0.030070696\n",
      "Test loss at epoch (1826):  0.017187005\n",
      "Train loss at epoch (1827):  0.03007353\n",
      "Test loss at epoch (1827):  0.017184265\n",
      "Train loss at epoch (1828):  0.030042836\n",
      "Test loss at epoch (1828):  0.017155565\n",
      "Train loss at epoch (1829):  0.03006187\n",
      "Test loss at epoch (1829):  0.017186303\n",
      "Train loss at epoch (1830):  0.030226415\n",
      "Test loss at epoch (1830):  0.01719629\n",
      "Train loss at epoch (1831):  0.030493611\n",
      "Test loss at epoch (1831):  0.017223047\n",
      "Train loss at epoch (1832):  0.030071551\n",
      "Test loss at epoch (1832):  0.017211009\n",
      "Train loss at epoch (1833):  0.02996591\n",
      "Test loss at epoch (1833):  0.017189573\n",
      "Train loss at epoch (1834):  0.030074902\n",
      "Test loss at epoch (1834):  0.017214512\n",
      "Train loss at epoch (1835):  0.030033808\n",
      "Test loss at epoch (1835):  0.017155305\n",
      "Train loss at epoch (1836):  0.030202646\n",
      "Test loss at epoch (1836):  0.017169526\n",
      "Train loss at epoch (1837):  0.030199353\n",
      "Test loss at epoch (1837):  0.017191812\n",
      "Train loss at epoch (1838):  0.030108433\n",
      "Test loss at epoch (1838):  0.01715467\n",
      "Train loss at epoch (1839):  0.030064328\n",
      "Test loss at epoch (1839):  0.017194184\n",
      "Train loss at epoch (1840):  0.030190365\n",
      "Test loss at epoch (1840):  0.01717833\n",
      "Train loss at epoch (1841):  0.0300857\n",
      "Test loss at epoch (1841):  0.017155705\n",
      "Train loss at epoch (1842):  0.03011722\n",
      "Test loss at epoch (1842):  0.017172774\n",
      "Train loss at epoch (1843):  0.030112378\n",
      "Test loss at epoch (1843):  0.017177505\n",
      "Train loss at epoch (1844):  0.030097056\n",
      "Test loss at epoch (1844):  0.017216137\n",
      "Train loss at epoch (1845):  0.030209005\n",
      "Test loss at epoch (1845):  0.01719924\n",
      "Train loss at epoch (1846):  0.030190647\n",
      "Test loss at epoch (1846):  0.017205281\n",
      "Train loss at epoch (1847):  0.03003603\n",
      "Test loss at epoch (1847):  0.017168356\n",
      "Train loss at epoch (1848):  0.029951788\n",
      "Test loss at epoch (1848):  0.017176723\n",
      "Train loss at epoch (1849):  0.030326487\n",
      "Test loss at epoch (1849):  0.017192429\n",
      "Train loss at epoch (1850):  0.030239187\n",
      "Test loss at epoch (1850):  0.017181018\n",
      "Train loss at epoch (1851):  0.03013503\n",
      "Test loss at epoch (1851):  0.017222175\n",
      "Train loss at epoch (1852):  0.0302901\n",
      "Test loss at epoch (1852):  0.017183032\n",
      "Train loss at epoch (1853):  0.03024394\n",
      "Test loss at epoch (1853):  0.017169818\n",
      "Train loss at epoch (1854):  0.029912628\n",
      "Test loss at epoch (1854):  0.01720169\n",
      "Train loss at epoch (1855):  0.030142529\n",
      "Test loss at epoch (1855):  0.017167227\n",
      "Train loss at epoch (1856):  0.030195989\n",
      "Test loss at epoch (1856):  0.017163021\n",
      "Train loss at epoch (1857):  0.030075295\n",
      "Test loss at epoch (1857):  0.017182743\n",
      "Train loss at epoch (1858):  0.030082524\n",
      "Test loss at epoch (1858):  0.017200282\n",
      "Train loss at epoch (1859):  0.03021372\n",
      "Test loss at epoch (1859):  0.017167896\n",
      "Train loss at epoch (1860):  0.02998807\n",
      "Test loss at epoch (1860):  0.017170876\n",
      "Train loss at epoch (1861):  0.030226046\n",
      "Test loss at epoch (1861):  0.017159138\n",
      "Train loss at epoch (1862):  0.030182583\n",
      "Test loss at epoch (1862):  0.017155474\n",
      "Train loss at epoch (1863):  0.030492531\n",
      "Test loss at epoch (1863):  0.01715898\n",
      "Train loss at epoch (1864):  0.029978562\n",
      "Test loss at epoch (1864):  0.01716069\n",
      "Train loss at epoch (1865):  0.030224044\n",
      "Test loss at epoch (1865):  0.017168712\n",
      "Train loss at epoch (1866):  0.03013435\n",
      "Test loss at epoch (1866):  0.01717264\n",
      "Train loss at epoch (1867):  0.030267706\n",
      "Test loss at epoch (1867):  0.01716981\n",
      "Train loss at epoch (1868):  0.030028353\n",
      "Test loss at epoch (1868):  0.017175764\n",
      "Train loss at epoch (1869):  0.030048335\n",
      "Test loss at epoch (1869):  0.017164722\n",
      "Train loss at epoch (1870):  0.030120226\n",
      "Test loss at epoch (1870):  0.017168934\n",
      "Train loss at epoch (1871):  0.029891562\n",
      "Test loss at epoch (1871):  0.017161917\n",
      "Train loss at epoch (1872):  0.030052405\n",
      "Test loss at epoch (1872):  0.017174974\n",
      "Train loss at epoch (1873):  0.030113665\n",
      "Test loss at epoch (1873):  0.01716111\n",
      "Train loss at epoch (1874):  0.029922297\n",
      "Test loss at epoch (1874):  0.017191269\n",
      "Train loss at epoch (1875):  0.030162487\n",
      "Test loss at epoch (1875):  0.017157003\n",
      "Train loss at epoch (1876):  0.030043764\n",
      "Test loss at epoch (1876):  0.017159762\n",
      "Train loss at epoch (1877):  0.030072168\n",
      "Test loss at epoch (1877):  0.017174171\n",
      "Train loss at epoch (1878):  0.03022821\n",
      "Test loss at epoch (1878):  0.017165812\n",
      "Train loss at epoch (1879):  0.030094583\n",
      "Test loss at epoch (1879):  0.017168721\n",
      "Train loss at epoch (1880):  0.030369543\n",
      "Test loss at epoch (1880):  0.017172553\n",
      "Train loss at epoch (1881):  0.029946906\n",
      "Test loss at epoch (1881):  0.017171226\n",
      "Train loss at epoch (1882):  0.030431306\n",
      "Test loss at epoch (1882):  0.017179217\n",
      "Train loss at epoch (1883):  0.029868543\n",
      "Test loss at epoch (1883):  0.017177537\n",
      "Train loss at epoch (1884):  0.030022563\n",
      "Test loss at epoch (1884):  0.017175455\n",
      "Train loss at epoch (1885):  0.029915422\n",
      "Test loss at epoch (1885):  0.017182201\n",
      "Train loss at epoch (1886):  0.030210463\n",
      "Test loss at epoch (1886):  0.017156057\n",
      "Train loss at epoch (1887):  0.030113203\n",
      "Test loss at epoch (1887):  0.017144592\n",
      "Train loss at epoch (1888):  0.03007058\n",
      "Test loss at epoch (1888):  0.017166058\n",
      "Train loss at epoch (1889):  0.03018305\n",
      "Test loss at epoch (1889):  0.017164107\n",
      "Train loss at epoch (1890):  0.030094258\n",
      "Test loss at epoch (1890):  0.017157482\n",
      "Train loss at epoch (1891):  0.030128743\n",
      "Test loss at epoch (1891):  0.017170882\n",
      "Train loss at epoch (1892):  0.030116027\n",
      "Test loss at epoch (1892):  0.017164022\n",
      "Train loss at epoch (1893):  0.02996315\n",
      "Test loss at epoch (1893):  0.017172996\n",
      "Train loss at epoch (1894):  0.029986506\n",
      "Test loss at epoch (1894):  0.017166214\n",
      "Train loss at epoch (1895):  0.030134449\n",
      "Test loss at epoch (1895):  0.017176824\n",
      "Train loss at epoch (1896):  0.030203447\n",
      "Test loss at epoch (1896):  0.017172093\n",
      "Train loss at epoch (1897):  0.030323757\n",
      "Test loss at epoch (1897):  0.017145755\n",
      "Train loss at epoch (1898):  0.03016632\n",
      "Test loss at epoch (1898):  0.01715659\n",
      "Train loss at epoch (1899):  0.030053768\n",
      "Test loss at epoch (1899):  0.017157525\n",
      "Train loss at epoch (1900):  0.030152794\n",
      "Test loss at epoch (1900):  0.017155377\n",
      "Train loss at epoch (1901):  0.030295331\n",
      "Test loss at epoch (1901):  0.017155297\n",
      "Train loss at epoch (1902):  0.029812602\n",
      "Test loss at epoch (1902):  0.017171446\n",
      "Train loss at epoch (1903):  0.030258477\n",
      "Test loss at epoch (1903):  0.017160054\n",
      "Train loss at epoch (1904):  0.030109504\n",
      "Test loss at epoch (1904):  0.017174257\n",
      "Train loss at epoch (1905):  0.030052558\n",
      "Test loss at epoch (1905):  0.01717778\n",
      "Train loss at epoch (1906):  0.030098146\n",
      "Test loss at epoch (1906):  0.01714882\n",
      "Train loss at epoch (1907):  0.030203402\n",
      "Test loss at epoch (1907):  0.01714722\n",
      "Train loss at epoch (1908):  0.03025139\n",
      "Test loss at epoch (1908):  0.0171562\n",
      "Train loss at epoch (1909):  0.030008558\n",
      "Test loss at epoch (1909):  0.017156322\n",
      "Train loss at epoch (1910):  0.03015184\n",
      "Test loss at epoch (1910):  0.017180251\n",
      "Train loss at epoch (1911):  0.030133305\n",
      "Test loss at epoch (1911):  0.017184967\n",
      "Train loss at epoch (1912):  0.029939111\n",
      "Test loss at epoch (1912):  0.01717819\n",
      "Train loss at epoch (1913):  0.030290162\n",
      "Test loss at epoch (1913):  0.017152052\n",
      "Train loss at epoch (1914):  0.030062325\n",
      "Test loss at epoch (1914):  0.017164016\n",
      "Train loss at epoch (1915):  0.030262426\n",
      "Test loss at epoch (1915):  0.017148927\n",
      "Train loss at epoch (1916):  0.030002791\n",
      "Test loss at epoch (1916):  0.017163372\n",
      "Train loss at epoch (1917):  0.030037515\n",
      "Test loss at epoch (1917):  0.017162994\n",
      "Train loss at epoch (1918):  0.030076211\n",
      "Test loss at epoch (1918):  0.017167913\n",
      "Train loss at epoch (1919):  0.030142698\n",
      "Test loss at epoch (1919):  0.01716456\n",
      "Train loss at epoch (1920):  0.030094808\n",
      "Test loss at epoch (1920):  0.017165985\n",
      "Train loss at epoch (1921):  0.030214913\n",
      "Test loss at epoch (1921):  0.017157212\n",
      "Train loss at epoch (1922):  0.03018265\n",
      "Test loss at epoch (1922):  0.017169489\n",
      "Train loss at epoch (1923):  0.030005634\n",
      "Test loss at epoch (1923):  0.017172456\n",
      "Train loss at epoch (1924):  0.029843682\n",
      "Test loss at epoch (1924):  0.017179545\n",
      "Train loss at epoch (1925):  0.029715989\n",
      "Test loss at epoch (1925):  0.017151147\n",
      "Train loss at epoch (1926):  0.030141858\n",
      "Test loss at epoch (1926):  0.01715888\n",
      "Train loss at epoch (1927):  0.030077925\n",
      "Test loss at epoch (1927):  0.01715706\n",
      "Train loss at epoch (1928):  0.030206636\n",
      "Test loss at epoch (1928):  0.01716351\n",
      "Train loss at epoch (1929):  0.030003227\n",
      "Test loss at epoch (1929):  0.017177083\n",
      "Train loss at epoch (1930):  0.030285768\n",
      "Test loss at epoch (1930):  0.01719052\n",
      "Train loss at epoch (1931):  0.030253837\n",
      "Test loss at epoch (1931):  0.017184408\n",
      "Train loss at epoch (1932):  0.030052995\n",
      "Test loss at epoch (1932):  0.017157732\n",
      "Train loss at epoch (1933):  0.030112565\n",
      "Test loss at epoch (1933):  0.017183322\n",
      "Train loss at epoch (1934):  0.030192947\n",
      "Test loss at epoch (1934):  0.017154278\n",
      "Train loss at epoch (1935):  0.030024804\n",
      "Test loss at epoch (1935):  0.017201424\n",
      "Train loss at epoch (1936):  0.03041813\n",
      "Test loss at epoch (1936):  0.017160773\n",
      "Train loss at epoch (1937):  0.030120008\n",
      "Test loss at epoch (1937):  0.01718014\n",
      "Train loss at epoch (1938):  0.030176895\n",
      "Test loss at epoch (1938):  0.017157892\n",
      "Train loss at epoch (1939):  0.029974973\n",
      "Test loss at epoch (1939):  0.017146299\n",
      "Train loss at epoch (1940):  0.030323591\n",
      "Test loss at epoch (1940):  0.017147416\n",
      "Train loss at epoch (1941):  0.03001443\n",
      "Test loss at epoch (1941):  0.01714524\n",
      "Train loss at epoch (1942):  0.030083869\n",
      "Test loss at epoch (1942):  0.017162358\n",
      "Train loss at epoch (1943):  0.030245934\n",
      "Test loss at epoch (1943):  0.017166484\n",
      "Train loss at epoch (1944):  0.03011691\n",
      "Test loss at epoch (1944):  0.01716921\n",
      "Train loss at epoch (1945):  0.030134805\n",
      "Test loss at epoch (1945):  0.017173186\n",
      "Train loss at epoch (1946):  0.03010487\n",
      "Test loss at epoch (1946):  0.017167902\n",
      "Train loss at epoch (1947):  0.030163348\n",
      "Test loss at epoch (1947):  0.01717474\n",
      "Train loss at epoch (1948):  0.030061325\n",
      "Test loss at epoch (1948):  0.01715902\n",
      "Train loss at epoch (1949):  0.029918728\n",
      "Test loss at epoch (1949):  0.017169312\n",
      "Train loss at epoch (1950):  0.030122528\n",
      "Test loss at epoch (1950):  0.017170683\n",
      "Train loss at epoch (1951):  0.030153783\n",
      "Test loss at epoch (1951):  0.017152004\n",
      "Train loss at epoch (1952):  0.029873466\n",
      "Test loss at epoch (1952):  0.017167144\n",
      "Train loss at epoch (1953):  0.030145371\n",
      "Test loss at epoch (1953):  0.017163701\n",
      "Train loss at epoch (1954):  0.030091412\n",
      "Test loss at epoch (1954):  0.017161647\n",
      "Train loss at epoch (1955):  0.030105902\n",
      "Test loss at epoch (1955):  0.017187357\n",
      "Train loss at epoch (1956):  0.03010954\n",
      "Test loss at epoch (1956):  0.017181426\n",
      "Train loss at epoch (1957):  0.03005442\n",
      "Test loss at epoch (1957):  0.017163327\n",
      "Train loss at epoch (1958):  0.030074097\n",
      "Test loss at epoch (1958):  0.017167812\n",
      "Train loss at epoch (1959):  0.030185608\n",
      "Test loss at epoch (1959):  0.017146686\n",
      "Train loss at epoch (1960):  0.030118223\n",
      "Test loss at epoch (1960):  0.017155932\n",
      "Train loss at epoch (1961):  0.029997718\n",
      "Test loss at epoch (1961):  0.017156402\n",
      "Train loss at epoch (1962):  0.030113108\n",
      "Test loss at epoch (1962):  0.017159587\n",
      "Train loss at epoch (1963):  0.029873272\n",
      "Test loss at epoch (1963):  0.01716286\n",
      "Train loss at epoch (1964):  0.030054143\n",
      "Test loss at epoch (1964):  0.017168619\n",
      "Train loss at epoch (1965):  0.029931955\n",
      "Test loss at epoch (1965):  0.017153965\n",
      "Train loss at epoch (1966):  0.03012928\n",
      "Test loss at epoch (1966):  0.017178262\n",
      "Train loss at epoch (1967):  0.030065676\n",
      "Test loss at epoch (1967):  0.017155891\n",
      "Train loss at epoch (1968):  0.030073054\n",
      "Test loss at epoch (1968):  0.017173326\n",
      "Train loss at epoch (1969):  0.0301671\n",
      "Test loss at epoch (1969):  0.017162692\n",
      "Train loss at epoch (1970):  0.030190283\n",
      "Test loss at epoch (1970):  0.017171428\n",
      "Train loss at epoch (1971):  0.030102856\n",
      "Test loss at epoch (1971):  0.017166758\n",
      "Train loss at epoch (1972):  0.03008866\n",
      "Test loss at epoch (1972):  0.017173743\n",
      "Train loss at epoch (1973):  0.029967498\n",
      "Test loss at epoch (1973):  0.017179048\n",
      "Train loss at epoch (1974):  0.02984854\n",
      "Test loss at epoch (1974):  0.017154083\n",
      "Train loss at epoch (1975):  0.030180702\n",
      "Test loss at epoch (1975):  0.017163243\n",
      "Train loss at epoch (1976):  0.029902302\n",
      "Test loss at epoch (1976):  0.017166639\n",
      "Train loss at epoch (1977):  0.029994091\n",
      "Test loss at epoch (1977):  0.017149424\n",
      "Train loss at epoch (1978):  0.030112347\n",
      "Test loss at epoch (1978):  0.01715794\n",
      "Train loss at epoch (1979):  0.030099109\n",
      "Test loss at epoch (1979):  0.017176867\n",
      "Train loss at epoch (1980):  0.029901482\n",
      "Test loss at epoch (1980):  0.017162813\n",
      "Train loss at epoch (1981):  0.029796889\n",
      "Test loss at epoch (1981):  0.017157199\n",
      "Train loss at epoch (1982):  0.030100828\n",
      "Test loss at epoch (1982):  0.017138526\n",
      "Train loss at epoch (1983):  0.029951794\n",
      "Test loss at epoch (1983):  0.017157592\n",
      "Train loss at epoch (1984):  0.030267045\n",
      "Test loss at epoch (1984):  0.017168775\n",
      "Train loss at epoch (1985):  0.030037139\n",
      "Test loss at epoch (1985):  0.017179906\n",
      "Train loss at epoch (1986):  0.030289885\n",
      "Test loss at epoch (1986):  0.017151365\n",
      "Train loss at epoch (1987):  0.030220607\n",
      "Test loss at epoch (1987):  0.017155454\n",
      "Train loss at epoch (1988):  0.030179614\n",
      "Test loss at epoch (1988):  0.017157024\n",
      "Train loss at epoch (1989):  0.030114619\n",
      "Test loss at epoch (1989):  0.017169619\n",
      "Train loss at epoch (1990):  0.029956508\n",
      "Test loss at epoch (1990):  0.017174477\n",
      "Train loss at epoch (1991):  0.029973065\n",
      "Test loss at epoch (1991):  0.017154977\n",
      "Train loss at epoch (1992):  0.030204404\n",
      "Test loss at epoch (1992):  0.01717806\n",
      "Train loss at epoch (1993):  0.030074809\n",
      "Test loss at epoch (1993):  0.01716894\n",
      "Train loss at epoch (1994):  0.030130936\n",
      "Test loss at epoch (1994):  0.017167425\n",
      "Train loss at epoch (1995):  0.030006269\n",
      "Test loss at epoch (1995):  0.017179267\n",
      "Train loss at epoch (1996):  0.030194998\n",
      "Test loss at epoch (1996):  0.017152768\n",
      "Train loss at epoch (1997):  0.030185921\n",
      "Test loss at epoch (1997):  0.017156497\n",
      "Train loss at epoch (1998):  0.030115267\n",
      "Test loss at epoch (1998):  0.017175745\n",
      "Train loss at epoch (1999):  0.030002292\n",
      "Test loss at epoch (1999):  0.017174928\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "EPOCHS = 2000\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "decoder = TransformerDecoder(cfg, attn_factory)\n",
    "optimizer = SGD(decoder.parameters(), lr=1e-3, momentum=0.9)\n",
    "lr_scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "trainer = Trainer(\n",
    "    train_data_loader=train_data_loader,\n",
    "    test_data_loader=test_data_loader,\n",
    "    # optimizer=AdamW(decoder.parameters(), lr=1e-5),\n",
    "    optimizer=optimizer,\n",
    "    model=decoder,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    epochs=EPOCHS\n",
    ")\n",
    "\n",
    "def loss(predictions, target):\n",
    "    return ((predictions - target) ** 2).mean()\n",
    "\n",
    "trainer.train(loss_fn=nn.L1Loss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, n_tokens, inputs):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_len = inputs.size(-2)\n",
    "        for i in range(n_tokens):\n",
    "            inputs = torch.cat((inputs, model.predict(inputs)), dim=-2)\n",
    "        return inputs[:, input_len:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "decoder.eval()\n",
    "x_sample, y_sample = test_dataset[np.random.randint(0, TEST_SIZE)]\n",
    "predictions = predict(decoder, TARGET_SEQ_LEN, x_sample.unsqueeze(0)[:, :SOURCE_SEQ_LEN, :])\n",
    "labels = torch.cat((x_sample, y_sample[-1:]))\n",
    "\n",
    "plot_circle(\n",
    "    input=labels.detach().numpy()[0:],\n",
    "    prediction=torch.cat((labels[0:SOURCE_SEQ_LEN, :], predictions[0])).detach().numpy()[0:],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
