{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW, Optimizer, SGD\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from scratchers.vanilla_transformer.attn import Attention, MultiheadAttention\n",
    "from scratchers.vanilla_transformer.transformer import TransformerBlock, TransformerDecoder\n",
    "from scratchers.transformer_config import TransformerConfig\n",
    "from scratchers.transformer.attn import Attention as CachedAttention\n",
    "from scratchers.transformer.transformer import TransformerDecoder as CachedTransformerDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'scratchers' (namespace) from ['/Users/s-a-bakulin/mosquitto/ml/scratchers']>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scratchers\n",
    "importlib.reload(scratchers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_SEQ_LEN = 12\n",
    "TARGET_SEQ_LEN = 12\n",
    "\n",
    "DATA_SIZE = 1500\n",
    "TRAIN_SIZE = int(DATA_SIZE * 0.8)\n",
    "TEST_SIZE = DATA_SIZE - TRAIN_SIZE\n",
    "\n",
    "# radii = np.random.ranf((DATA_SIZE)) # * 9 + 1\n",
    "radii = np.ones((DATA_SIZE))\n",
    "starting_radian = np.random.ranf((DATA_SIZE)) * 2 * np.pi\n",
    "directions = np.random.randint(2, size=DATA_SIZE) * 2 - 1\n",
    "\n",
    "def to_cartesian(\n",
    "    radius: float,\n",
    "    start_radian: float, \n",
    "    direction: int\n",
    "):\n",
    "    delta = 2 * np.pi / (SOURCE_SEQ_LEN + TARGET_SEQ_LEN)\n",
    "    seq = np.array([\n",
    "        np.array([\n",
    "            radius * np.cos(start_radian + (i * direction * delta)),\n",
    "            radius * np.sin(start_radian + (i * direction * delta))\n",
    "        ])\n",
    "        for i in range(SOURCE_SEQ_LEN + TARGET_SEQ_LEN)\n",
    "    ])\n",
    "    source_seq = seq[:-1]\n",
    "    target_seq = seq[1:]\n",
    "    return source_seq, target_seq\n",
    "\n",
    "def make_circles_data():\n",
    "    X = np.empty((DATA_SIZE, (SOURCE_SEQ_LEN + TARGET_SEQ_LEN) - 1, 2))\n",
    "    Y = np.empty((DATA_SIZE, (SOURCE_SEQ_LEN + TARGET_SEQ_LEN) - 1, 2))\n",
    "    for idx, (radius, start_radian, direction) in enumerate(zip(radii, starting_radian, directions)):\n",
    "        x, y = to_cartesian(radius, start_radian, direction)\n",
    "        X[idx, :, :] = x\n",
    "        Y[idx, :, :] = y\n",
    "\n",
    "    return (\n",
    "        torch.from_numpy(X[:TRAIN_SIZE]).float(), \n",
    "        torch.from_numpy(Y[:TRAIN_SIZE]).float(),\n",
    "        torch.from_numpy(X[TRAIN_SIZE:]).float(), \n",
    "        torch.from_numpy(Y[TRAIN_SIZE:]).float()\n",
    "    )\n",
    "\n",
    "def make_squares_data():\n",
    "    X = np.empty((DATA_SIZE, (2 + 2) - 1, 2))\n",
    "    Y = np.empty((DATA_SIZE, (2 + 2) - 1, 2))\n",
    "\n",
    "    def map(elements):\n",
    "        res = []\n",
    "        for element in elements:\n",
    "            if element == 0:\n",
    "                res.append(np.array([-1, -1]))\n",
    "            elif element == 1:\n",
    "                res.append(np.array([1, -1]))\n",
    "            elif element == 2:\n",
    "                res.append(np.array([1, 1]))\n",
    "            elif element == 3:\n",
    "                res.append(np.array([-1, 1]))\n",
    "\n",
    "        return np.array(res)\n",
    "\n",
    "    first = np.random.randint(0, 4, size=(DATA_SIZE))\n",
    "    second = (first + 1) % 4\n",
    "    third = (second + 1) % 4\n",
    "    fourth = (third + 1) % 4\n",
    "    first = map(first)\n",
    "    second = map(second)\n",
    "    third = map(third)\n",
    "    fourth = map(fourth)\n",
    "\n",
    "    X[:, 0, :] = first\n",
    "    X[:, 1, :] = second\n",
    "    X[:, 2, :] = third\n",
    "    Y[:, 0, :] = second\n",
    "    Y[:, 1, :] = third\n",
    "    Y[:, 2, :] = fourth\n",
    "\n",
    "    return (\n",
    "        torch.from_numpy(X[:TRAIN_SIZE]).float(), \n",
    "        torch.from_numpy(Y[:TRAIN_SIZE]).float(),\n",
    "        torch.from_numpy(X[TRAIN_SIZE:]).float(), \n",
    "        torch.from_numpy(Y[TRAIN_SIZE:]).float()\n",
    "    )\n",
    "\n",
    "\n",
    "X_train, y_train, X_test, y_test = make_circles_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_circle(input, prediction, figsize=(6, 6)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    colors = cm.rainbow(np.linspace(0, 1, len(input)))\n",
    "    plt.scatter(\n",
    "        input[:, 0], input[:, 1], label=\"input\", marker=\"*\", s=250, alpha=0.5, color=colors\n",
    "    )\n",
    "    plt.scatter(prediction[:, 0], prediction[:, 1], label=\"prediction\", color=colors)\n",
    "    plt.legend()\n",
    "\n",
    "plot_circle(X_train[4], y_train[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = TransformerConfig(\n",
    "    input_size=2,\n",
    "    attn_d_k=32,\n",
    "    transformer_proj_dim=64,\n",
    "    dropout=0.2,\n",
    "    nlayers=2,\n",
    "    is_self_attn=False,\n",
    "    max_seq_len=TARGET_SEQ_LEN + SOURCE_SEQ_LEN,\n",
    "    nheads=2,\n",
    "    pre_layer_norm=True,\n",
    "    use_cache=True\n",
    ")\n",
    "\n",
    "def attn_factory(config: TransformerConfig):\n",
    "    return Attention(\n",
    "        d_k=config.attn_d_k,\n",
    "        # nheads=config.nheads,\n",
    "        is_self_attn=config.is_self_attn,\n",
    "        dropout=config.dropout\n",
    "    )\n",
    "\n",
    "def caching_attn_factory(config: TransformerConfig):\n",
    "    return CachedAttention(\n",
    "        d_k=config.attn_d_k,\n",
    "        dropout=config.dropout\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_data_loader: DataLoader,\n",
    "        test_data_loader: DataLoader,\n",
    "        optimizer: Optimizer,\n",
    "        model: nn.Module,\n",
    "        lr_scheduler = None,\n",
    "        epochs: int = 10,\n",
    "    ):\n",
    "        self.train_data_loader = train_data_loader\n",
    "        self.test_data_loader = test_data_loader\n",
    "        self.optimizer = optimizer\n",
    "        self.model = model\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def train(self, loss_fn: callable):\n",
    "        for epoch in range(self.epochs):\n",
    "            losses = []\n",
    "            self.model.train()\n",
    "            for x_batch, y_batch in self.train_data_loader:\n",
    "                out = self.model(x_batch)\n",
    "                loss = loss_fn(out, y_batch)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                losses.append(loss.detach().cpu().numpy())\n",
    "\n",
    "            if self.lr_scheduler is not None:\n",
    "                self.lr_scheduler.step()\n",
    "\n",
    "            print(f\"Train loss at epoch ({epoch}): \", np.array(losses).mean())\n",
    "\n",
    "            with torch.no_grad():\n",
    "                self.model.eval()\n",
    "                test_losses = []\n",
    "                for x_batch, y_batch in self.test_data_loader:\n",
    "                    out = self.model(x_batch)\n",
    "                    loss = loss_fn(out, y_batch)\n",
    "                    test_losses.append(loss.detach().cpu().numpy())\n",
    "\n",
    "                print(f\"Test loss at epoch ({epoch}): \", np.array(test_losses).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "EPOCHS = 1500\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "decoder = TransformerDecoder(cfg, attn_factory)\n",
    "optimizer = SGD(decoder.parameters(), lr=1e-4, momentum=0.9)\n",
    "lr_scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "trainer = Trainer(\n",
    "    train_data_loader=train_data_loader,\n",
    "    test_data_loader=test_data_loader,\n",
    "    # optimizer=AdamW(decoder.parameters(), lr=1e-5),\n",
    "    optimizer=optimizer,\n",
    "    model=decoder,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    epochs=EPOCHS\n",
    ")\n",
    "\n",
    "# trainer.train(loss_fn=nn.L1Loss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss at epoch (0):  0.64254284\n",
      "Test loss at epoch (0):  0.5401236\n",
      "Train loss at epoch (1):  0.5194171\n",
      "Test loss at epoch (1):  0.4548389\n",
      "Train loss at epoch (2):  0.4497382\n",
      "Test loss at epoch (2):  0.39429688\n",
      "Train loss at epoch (3):  0.39542273\n",
      "Test loss at epoch (3):  0.3440284\n",
      "Train loss at epoch (4):  0.3493704\n",
      "Test loss at epoch (4):  0.30080995\n",
      "Train loss at epoch (5):  0.31207958\n",
      "Test loss at epoch (5):  0.26429525\n",
      "Train loss at epoch (6):  0.28247172\n",
      "Test loss at epoch (6):  0.23614411\n",
      "Train loss at epoch (7):  0.26019448\n",
      "Test loss at epoch (7):  0.21579497\n",
      "Train loss at epoch (8):  0.24429993\n",
      "Test loss at epoch (8):  0.20142001\n",
      "Train loss at epoch (9):  0.2327241\n",
      "Test loss at epoch (9):  0.19101553\n",
      "Train loss at epoch (10):  0.22547412\n",
      "Test loss at epoch (10):  0.18327294\n",
      "Train loss at epoch (11):  0.21830198\n",
      "Test loss at epoch (11):  0.17671333\n",
      "Train loss at epoch (12):  0.21108925\n",
      "Test loss at epoch (12):  0.1711454\n",
      "Train loss at epoch (13):  0.20660947\n",
      "Test loss at epoch (13):  0.16675732\n",
      "Train loss at epoch (14):  0.2020983\n",
      "Test loss at epoch (14):  0.16261838\n",
      "Train loss at epoch (15):  0.19941925\n",
      "Test loss at epoch (15):  0.1590503\n",
      "Train loss at epoch (16):  0.19545768\n",
      "Test loss at epoch (16):  0.15612097\n",
      "Train loss at epoch (17):  0.19239901\n",
      "Test loss at epoch (17):  0.15331763\n",
      "Train loss at epoch (18):  0.18905029\n",
      "Test loss at epoch (18):  0.15074234\n",
      "Train loss at epoch (19):  0.18601477\n",
      "Test loss at epoch (19):  0.14859188\n",
      "Train loss at epoch (20):  0.18360983\n",
      "Test loss at epoch (20):  0.14689559\n",
      "Train loss at epoch (21):  0.1819728\n",
      "Test loss at epoch (21):  0.14492688\n",
      "Train loss at epoch (22):  0.17967771\n",
      "Test loss at epoch (22):  0.14345853\n",
      "Train loss at epoch (23):  0.17759809\n",
      "Test loss at epoch (23):  0.14180678\n",
      "Train loss at epoch (24):  0.17576765\n",
      "Test loss at epoch (24):  0.14047328\n",
      "Train loss at epoch (25):  0.17398365\n",
      "Test loss at epoch (25):  0.13940573\n",
      "Train loss at epoch (26):  0.17235745\n",
      "Test loss at epoch (26):  0.13784644\n",
      "Train loss at epoch (27):  0.17005369\n",
      "Test loss at epoch (27):  0.13692021\n",
      "Train loss at epoch (28):  0.1692831\n",
      "Test loss at epoch (28):  0.13590986\n",
      "Train loss at epoch (29):  0.16876571\n",
      "Test loss at epoch (29):  0.13489664\n",
      "Train loss at epoch (30):  0.16787115\n",
      "Test loss at epoch (30):  0.1341945\n",
      "Train loss at epoch (31):  0.16591129\n",
      "Test loss at epoch (31):  0.13289744\n",
      "Train loss at epoch (32):  0.16377068\n",
      "Test loss at epoch (32):  0.13206807\n",
      "Train loss at epoch (33):  0.16323158\n",
      "Test loss at epoch (33):  0.13122642\n",
      "Train loss at epoch (34):  0.1622192\n",
      "Test loss at epoch (34):  0.13044374\n",
      "Train loss at epoch (35):  0.16007462\n",
      "Test loss at epoch (35):  0.1295184\n",
      "Train loss at epoch (36):  0.16050969\n",
      "Test loss at epoch (36):  0.1285954\n",
      "Train loss at epoch (37):  0.15836275\n",
      "Test loss at epoch (37):  0.1277765\n",
      "Train loss at epoch (38):  0.15718663\n",
      "Test loss at epoch (38):  0.1270585\n",
      "Train loss at epoch (39):  0.15615812\n",
      "Test loss at epoch (39):  0.12623084\n",
      "Train loss at epoch (40):  0.15472543\n",
      "Test loss at epoch (40):  0.1255721\n",
      "Train loss at epoch (41):  0.15493298\n",
      "Test loss at epoch (41):  0.12491439\n",
      "Train loss at epoch (42):  0.15365946\n",
      "Test loss at epoch (42):  0.12434127\n",
      "Train loss at epoch (43):  0.15236439\n",
      "Test loss at epoch (43):  0.12359929\n",
      "Train loss at epoch (44):  0.15207438\n",
      "Test loss at epoch (44):  0.122984864\n",
      "Train loss at epoch (45):  0.1508236\n",
      "Test loss at epoch (45):  0.12220856\n",
      "Train loss at epoch (46):  0.14985137\n",
      "Test loss at epoch (46):  0.12139559\n",
      "Train loss at epoch (47):  0.14854555\n",
      "Test loss at epoch (47):  0.120783955\n",
      "Train loss at epoch (48):  0.14887112\n",
      "Test loss at epoch (48):  0.11999582\n",
      "Train loss at epoch (49):  0.1472041\n",
      "Test loss at epoch (49):  0.11933515\n",
      "Train loss at epoch (50):  0.1468516\n",
      "Test loss at epoch (50):  0.118600644\n",
      "Train loss at epoch (51):  0.1460053\n",
      "Test loss at epoch (51):  0.11804238\n",
      "Train loss at epoch (52):  0.14533281\n",
      "Test loss at epoch (52):  0.11737567\n",
      "Train loss at epoch (53):  0.1447731\n",
      "Test loss at epoch (53):  0.11687774\n",
      "Train loss at epoch (54):  0.14357305\n",
      "Test loss at epoch (54):  0.11622495\n",
      "Train loss at epoch (55):  0.1429179\n",
      "Test loss at epoch (55):  0.11554393\n",
      "Train loss at epoch (56):  0.14222814\n",
      "Test loss at epoch (56):  0.11467451\n",
      "Train loss at epoch (57):  0.14127018\n",
      "Test loss at epoch (57):  0.11411176\n",
      "Train loss at epoch (58):  0.14087088\n",
      "Test loss at epoch (58):  0.11342848\n",
      "Train loss at epoch (59):  0.13997489\n",
      "Test loss at epoch (59):  0.112850666\n",
      "Train loss at epoch (60):  0.13927603\n",
      "Test loss at epoch (60):  0.11230578\n",
      "Train loss at epoch (61):  0.1383103\n",
      "Test loss at epoch (61):  0.11152689\n",
      "Train loss at epoch (62):  0.13819577\n",
      "Test loss at epoch (62):  0.11089817\n",
      "Train loss at epoch (63):  0.13750586\n",
      "Test loss at epoch (63):  0.11047795\n",
      "Train loss at epoch (64):  0.13693714\n",
      "Test loss at epoch (64):  0.10990946\n",
      "Train loss at epoch (65):  0.13511638\n",
      "Test loss at epoch (65):  0.1091183\n",
      "Train loss at epoch (66):  0.13575204\n",
      "Test loss at epoch (66):  0.10861685\n",
      "Train loss at epoch (67):  0.13537443\n",
      "Test loss at epoch (67):  0.10802792\n",
      "Train loss at epoch (68):  0.13371201\n",
      "Test loss at epoch (68):  0.1073893\n",
      "Train loss at epoch (69):  0.13381043\n",
      "Test loss at epoch (69):  0.10676303\n",
      "Train loss at epoch (70):  0.13346893\n",
      "Test loss at epoch (70):  0.10623664\n",
      "Train loss at epoch (71):  0.13259563\n",
      "Test loss at epoch (71):  0.10546424\n",
      "Train loss at epoch (72):  0.13178831\n",
      "Test loss at epoch (72):  0.10494546\n",
      "Train loss at epoch (73):  0.13076656\n",
      "Test loss at epoch (73):  0.10446227\n",
      "Train loss at epoch (74):  0.13065493\n",
      "Test loss at epoch (74):  0.10384991\n",
      "Train loss at epoch (75):  0.12978733\n",
      "Test loss at epoch (75):  0.10327659\n",
      "Train loss at epoch (76):  0.12903738\n",
      "Test loss at epoch (76):  0.10264784\n",
      "Train loss at epoch (77):  0.128652\n",
      "Test loss at epoch (77):  0.102309085\n",
      "Train loss at epoch (78):  0.12819062\n",
      "Test loss at epoch (78):  0.1014848\n",
      "Train loss at epoch (79):  0.12739047\n",
      "Test loss at epoch (79):  0.1008909\n",
      "Train loss at epoch (80):  0.12767762\n",
      "Test loss at epoch (80):  0.100382134\n",
      "Train loss at epoch (81):  0.12715861\n",
      "Test loss at epoch (81):  0.09963581\n",
      "Train loss at epoch (82):  0.12587048\n",
      "Test loss at epoch (82):  0.099107884\n",
      "Train loss at epoch (83):  0.12542616\n",
      "Test loss at epoch (83):  0.09863342\n",
      "Train loss at epoch (84):  0.12459263\n",
      "Test loss at epoch (84):  0.098055735\n",
      "Train loss at epoch (85):  0.124242194\n",
      "Test loss at epoch (85):  0.097514\n",
      "Train loss at epoch (86):  0.12400669\n",
      "Test loss at epoch (86):  0.0970363\n",
      "Train loss at epoch (87):  0.12364632\n",
      "Test loss at epoch (87):  0.096564345\n",
      "Train loss at epoch (88):  0.12283278\n",
      "Test loss at epoch (88):  0.09584665\n",
      "Train loss at epoch (89):  0.12293378\n",
      "Test loss at epoch (89):  0.09523334\n",
      "Train loss at epoch (90):  0.121625215\n",
      "Test loss at epoch (90):  0.09477171\n",
      "Train loss at epoch (91):  0.121793084\n",
      "Test loss at epoch (91):  0.094147444\n",
      "Train loss at epoch (92):  0.121658616\n",
      "Test loss at epoch (92):  0.09373865\n",
      "Train loss at epoch (93):  0.12069596\n",
      "Test loss at epoch (93):  0.093261324\n",
      "Train loss at epoch (94):  0.11993626\n",
      "Test loss at epoch (94):  0.09253113\n",
      "Train loss at epoch (95):  0.11966597\n",
      "Test loss at epoch (95):  0.09222857\n",
      "Train loss at epoch (96):  0.119274214\n",
      "Test loss at epoch (96):  0.091668114\n",
      "Train loss at epoch (97):  0.11927639\n",
      "Test loss at epoch (97):  0.091118686\n",
      "Train loss at epoch (98):  0.11844649\n",
      "Test loss at epoch (98):  0.09069437\n",
      "Train loss at epoch (99):  0.11801536\n",
      "Test loss at epoch (99):  0.090243556\n",
      "Train loss at epoch (100):  0.11795723\n",
      "Test loss at epoch (100):  0.089676626\n",
      "Train loss at epoch (101):  0.117105685\n",
      "Test loss at epoch (101):  0.08912038\n",
      "Train loss at epoch (102):  0.11659806\n",
      "Test loss at epoch (102):  0.08872333\n",
      "Train loss at epoch (103):  0.11656569\n",
      "Test loss at epoch (103):  0.08816698\n",
      "Train loss at epoch (104):  0.11627922\n",
      "Test loss at epoch (104):  0.087638915\n",
      "Train loss at epoch (105):  0.11529014\n",
      "Test loss at epoch (105):  0.08732033\n",
      "Train loss at epoch (106):  0.11524085\n",
      "Test loss at epoch (106):  0.086809896\n",
      "Train loss at epoch (107):  0.11486426\n",
      "Test loss at epoch (107):  0.086339585\n",
      "Train loss at epoch (108):  0.11440301\n",
      "Test loss at epoch (108):  0.08595001\n",
      "Train loss at epoch (109):  0.113765806\n",
      "Test loss at epoch (109):  0.08537028\n",
      "Train loss at epoch (110):  0.11329614\n",
      "Test loss at epoch (110):  0.08485307\n",
      "Train loss at epoch (111):  0.113098055\n",
      "Test loss at epoch (111):  0.08451646\n",
      "Train loss at epoch (112):  0.11296576\n",
      "Test loss at epoch (112):  0.083960734\n",
      "Train loss at epoch (113):  0.11221303\n",
      "Test loss at epoch (113):  0.08347624\n",
      "Train loss at epoch (114):  0.11192007\n",
      "Test loss at epoch (114):  0.08308574\n",
      "Train loss at epoch (115):  0.111450605\n",
      "Test loss at epoch (115):  0.08267965\n",
      "Train loss at epoch (116):  0.110861294\n",
      "Test loss at epoch (116):  0.08212856\n",
      "Train loss at epoch (117):  0.110803016\n",
      "Test loss at epoch (117):  0.08173921\n",
      "Train loss at epoch (118):  0.11059551\n",
      "Test loss at epoch (118):  0.08135582\n",
      "Train loss at epoch (119):  0.11005486\n",
      "Test loss at epoch (119):  0.08094245\n",
      "Train loss at epoch (120):  0.11000515\n",
      "Test loss at epoch (120):  0.080561705\n",
      "Train loss at epoch (121):  0.109228745\n",
      "Test loss at epoch (121):  0.07996134\n",
      "Train loss at epoch (122):  0.10874718\n",
      "Test loss at epoch (122):  0.07957568\n",
      "Train loss at epoch (123):  0.108534954\n",
      "Test loss at epoch (123):  0.07939643\n",
      "Train loss at epoch (124):  0.108893864\n",
      "Test loss at epoch (124):  0.07884179\n",
      "Train loss at epoch (125):  0.10840342\n",
      "Test loss at epoch (125):  0.078287624\n",
      "Train loss at epoch (126):  0.108124666\n",
      "Test loss at epoch (126):  0.0780291\n",
      "Train loss at epoch (127):  0.107315026\n",
      "Test loss at epoch (127):  0.07784629\n",
      "Train loss at epoch (128):  0.10700307\n",
      "Test loss at epoch (128):  0.077254385\n",
      "Train loss at epoch (129):  0.106922865\n",
      "Test loss at epoch (129):  0.076866664\n",
      "Train loss at epoch (130):  0.106762126\n",
      "Test loss at epoch (130):  0.07643883\n",
      "Train loss at epoch (131):  0.10594786\n",
      "Test loss at epoch (131):  0.076034844\n",
      "Train loss at epoch (132):  0.10609759\n",
      "Test loss at epoch (132):  0.07573992\n",
      "Train loss at epoch (133):  0.1059319\n",
      "Test loss at epoch (133):  0.07531529\n",
      "Train loss at epoch (134):  0.1055285\n",
      "Test loss at epoch (134):  0.075129926\n",
      "Train loss at epoch (135):  0.10513227\n",
      "Test loss at epoch (135):  0.074756466\n",
      "Train loss at epoch (136):  0.10504291\n",
      "Test loss at epoch (136):  0.07424124\n",
      "Train loss at epoch (137):  0.10424401\n",
      "Test loss at epoch (137):  0.07408407\n",
      "Train loss at epoch (138):  0.1044982\n",
      "Test loss at epoch (138):  0.07377077\n",
      "Train loss at epoch (139):  0.10397981\n",
      "Test loss at epoch (139):  0.0732674\n",
      "Train loss at epoch (140):  0.103472516\n",
      "Test loss at epoch (140):  0.07299937\n",
      "Train loss at epoch (141):  0.10313936\n",
      "Test loss at epoch (141):  0.07244817\n",
      "Train loss at epoch (142):  0.10320861\n",
      "Test loss at epoch (142):  0.07244216\n",
      "Train loss at epoch (143):  0.103454135\n",
      "Test loss at epoch (143):  0.071899876\n",
      "Train loss at epoch (144):  0.10279909\n",
      "Test loss at epoch (144):  0.071470626\n",
      "Train loss at epoch (145):  0.102406695\n",
      "Test loss at epoch (145):  0.0713748\n",
      "Train loss at epoch (146):  0.1022842\n",
      "Test loss at epoch (146):  0.07091474\n",
      "Train loss at epoch (147):  0.102300666\n",
      "Test loss at epoch (147):  0.07055643\n",
      "Train loss at epoch (148):  0.10191547\n",
      "Test loss at epoch (148):  0.0701783\n",
      "Train loss at epoch (149):  0.10105998\n",
      "Test loss at epoch (149):  0.06993824\n",
      "Train loss at epoch (150):  0.10127932\n",
      "Test loss at epoch (150):  0.06979734\n",
      "Train loss at epoch (151):  0.10087128\n",
      "Test loss at epoch (151):  0.06936711\n",
      "Train loss at epoch (152):  0.100733936\n",
      "Test loss at epoch (152):  0.06921083\n",
      "Train loss at epoch (153):  0.10051617\n",
      "Test loss at epoch (153):  0.06886619\n",
      "Train loss at epoch (154):  0.09993231\n",
      "Test loss at epoch (154):  0.06869168\n",
      "Train loss at epoch (155):  0.099818684\n",
      "Test loss at epoch (155):  0.068342075\n",
      "Train loss at epoch (156):  0.09959612\n",
      "Test loss at epoch (156):  0.06801559\n",
      "Train loss at epoch (157):  0.099663734\n",
      "Test loss at epoch (157):  0.06777523\n",
      "Train loss at epoch (158):  0.09919759\n",
      "Test loss at epoch (158):  0.06742896\n",
      "Train loss at epoch (159):  0.09891926\n",
      "Test loss at epoch (159):  0.06713501\n",
      "Train loss at epoch (160):  0.098837145\n",
      "Test loss at epoch (160):  0.06696604\n",
      "Train loss at epoch (161):  0.09859494\n",
      "Test loss at epoch (161):  0.06685491\n",
      "Train loss at epoch (162):  0.098936535\n",
      "Test loss at epoch (162):  0.06658863\n",
      "Train loss at epoch (163):  0.09839982\n",
      "Test loss at epoch (163):  0.06598581\n",
      "Train loss at epoch (164):  0.098093845\n",
      "Test loss at epoch (164):  0.06587173\n",
      "Train loss at epoch (165):  0.09776721\n",
      "Test loss at epoch (165):  0.06566439\n",
      "Train loss at epoch (166):  0.09723935\n",
      "Test loss at epoch (166):  0.065536\n",
      "Train loss at epoch (167):  0.0972116\n",
      "Test loss at epoch (167):  0.06552893\n",
      "Train loss at epoch (168):  0.096712746\n",
      "Test loss at epoch (168):  0.06489766\n",
      "Train loss at epoch (169):  0.09685497\n",
      "Test loss at epoch (169):  0.064508855\n",
      "Train loss at epoch (170):  0.09731359\n",
      "Test loss at epoch (170):  0.06430104\n",
      "Train loss at epoch (171):  0.0966849\n",
      "Test loss at epoch (171):  0.06417654\n",
      "Train loss at epoch (172):  0.09638729\n",
      "Test loss at epoch (172):  0.06388169\n",
      "Train loss at epoch (173):  0.096566536\n",
      "Test loss at epoch (173):  0.06390581\n",
      "Train loss at epoch (174):  0.095867395\n",
      "Test loss at epoch (174):  0.06371296\n",
      "Train loss at epoch (175):  0.095615365\n",
      "Test loss at epoch (175):  0.063288294\n",
      "Train loss at epoch (176):  0.09579777\n",
      "Test loss at epoch (176):  0.06331315\n",
      "Train loss at epoch (177):  0.09553966\n",
      "Test loss at epoch (177):  0.06306754\n",
      "Train loss at epoch (178):  0.095151514\n",
      "Test loss at epoch (178):  0.06292431\n",
      "Train loss at epoch (179):  0.09511577\n",
      "Test loss at epoch (179):  0.06256189\n",
      "Train loss at epoch (180):  0.09500538\n",
      "Test loss at epoch (180):  0.06227772\n",
      "Train loss at epoch (181):  0.09423594\n",
      "Test loss at epoch (181):  0.06235153\n",
      "Train loss at epoch (182):  0.0942779\n",
      "Test loss at epoch (182):  0.06186554\n",
      "Train loss at epoch (183):  0.09439442\n",
      "Test loss at epoch (183):  0.061732136\n",
      "Train loss at epoch (184):  0.09457039\n",
      "Test loss at epoch (184):  0.061619006\n",
      "Train loss at epoch (185):  0.09395257\n",
      "Test loss at epoch (185):  0.061530527\n",
      "Train loss at epoch (186):  0.09428612\n",
      "Test loss at epoch (186):  0.061151713\n",
      "Train loss at epoch (187):  0.09342488\n",
      "Test loss at epoch (187):  0.060906492\n",
      "Train loss at epoch (188):  0.09423852\n",
      "Test loss at epoch (188):  0.060640253\n",
      "Train loss at epoch (189):  0.09380802\n",
      "Test loss at epoch (189):  0.060536932\n",
      "Train loss at epoch (190):  0.09345022\n",
      "Test loss at epoch (190):  0.06042142\n",
      "Train loss at epoch (191):  0.09298797\n",
      "Test loss at epoch (191):  0.06013234\n",
      "Train loss at epoch (192):  0.0926872\n",
      "Test loss at epoch (192):  0.05995217\n",
      "Train loss at epoch (193):  0.09274068\n",
      "Test loss at epoch (193):  0.05964702\n",
      "Train loss at epoch (194):  0.092521355\n",
      "Test loss at epoch (194):  0.059487056\n",
      "Train loss at epoch (195):  0.09196596\n",
      "Test loss at epoch (195):  0.059449982\n",
      "Train loss at epoch (196):  0.09215818\n",
      "Test loss at epoch (196):  0.059326697\n",
      "Train loss at epoch (197):  0.09196571\n",
      "Test loss at epoch (197):  0.059189927\n",
      "Train loss at epoch (198):  0.092022255\n",
      "Test loss at epoch (198):  0.058899466\n",
      "Train loss at epoch (199):  0.09199742\n",
      "Test loss at epoch (199):  0.05867928\n",
      "Train loss at epoch (200):  0.09183478\n",
      "Test loss at epoch (200):  0.05910396\n",
      "Train loss at epoch (201):  0.091373354\n",
      "Test loss at epoch (201):  0.058421668\n",
      "Train loss at epoch (202):  0.09183698\n",
      "Test loss at epoch (202):  0.05833524\n",
      "Train loss at epoch (203):  0.09168146\n",
      "Test loss at epoch (203):  0.058025636\n",
      "Train loss at epoch (204):  0.090575375\n",
      "Test loss at epoch (204):  0.05817375\n",
      "Train loss at epoch (205):  0.09094872\n",
      "Test loss at epoch (205):  0.058161214\n",
      "Train loss at epoch (206):  0.0906368\n",
      "Test loss at epoch (206):  0.05794583\n",
      "Train loss at epoch (207):  0.09068012\n",
      "Test loss at epoch (207):  0.05743078\n",
      "Train loss at epoch (208):  0.09018754\n",
      "Test loss at epoch (208):  0.057390396\n",
      "Train loss at epoch (209):  0.09068465\n",
      "Test loss at epoch (209):  0.05719041\n",
      "Train loss at epoch (210):  0.09037061\n",
      "Test loss at epoch (210):  0.057071008\n",
      "Train loss at epoch (211):  0.08996412\n",
      "Test loss at epoch (211):  0.05690779\n",
      "Train loss at epoch (212):  0.09027959\n",
      "Test loss at epoch (212):  0.05684759\n",
      "Train loss at epoch (213):  0.08998949\n",
      "Test loss at epoch (213):  0.056754425\n",
      "Train loss at epoch (214):  0.08965607\n",
      "Test loss at epoch (214):  0.056667052\n",
      "Train loss at epoch (215):  0.089926146\n",
      "Test loss at epoch (215):  0.056774404\n",
      "Train loss at epoch (216):  0.0896344\n",
      "Test loss at epoch (216):  0.0562665\n",
      "Train loss at epoch (217):  0.08938468\n",
      "Test loss at epoch (217):  0.056131456\n",
      "Train loss at epoch (218):  0.08905942\n",
      "Test loss at epoch (218):  0.055936795\n",
      "Train loss at epoch (219):  0.08962544\n",
      "Test loss at epoch (219):  0.05608778\n",
      "Train loss at epoch (220):  0.089079306\n",
      "Test loss at epoch (220):  0.055809926\n",
      "Train loss at epoch (221):  0.08867007\n",
      "Test loss at epoch (221):  0.05559696\n",
      "Train loss at epoch (222):  0.08885368\n",
      "Test loss at epoch (222):  0.055302788\n",
      "Train loss at epoch (223):  0.08840112\n",
      "Test loss at epoch (223):  0.055609692\n",
      "Train loss at epoch (224):  0.08881479\n",
      "Test loss at epoch (224):  0.055164374\n",
      "Train loss at epoch (225):  0.088521376\n",
      "Test loss at epoch (225):  0.054986503\n",
      "Train loss at epoch (226):  0.08848014\n",
      "Test loss at epoch (226):  0.05493764\n",
      "Train loss at epoch (227):  0.08811438\n",
      "Test loss at epoch (227):  0.054828037\n",
      "Train loss at epoch (228):  0.08767334\n",
      "Test loss at epoch (228):  0.05478982\n",
      "Train loss at epoch (229):  0.0878697\n",
      "Test loss at epoch (229):  0.05478977\n",
      "Train loss at epoch (230):  0.08827386\n",
      "Test loss at epoch (230):  0.054737914\n",
      "Train loss at epoch (231):  0.08750048\n",
      "Test loss at epoch (231):  0.05460451\n",
      "Train loss at epoch (232):  0.087511905\n",
      "Test loss at epoch (232):  0.054372557\n",
      "Train loss at epoch (233):  0.087475605\n",
      "Test loss at epoch (233):  0.05420939\n",
      "Train loss at epoch (234):  0.08754828\n",
      "Test loss at epoch (234):  0.054171782\n",
      "Train loss at epoch (235):  0.087045014\n",
      "Test loss at epoch (235):  0.0539944\n",
      "Train loss at epoch (236):  0.08697984\n",
      "Test loss at epoch (236):  0.053924304\n",
      "Train loss at epoch (237):  0.0869212\n",
      "Test loss at epoch (237):  0.05407005\n",
      "Train loss at epoch (238):  0.086559206\n",
      "Test loss at epoch (238):  0.053812813\n",
      "Train loss at epoch (239):  0.08671576\n",
      "Test loss at epoch (239):  0.05363477\n",
      "Train loss at epoch (240):  0.08672006\n",
      "Test loss at epoch (240):  0.053516757\n",
      "Train loss at epoch (241):  0.0864913\n",
      "Test loss at epoch (241):  0.05350676\n",
      "Train loss at epoch (242):  0.08617991\n",
      "Test loss at epoch (242):  0.053364277\n",
      "Train loss at epoch (243):  0.086590044\n",
      "Test loss at epoch (243):  0.05302334\n",
      "Train loss at epoch (244):  0.086572126\n",
      "Test loss at epoch (244):  0.05318486\n",
      "Train loss at epoch (245):  0.08673589\n",
      "Test loss at epoch (245):  0.05312454\n",
      "Train loss at epoch (246):  0.086053796\n",
      "Test loss at epoch (246):  0.052835036\n",
      "Train loss at epoch (247):  0.08628949\n",
      "Test loss at epoch (247):  0.05264562\n",
      "Train loss at epoch (248):  0.08569892\n",
      "Test loss at epoch (248):  0.05271529\n",
      "Train loss at epoch (249):  0.08550235\n",
      "Test loss at epoch (249):  0.052748177\n",
      "Train loss at epoch (250):  0.085468635\n",
      "Test loss at epoch (250):  0.052463487\n",
      "Train loss at epoch (251):  0.086023584\n",
      "Test loss at epoch (251):  0.052445065\n",
      "Train loss at epoch (252):  0.085472085\n",
      "Test loss at epoch (252):  0.052204803\n",
      "Train loss at epoch (253):  0.08556107\n",
      "Test loss at epoch (253):  0.052047268\n",
      "Train loss at epoch (254):  0.08559761\n",
      "Test loss at epoch (254):  0.05196464\n",
      "Train loss at epoch (255):  0.08523551\n",
      "Test loss at epoch (255):  0.051849112\n",
      "Train loss at epoch (256):  0.08496295\n",
      "Test loss at epoch (256):  0.05183203\n",
      "Train loss at epoch (257):  0.08482124\n",
      "Test loss at epoch (257):  0.051740658\n",
      "Train loss at epoch (258):  0.08502245\n",
      "Test loss at epoch (258):  0.05167619\n",
      "Train loss at epoch (259):  0.085077144\n",
      "Test loss at epoch (259):  0.05160228\n",
      "Train loss at epoch (260):  0.08449921\n",
      "Test loss at epoch (260):  0.05164156\n",
      "Train loss at epoch (261):  0.084781334\n",
      "Test loss at epoch (261):  0.051299687\n",
      "Train loss at epoch (262):  0.08458105\n",
      "Test loss at epoch (262):  0.051285315\n",
      "Train loss at epoch (263):  0.08498428\n",
      "Test loss at epoch (263):  0.051286466\n",
      "Train loss at epoch (264):  0.08416472\n",
      "Test loss at epoch (264):  0.050951708\n",
      "Train loss at epoch (265):  0.084404625\n",
      "Test loss at epoch (265):  0.051140554\n",
      "Train loss at epoch (266):  0.08444172\n",
      "Test loss at epoch (266):  0.05093051\n",
      "Train loss at epoch (267):  0.08414698\n",
      "Test loss at epoch (267):  0.05090477\n",
      "Train loss at epoch (268):  0.08416473\n",
      "Test loss at epoch (268):  0.050829817\n",
      "Train loss at epoch (269):  0.0840625\n",
      "Test loss at epoch (269):  0.050670255\n",
      "Train loss at epoch (270):  0.08389165\n",
      "Test loss at epoch (270):  0.0508068\n",
      "Train loss at epoch (271):  0.083926566\n",
      "Test loss at epoch (271):  0.050691035\n",
      "Train loss at epoch (272):  0.0837805\n",
      "Test loss at epoch (272):  0.050467778\n",
      "Train loss at epoch (273):  0.083585955\n",
      "Test loss at epoch (273):  0.050281685\n",
      "Train loss at epoch (274):  0.08360364\n",
      "Test loss at epoch (274):  0.05036555\n",
      "Train loss at epoch (275):  0.083939604\n",
      "Test loss at epoch (275):  0.05017927\n",
      "Train loss at epoch (276):  0.08329697\n",
      "Test loss at epoch (276):  0.05016338\n",
      "Train loss at epoch (277):  0.08295525\n",
      "Test loss at epoch (277):  0.050109576\n",
      "Train loss at epoch (278):  0.083383396\n",
      "Test loss at epoch (278):  0.050054226\n",
      "Train loss at epoch (279):  0.08294603\n",
      "Test loss at epoch (279):  0.049834304\n",
      "Train loss at epoch (280):  0.08297407\n",
      "Test loss at epoch (280):  0.04995033\n",
      "Train loss at epoch (281):  0.08294194\n",
      "Test loss at epoch (281):  0.04976452\n",
      "Train loss at epoch (282):  0.08295938\n",
      "Test loss at epoch (282):  0.04977085\n",
      "Train loss at epoch (283):  0.08278074\n",
      "Test loss at epoch (283):  0.049568098\n",
      "Train loss at epoch (284):  0.083378956\n",
      "Test loss at epoch (284):  0.049597237\n",
      "Train loss at epoch (285):  0.08279508\n",
      "Test loss at epoch (285):  0.04983117\n",
      "Train loss at epoch (286):  0.08273146\n",
      "Test loss at epoch (286):  0.049393144\n",
      "Train loss at epoch (287):  0.08199298\n",
      "Test loss at epoch (287):  0.04930908\n",
      "Train loss at epoch (288):  0.08210615\n",
      "Test loss at epoch (288):  0.0496592\n",
      "Train loss at epoch (289):  0.08247196\n",
      "Test loss at epoch (289):  0.049170755\n",
      "Train loss at epoch (290):  0.082084045\n",
      "Test loss at epoch (290):  0.049239997\n",
      "Train loss at epoch (291):  0.08199584\n",
      "Test loss at epoch (291):  0.049014773\n",
      "Train loss at epoch (292):  0.08218387\n",
      "Test loss at epoch (292):  0.048849322\n",
      "Train loss at epoch (293):  0.082076214\n",
      "Test loss at epoch (293):  0.04884818\n",
      "Train loss at epoch (294):  0.0821768\n",
      "Test loss at epoch (294):  0.048880387\n",
      "Train loss at epoch (295):  0.08190515\n",
      "Test loss at epoch (295):  0.04883705\n",
      "Train loss at epoch (296):  0.081914045\n",
      "Test loss at epoch (296):  0.048772108\n",
      "Train loss at epoch (297):  0.081912406\n",
      "Test loss at epoch (297):  0.04869911\n",
      "Train loss at epoch (298):  0.08188741\n",
      "Test loss at epoch (298):  0.04865984\n",
      "Train loss at epoch (299):  0.08171035\n",
      "Test loss at epoch (299):  0.048621345\n",
      "Train loss at epoch (300):  0.081367746\n",
      "Test loss at epoch (300):  0.048557084\n",
      "Train loss at epoch (301):  0.0817369\n",
      "Test loss at epoch (301):  0.04857483\n",
      "Train loss at epoch (302):  0.08083262\n",
      "Test loss at epoch (302):  0.04844853\n",
      "Train loss at epoch (303):  0.08136072\n",
      "Test loss at epoch (303):  0.048354544\n",
      "Train loss at epoch (304):  0.08095521\n",
      "Test loss at epoch (304):  0.048435085\n",
      "Train loss at epoch (305):  0.08121376\n",
      "Test loss at epoch (305):  0.048028216\n",
      "Train loss at epoch (306):  0.08126431\n",
      "Test loss at epoch (306):  0.048223667\n",
      "Train loss at epoch (307):  0.080944486\n",
      "Test loss at epoch (307):  0.048212133\n",
      "Train loss at epoch (308):  0.080394775\n",
      "Test loss at epoch (308):  0.048141032\n",
      "Train loss at epoch (309):  0.0806145\n",
      "Test loss at epoch (309):  0.047792297\n",
      "Train loss at epoch (310):  0.08066478\n",
      "Test loss at epoch (310):  0.047978085\n",
      "Train loss at epoch (311):  0.081027105\n",
      "Test loss at epoch (311):  0.047822755\n",
      "Train loss at epoch (312):  0.080564916\n",
      "Test loss at epoch (312):  0.047805063\n",
      "Train loss at epoch (313):  0.080350704\n",
      "Test loss at epoch (313):  0.047662877\n",
      "Train loss at epoch (314):  0.08042764\n",
      "Test loss at epoch (314):  0.04760686\n",
      "Train loss at epoch (315):  0.07997067\n",
      "Test loss at epoch (315):  0.04756706\n",
      "Train loss at epoch (316):  0.08031218\n",
      "Test loss at epoch (316):  0.047632217\n",
      "Train loss at epoch (317):  0.0803479\n",
      "Test loss at epoch (317):  0.047339547\n",
      "Train loss at epoch (318):  0.07978849\n",
      "Test loss at epoch (318):  0.047448527\n",
      "Train loss at epoch (319):  0.080118544\n",
      "Test loss at epoch (319):  0.04727689\n",
      "Train loss at epoch (320):  0.080247425\n",
      "Test loss at epoch (320):  0.047134247\n",
      "Train loss at epoch (321):  0.08004311\n",
      "Test loss at epoch (321):  0.04737358\n",
      "Train loss at epoch (322):  0.07993832\n",
      "Test loss at epoch (322):  0.047186226\n",
      "Train loss at epoch (323):  0.08009525\n",
      "Test loss at epoch (323):  0.047090728\n",
      "Train loss at epoch (324):  0.080171995\n",
      "Test loss at epoch (324):  0.047041647\n",
      "Train loss at epoch (325):  0.07998813\n",
      "Test loss at epoch (325):  0.04702103\n",
      "Train loss at epoch (326):  0.07974891\n",
      "Test loss at epoch (326):  0.046950337\n",
      "Train loss at epoch (327):  0.07905832\n",
      "Test loss at epoch (327):  0.046971202\n",
      "Train loss at epoch (328):  0.07948861\n",
      "Test loss at epoch (328):  0.046875883\n",
      "Train loss at epoch (329):  0.079689085\n",
      "Test loss at epoch (329):  0.046733283\n",
      "Train loss at epoch (330):  0.07931956\n",
      "Test loss at epoch (330):  0.046628054\n",
      "Train loss at epoch (331):  0.079339854\n",
      "Test loss at epoch (331):  0.04671571\n",
      "Train loss at epoch (332):  0.07954984\n",
      "Test loss at epoch (332):  0.04679178\n",
      "Train loss at epoch (333):  0.07972607\n",
      "Test loss at epoch (333):  0.046617255\n",
      "Train loss at epoch (334):  0.079275474\n",
      "Test loss at epoch (334):  0.046703063\n",
      "Train loss at epoch (335):  0.07911471\n",
      "Test loss at epoch (335):  0.046467386\n",
      "Train loss at epoch (336):  0.07957578\n",
      "Test loss at epoch (336):  0.046329614\n",
      "Train loss at epoch (337):  0.0791735\n",
      "Test loss at epoch (337):  0.046577204\n",
      "Train loss at epoch (338):  0.079352215\n",
      "Test loss at epoch (338):  0.046260353\n",
      "Train loss at epoch (339):  0.07883178\n",
      "Test loss at epoch (339):  0.046346094\n",
      "Train loss at epoch (340):  0.07903444\n",
      "Test loss at epoch (340):  0.04629812\n",
      "Train loss at epoch (341):  0.07867823\n",
      "Test loss at epoch (341):  0.046130072\n",
      "Train loss at epoch (342):  0.079048514\n",
      "Test loss at epoch (342):  0.0460745\n",
      "Train loss at epoch (343):  0.078572854\n",
      "Test loss at epoch (343):  0.046098296\n",
      "Train loss at epoch (344):  0.07859449\n",
      "Test loss at epoch (344):  0.046073303\n",
      "Train loss at epoch (345):  0.07853944\n",
      "Test loss at epoch (345):  0.045861587\n",
      "Train loss at epoch (346):  0.07807362\n",
      "Test loss at epoch (346):  0.045963746\n",
      "Train loss at epoch (347):  0.07822931\n",
      "Test loss at epoch (347):  0.045917887\n",
      "Train loss at epoch (348):  0.078683704\n",
      "Test loss at epoch (348):  0.045765527\n",
      "Train loss at epoch (349):  0.07829359\n",
      "Test loss at epoch (349):  0.045829177\n",
      "Train loss at epoch (350):  0.07808805\n",
      "Test loss at epoch (350):  0.04575535\n",
      "Train loss at epoch (351):  0.078220814\n",
      "Test loss at epoch (351):  0.045608666\n",
      "Train loss at epoch (352):  0.07874221\n",
      "Test loss at epoch (352):  0.045712877\n",
      "Train loss at epoch (353):  0.078066155\n",
      "Test loss at epoch (353):  0.045424283\n",
      "Train loss at epoch (354):  0.07767009\n",
      "Test loss at epoch (354):  0.045586597\n",
      "Train loss at epoch (355):  0.07804723\n",
      "Test loss at epoch (355):  0.045388047\n",
      "Train loss at epoch (356):  0.07762422\n",
      "Test loss at epoch (356):  0.045261186\n",
      "Train loss at epoch (357):  0.07781343\n",
      "Test loss at epoch (357):  0.045304466\n",
      "Train loss at epoch (358):  0.07812752\n",
      "Test loss at epoch (358):  0.04537781\n",
      "Train loss at epoch (359):  0.07755425\n",
      "Test loss at epoch (359):  0.04529379\n",
      "Train loss at epoch (360):  0.07706904\n",
      "Test loss at epoch (360):  0.045203596\n",
      "Train loss at epoch (361):  0.07753753\n",
      "Test loss at epoch (361):  0.045093704\n",
      "Train loss at epoch (362):  0.0775047\n",
      "Test loss at epoch (362):  0.045067754\n",
      "Train loss at epoch (363):  0.07766484\n",
      "Test loss at epoch (363):  0.04505055\n",
      "Train loss at epoch (364):  0.07727326\n",
      "Test loss at epoch (364):  0.045076936\n",
      "Train loss at epoch (365):  0.077358454\n",
      "Test loss at epoch (365):  0.045093887\n",
      "Train loss at epoch (366):  0.077582814\n",
      "Test loss at epoch (366):  0.044909608\n",
      "Train loss at epoch (367):  0.07757396\n",
      "Test loss at epoch (367):  0.0448161\n",
      "Train loss at epoch (368):  0.07717891\n",
      "Test loss at epoch (368):  0.044867933\n",
      "Train loss at epoch (369):  0.077257834\n",
      "Test loss at epoch (369):  0.044754006\n",
      "Train loss at epoch (370):  0.07708895\n",
      "Test loss at epoch (370):  0.044794716\n",
      "Train loss at epoch (371):  0.07699044\n",
      "Test loss at epoch (371):  0.044772137\n",
      "Train loss at epoch (372):  0.076928474\n",
      "Test loss at epoch (372):  0.044657975\n",
      "Train loss at epoch (373):  0.07667811\n",
      "Test loss at epoch (373):  0.044684898\n",
      "Train loss at epoch (374):  0.07707521\n",
      "Test loss at epoch (374):  0.044579078\n",
      "Train loss at epoch (375):  0.076793\n",
      "Test loss at epoch (375):  0.044531446\n",
      "Train loss at epoch (376):  0.07656385\n",
      "Test loss at epoch (376):  0.044397462\n",
      "Train loss at epoch (377):  0.07711408\n",
      "Test loss at epoch (377):  0.04435861\n",
      "Train loss at epoch (378):  0.07655352\n",
      "Test loss at epoch (378):  0.04434992\n",
      "Train loss at epoch (379):  0.07731936\n",
      "Test loss at epoch (379):  0.044199873\n",
      "Train loss at epoch (380):  0.076587245\n",
      "Test loss at epoch (380):  0.044236597\n",
      "Train loss at epoch (381):  0.076761626\n",
      "Test loss at epoch (381):  0.04440336\n",
      "Train loss at epoch (382):  0.07664502\n",
      "Test loss at epoch (382):  0.044249047\n",
      "Train loss at epoch (383):  0.076704554\n",
      "Test loss at epoch (383):  0.044051196\n",
      "Train loss at epoch (384):  0.076849595\n",
      "Test loss at epoch (384):  0.04412763\n",
      "Train loss at epoch (385):  0.07615029\n",
      "Test loss at epoch (385):  0.044046618\n",
      "Train loss at epoch (386):  0.076332174\n",
      "Test loss at epoch (386):  0.044165846\n",
      "Train loss at epoch (387):  0.07653251\n",
      "Test loss at epoch (387):  0.04437849\n",
      "Train loss at epoch (388):  0.07642326\n",
      "Test loss at epoch (388):  0.043976404\n",
      "Train loss at epoch (389):  0.07574845\n",
      "Test loss at epoch (389):  0.04393653\n",
      "Train loss at epoch (390):  0.07603171\n",
      "Test loss at epoch (390):  0.04391065\n",
      "Train loss at epoch (391):  0.07629394\n",
      "Test loss at epoch (391):  0.04379021\n",
      "Train loss at epoch (392):  0.07583577\n",
      "Test loss at epoch (392):  0.043711636\n",
      "Train loss at epoch (393):  0.075882025\n",
      "Test loss at epoch (393):  0.043657497\n",
      "Train loss at epoch (394):  0.07607206\n",
      "Test loss at epoch (394):  0.04377205\n",
      "Train loss at epoch (395):  0.07580855\n",
      "Test loss at epoch (395):  0.04365972\n",
      "Train loss at epoch (396):  0.07611291\n",
      "Test loss at epoch (396):  0.043547146\n",
      "Train loss at epoch (397):  0.07605033\n",
      "Test loss at epoch (397):  0.043578863\n",
      "Train loss at epoch (398):  0.076284386\n",
      "Test loss at epoch (398):  0.043469857\n",
      "Train loss at epoch (399):  0.07573339\n",
      "Test loss at epoch (399):  0.0434704\n",
      "Train loss at epoch (400):  0.075282566\n",
      "Test loss at epoch (400):  0.043394517\n",
      "Train loss at epoch (401):  0.07583089\n",
      "Test loss at epoch (401):  0.043423485\n",
      "Train loss at epoch (402):  0.07530536\n",
      "Test loss at epoch (402):  0.043278\n",
      "Train loss at epoch (403):  0.075249895\n",
      "Test loss at epoch (403):  0.043282993\n",
      "Train loss at epoch (404):  0.07527527\n",
      "Test loss at epoch (404):  0.043396045\n",
      "Train loss at epoch (405):  0.07521006\n",
      "Test loss at epoch (405):  0.04324108\n",
      "Train loss at epoch (406):  0.0749667\n",
      "Test loss at epoch (406):  0.043272626\n",
      "Train loss at epoch (407):  0.07524568\n",
      "Test loss at epoch (407):  0.04316901\n",
      "Train loss at epoch (408):  0.07487753\n",
      "Test loss at epoch (408):  0.043146722\n",
      "Train loss at epoch (409):  0.07538338\n",
      "Test loss at epoch (409):  0.042975437\n",
      "Train loss at epoch (410):  0.07540896\n",
      "Test loss at epoch (410):  0.043028187\n",
      "Train loss at epoch (411):  0.07497917\n",
      "Test loss at epoch (411):  0.04313654\n",
      "Train loss at epoch (412):  0.07525348\n",
      "Test loss at epoch (412):  0.04313729\n",
      "Train loss at epoch (413):  0.07528901\n",
      "Test loss at epoch (413):  0.04301909\n",
      "Train loss at epoch (414):  0.07475228\n",
      "Test loss at epoch (414):  0.042844143\n",
      "Train loss at epoch (415):  0.074830286\n",
      "Test loss at epoch (415):  0.043004606\n",
      "Train loss at epoch (416):  0.07500588\n",
      "Test loss at epoch (416):  0.042755555\n",
      "Train loss at epoch (417):  0.07478748\n",
      "Test loss at epoch (417):  0.042675663\n",
      "Train loss at epoch (418):  0.074661195\n",
      "Test loss at epoch (418):  0.04275875\n",
      "Train loss at epoch (419):  0.074927\n",
      "Test loss at epoch (419):  0.042645592\n",
      "Train loss at epoch (420):  0.07450576\n",
      "Test loss at epoch (420):  0.04265952\n",
      "Train loss at epoch (421):  0.07460137\n",
      "Test loss at epoch (421):  0.0426951\n",
      "Train loss at epoch (422):  0.075033635\n",
      "Test loss at epoch (422):  0.04255528\n",
      "Train loss at epoch (423):  0.07437815\n",
      "Test loss at epoch (423):  0.0424606\n",
      "Train loss at epoch (424):  0.07474769\n",
      "Test loss at epoch (424):  0.0424529\n",
      "Train loss at epoch (425):  0.07491549\n",
      "Test loss at epoch (425):  0.04252257\n",
      "Train loss at epoch (426):  0.074194804\n",
      "Test loss at epoch (426):  0.042435672\n",
      "Train loss at epoch (427):  0.07410493\n",
      "Test loss at epoch (427):  0.042480327\n",
      "Train loss at epoch (428):  0.07466779\n",
      "Test loss at epoch (428):  0.042434696\n",
      "Train loss at epoch (429):  0.074655615\n",
      "Test loss at epoch (429):  0.04248018\n",
      "Train loss at epoch (430):  0.073810816\n",
      "Test loss at epoch (430):  0.04246561\n",
      "Train loss at epoch (431):  0.074105024\n",
      "Test loss at epoch (431):  0.04235812\n",
      "Train loss at epoch (432):  0.07425819\n",
      "Test loss at epoch (432):  0.042381573\n",
      "Train loss at epoch (433):  0.073929034\n",
      "Test loss at epoch (433):  0.04238321\n",
      "Train loss at epoch (434):  0.07429367\n",
      "Test loss at epoch (434):  0.0424483\n",
      "Train loss at epoch (435):  0.07366195\n",
      "Test loss at epoch (435):  0.04221326\n",
      "Train loss at epoch (436):  0.073747136\n",
      "Test loss at epoch (436):  0.042224832\n",
      "Train loss at epoch (437):  0.07379221\n",
      "Test loss at epoch (437):  0.04207176\n",
      "Train loss at epoch (438):  0.07395816\n",
      "Test loss at epoch (438):  0.042309333\n",
      "Train loss at epoch (439):  0.073891714\n",
      "Test loss at epoch (439):  0.042136505\n",
      "Train loss at epoch (440):  0.0742137\n",
      "Test loss at epoch (440):  0.042026646\n",
      "Train loss at epoch (441):  0.073894195\n",
      "Test loss at epoch (441):  0.042048186\n",
      "Train loss at epoch (442):  0.07403174\n",
      "Test loss at epoch (442):  0.042161543\n",
      "Train loss at epoch (443):  0.07387864\n",
      "Test loss at epoch (443):  0.04201429\n",
      "Train loss at epoch (444):  0.07396553\n",
      "Test loss at epoch (444):  0.042095087\n",
      "Train loss at epoch (445):  0.0737476\n",
      "Test loss at epoch (445):  0.041946758\n",
      "Train loss at epoch (446):  0.073688485\n",
      "Test loss at epoch (446):  0.041937817\n",
      "Train loss at epoch (447):  0.07344027\n",
      "Test loss at epoch (447):  0.04187815\n",
      "Train loss at epoch (448):  0.073827274\n",
      "Test loss at epoch (448):  0.041766413\n",
      "Train loss at epoch (449):  0.07350447\n",
      "Test loss at epoch (449):  0.04180585\n",
      "Train loss at epoch (450):  0.0738698\n",
      "Test loss at epoch (450):  0.041617762\n",
      "Train loss at epoch (451):  0.073389195\n",
      "Test loss at epoch (451):  0.041733302\n",
      "Train loss at epoch (452):  0.07342237\n",
      "Test loss at epoch (452):  0.041538697\n",
      "Train loss at epoch (453):  0.073153645\n",
      "Test loss at epoch (453):  0.041572776\n",
      "Train loss at epoch (454):  0.07324759\n",
      "Test loss at epoch (454):  0.04164036\n",
      "Train loss at epoch (455):  0.07342849\n",
      "Test loss at epoch (455):  0.04163309\n",
      "Train loss at epoch (456):  0.07300395\n",
      "Test loss at epoch (456):  0.04156752\n",
      "Train loss at epoch (457):  0.07332971\n",
      "Test loss at epoch (457):  0.04145697\n",
      "Train loss at epoch (458):  0.07348709\n",
      "Test loss at epoch (458):  0.04152416\n",
      "Train loss at epoch (459):  0.0734501\n",
      "Test loss at epoch (459):  0.041584842\n",
      "Train loss at epoch (460):  0.07298526\n",
      "Test loss at epoch (460):  0.041471746\n",
      "Train loss at epoch (461):  0.07285685\n",
      "Test loss at epoch (461):  0.04142679\n",
      "Train loss at epoch (462):  0.07331649\n",
      "Test loss at epoch (462):  0.04146417\n",
      "Train loss at epoch (463):  0.07307163\n",
      "Test loss at epoch (463):  0.041523527\n",
      "Train loss at epoch (464):  0.07288497\n",
      "Test loss at epoch (464):  0.04127758\n",
      "Train loss at epoch (465):  0.072590545\n",
      "Test loss at epoch (465):  0.041250825\n",
      "Train loss at epoch (466):  0.07252566\n",
      "Test loss at epoch (466):  0.041415483\n",
      "Train loss at epoch (467):  0.072730094\n",
      "Test loss at epoch (467):  0.041421186\n",
      "Train loss at epoch (468):  0.073301\n",
      "Test loss at epoch (468):  0.041152693\n",
      "Train loss at epoch (469):  0.072812125\n",
      "Test loss at epoch (469):  0.041104823\n",
      "Train loss at epoch (470):  0.07302399\n",
      "Test loss at epoch (470):  0.04103635\n",
      "Train loss at epoch (471):  0.07278441\n",
      "Test loss at epoch (471):  0.041155774\n",
      "Train loss at epoch (472):  0.07293991\n",
      "Test loss at epoch (472):  0.04100876\n",
      "Train loss at epoch (473):  0.0726151\n",
      "Test loss at epoch (473):  0.041160773\n",
      "Train loss at epoch (474):  0.072804086\n",
      "Test loss at epoch (474):  0.041145265\n",
      "Train loss at epoch (475):  0.07244494\n",
      "Test loss at epoch (475):  0.041020673\n",
      "Train loss at epoch (476):  0.072582856\n",
      "Test loss at epoch (476):  0.04102174\n",
      "Train loss at epoch (477):  0.07256484\n",
      "Test loss at epoch (477):  0.040940713\n",
      "Train loss at epoch (478):  0.07247879\n",
      "Test loss at epoch (478):  0.040952023\n",
      "Train loss at epoch (479):  0.07267117\n",
      "Test loss at epoch (479):  0.04092388\n",
      "Train loss at epoch (480):  0.07237066\n",
      "Test loss at epoch (480):  0.040967166\n",
      "Train loss at epoch (481):  0.07243522\n",
      "Test loss at epoch (481):  0.0408467\n",
      "Train loss at epoch (482):  0.072589666\n",
      "Test loss at epoch (482):  0.040887106\n",
      "Train loss at epoch (483):  0.07219502\n",
      "Test loss at epoch (483):  0.040850505\n",
      "Train loss at epoch (484):  0.072365075\n",
      "Test loss at epoch (484):  0.040859874\n",
      "Train loss at epoch (485):  0.07222315\n",
      "Test loss at epoch (485):  0.04072665\n",
      "Train loss at epoch (486):  0.07173839\n",
      "Test loss at epoch (486):  0.040642828\n",
      "Train loss at epoch (487):  0.07222709\n",
      "Test loss at epoch (487):  0.040734116\n",
      "Train loss at epoch (488):  0.07223358\n",
      "Test loss at epoch (488):  0.04057635\n",
      "Train loss at epoch (489):  0.07214589\n",
      "Test loss at epoch (489):  0.04058988\n",
      "Train loss at epoch (490):  0.07257051\n",
      "Test loss at epoch (490):  0.040739883\n",
      "Train loss at epoch (491):  0.07181527\n",
      "Test loss at epoch (491):  0.040593114\n",
      "Train loss at epoch (492):  0.072189495\n",
      "Test loss at epoch (492):  0.040582847\n",
      "Train loss at epoch (493):  0.07173819\n",
      "Test loss at epoch (493):  0.040486615\n",
      "Train loss at epoch (494):  0.07165228\n",
      "Test loss at epoch (494):  0.040483348\n",
      "Train loss at epoch (495):  0.071544595\n",
      "Test loss at epoch (495):  0.04055192\n",
      "Train loss at epoch (496):  0.07203256\n",
      "Test loss at epoch (496):  0.0404918\n",
      "Train loss at epoch (497):  0.07186213\n",
      "Test loss at epoch (497):  0.040297\n",
      "Train loss at epoch (498):  0.071272776\n",
      "Test loss at epoch (498):  0.040318556\n",
      "Train loss at epoch (499):  0.07148836\n",
      "Test loss at epoch (499):  0.04041245\n",
      "Train loss at epoch (500):  0.071801074\n",
      "Test loss at epoch (500):  0.040361192\n",
      "Train loss at epoch (501):  0.07170248\n",
      "Test loss at epoch (501):  0.040340155\n",
      "Train loss at epoch (502):  0.07159076\n",
      "Test loss at epoch (502):  0.040276412\n",
      "Train loss at epoch (503):  0.07144249\n",
      "Test loss at epoch (503):  0.04011732\n",
      "Train loss at epoch (504):  0.07140986\n",
      "Test loss at epoch (504):  0.04018713\n",
      "Train loss at epoch (505):  0.071574494\n",
      "Test loss at epoch (505):  0.040005483\n",
      "Train loss at epoch (506):  0.071209885\n",
      "Test loss at epoch (506):  0.040077314\n",
      "Train loss at epoch (507):  0.0714803\n",
      "Test loss at epoch (507):  0.040091116\n",
      "Train loss at epoch (508):  0.07147597\n",
      "Test loss at epoch (508):  0.04016026\n",
      "Train loss at epoch (509):  0.0714529\n",
      "Test loss at epoch (509):  0.040012386\n",
      "Train loss at epoch (510):  0.0716006\n",
      "Test loss at epoch (510):  0.03997692\n",
      "Train loss at epoch (511):  0.07126084\n",
      "Test loss at epoch (511):  0.04005526\n",
      "Train loss at epoch (512):  0.0714591\n",
      "Test loss at epoch (512):  0.04007006\n",
      "Train loss at epoch (513):  0.071243264\n",
      "Test loss at epoch (513):  0.040006366\n",
      "Train loss at epoch (514):  0.07118838\n",
      "Test loss at epoch (514):  0.040007565\n",
      "Train loss at epoch (515):  0.07077677\n",
      "Test loss at epoch (515):  0.040168777\n",
      "Train loss at epoch (516):  0.07149039\n",
      "Test loss at epoch (516):  0.04008247\n",
      "Train loss at epoch (517):  0.07135454\n",
      "Test loss at epoch (517):  0.040142726\n",
      "Train loss at epoch (518):  0.07115665\n",
      "Test loss at epoch (518):  0.039888952\n",
      "Train loss at epoch (519):  0.07130721\n",
      "Test loss at epoch (519):  0.040011697\n",
      "Train loss at epoch (520):  0.07100472\n",
      "Test loss at epoch (520):  0.040009446\n",
      "Train loss at epoch (521):  0.070791304\n",
      "Test loss at epoch (521):  0.039940633\n",
      "Train loss at epoch (522):  0.07118347\n",
      "Test loss at epoch (522):  0.03987707\n",
      "Train loss at epoch (523):  0.071365744\n",
      "Test loss at epoch (523):  0.039917182\n",
      "Train loss at epoch (524):  0.07127151\n",
      "Test loss at epoch (524):  0.03992658\n",
      "Train loss at epoch (525):  0.07067572\n",
      "Test loss at epoch (525):  0.039622225\n",
      "Train loss at epoch (526):  0.071157984\n",
      "Test loss at epoch (526):  0.039581146\n",
      "Train loss at epoch (527):  0.07084307\n",
      "Test loss at epoch (527):  0.039536368\n",
      "Train loss at epoch (528):  0.07074397\n",
      "Test loss at epoch (528):  0.039588615\n",
      "Train loss at epoch (529):  0.07088325\n",
      "Test loss at epoch (529):  0.039675146\n",
      "Train loss at epoch (530):  0.070688166\n",
      "Test loss at epoch (530):  0.039589673\n",
      "Train loss at epoch (531):  0.07083158\n",
      "Test loss at epoch (531):  0.039700124\n",
      "Train loss at epoch (532):  0.07062826\n",
      "Test loss at epoch (532):  0.039639484\n",
      "Train loss at epoch (533):  0.07074021\n",
      "Test loss at epoch (533):  0.039541256\n",
      "Train loss at epoch (534):  0.07075177\n",
      "Test loss at epoch (534):  0.03953386\n",
      "Train loss at epoch (535):  0.07071822\n",
      "Test loss at epoch (535):  0.039567064\n",
      "Train loss at epoch (536):  0.0706059\n",
      "Test loss at epoch (536):  0.03957775\n",
      "Train loss at epoch (537):  0.07093607\n",
      "Test loss at epoch (537):  0.03951085\n",
      "Train loss at epoch (538):  0.07041987\n",
      "Test loss at epoch (538):  0.039462652\n",
      "Train loss at epoch (539):  0.070616096\n",
      "Test loss at epoch (539):  0.03946797\n",
      "Train loss at epoch (540):  0.070563555\n",
      "Test loss at epoch (540):  0.039469566\n",
      "Train loss at epoch (541):  0.07014734\n",
      "Test loss at epoch (541):  0.039434686\n",
      "Train loss at epoch (542):  0.07062624\n",
      "Test loss at epoch (542):  0.039530672\n",
      "Train loss at epoch (543):  0.070518926\n",
      "Test loss at epoch (543):  0.039379884\n",
      "Train loss at epoch (544):  0.07022843\n",
      "Test loss at epoch (544):  0.03928097\n",
      "Train loss at epoch (545):  0.07044778\n",
      "Test loss at epoch (545):  0.03925739\n",
      "Train loss at epoch (546):  0.070527114\n",
      "Test loss at epoch (546):  0.039149348\n",
      "Train loss at epoch (547):  0.070103146\n",
      "Test loss at epoch (547):  0.03921047\n",
      "Train loss at epoch (548):  0.070581794\n",
      "Test loss at epoch (548):  0.039176933\n",
      "Train loss at epoch (549):  0.07023142\n",
      "Test loss at epoch (549):  0.03924702\n",
      "Train loss at epoch (550):  0.0704236\n",
      "Test loss at epoch (550):  0.039168328\n",
      "Train loss at epoch (551):  0.06968653\n",
      "Test loss at epoch (551):  0.039250698\n",
      "Train loss at epoch (552):  0.070134774\n",
      "Test loss at epoch (552):  0.039136887\n",
      "Train loss at epoch (553):  0.06986389\n",
      "Test loss at epoch (553):  0.039059702\n",
      "Train loss at epoch (554):  0.07027736\n",
      "Test loss at epoch (554):  0.039163973\n",
      "Train loss at epoch (555):  0.07027279\n",
      "Test loss at epoch (555):  0.03910878\n",
      "Train loss at epoch (556):  0.07017736\n",
      "Test loss at epoch (556):  0.039028827\n",
      "Train loss at epoch (557):  0.06997526\n",
      "Test loss at epoch (557):  0.038918186\n",
      "Train loss at epoch (558):  0.07010692\n",
      "Test loss at epoch (558):  0.03905137\n",
      "Train loss at epoch (559):  0.06991536\n",
      "Test loss at epoch (559):  0.0389268\n",
      "Train loss at epoch (560):  0.069863185\n",
      "Test loss at epoch (560):  0.038881265\n",
      "Train loss at epoch (561):  0.06957377\n",
      "Test loss at epoch (561):  0.038983166\n",
      "Train loss at epoch (562):  0.069765024\n",
      "Test loss at epoch (562):  0.038931288\n",
      "Train loss at epoch (563):  0.070225544\n",
      "Test loss at epoch (563):  0.03901668\n",
      "Train loss at epoch (564):  0.06986893\n",
      "Test loss at epoch (564):  0.03918128\n",
      "Train loss at epoch (565):  0.07000524\n",
      "Test loss at epoch (565):  0.0389498\n",
      "Train loss at epoch (566):  0.06946983\n",
      "Test loss at epoch (566):  0.03886808\n",
      "Train loss at epoch (567):  0.06971786\n",
      "Test loss at epoch (567):  0.03889538\n",
      "Train loss at epoch (568):  0.07011303\n",
      "Test loss at epoch (568):  0.03885867\n",
      "Train loss at epoch (569):  0.06988262\n",
      "Test loss at epoch (569):  0.038858008\n",
      "Train loss at epoch (570):  0.07018022\n",
      "Test loss at epoch (570):  0.038884886\n",
      "Train loss at epoch (571):  0.06984945\n",
      "Test loss at epoch (571):  0.038834065\n",
      "Train loss at epoch (572):  0.06987215\n",
      "Test loss at epoch (572):  0.038831767\n",
      "Train loss at epoch (573):  0.06966404\n",
      "Test loss at epoch (573):  0.03871476\n",
      "Train loss at epoch (574):  0.06938409\n",
      "Test loss at epoch (574):  0.038644742\n",
      "Train loss at epoch (575):  0.06988208\n",
      "Test loss at epoch (575):  0.038787156\n",
      "Train loss at epoch (576):  0.06902035\n",
      "Test loss at epoch (576):  0.038598716\n",
      "Train loss at epoch (577):  0.06933843\n",
      "Test loss at epoch (577):  0.0386115\n",
      "Train loss at epoch (578):  0.06969877\n",
      "Test loss at epoch (578):  0.038644906\n",
      "Train loss at epoch (579):  0.06953835\n",
      "Test loss at epoch (579):  0.038609955\n",
      "Train loss at epoch (580):  0.06901779\n",
      "Test loss at epoch (580):  0.038606595\n",
      "Train loss at epoch (581):  0.06919068\n",
      "Test loss at epoch (581):  0.038449794\n",
      "Train loss at epoch (582):  0.06914671\n",
      "Test loss at epoch (582):  0.038449336\n",
      "Train loss at epoch (583):  0.0696725\n",
      "Test loss at epoch (583):  0.0385727\n",
      "Train loss at epoch (584):  0.06973828\n",
      "Test loss at epoch (584):  0.038577568\n",
      "Train loss at epoch (585):  0.06939263\n",
      "Test loss at epoch (585):  0.038488775\n",
      "Train loss at epoch (586):  0.06946394\n",
      "Test loss at epoch (586):  0.038529485\n",
      "Train loss at epoch (587):  0.06905124\n",
      "Test loss at epoch (587):  0.038547218\n",
      "Train loss at epoch (588):  0.069275595\n",
      "Test loss at epoch (588):  0.03858134\n",
      "Train loss at epoch (589):  0.06907856\n",
      "Test loss at epoch (589):  0.038495637\n",
      "Train loss at epoch (590):  0.06936352\n",
      "Test loss at epoch (590):  0.038400967\n",
      "Train loss at epoch (591):  0.069425255\n",
      "Test loss at epoch (591):  0.038378045\n",
      "Train loss at epoch (592):  0.0691322\n",
      "Test loss at epoch (592):  0.03833472\n",
      "Train loss at epoch (593):  0.069235824\n",
      "Test loss at epoch (593):  0.038373485\n",
      "Train loss at epoch (594):  0.06893254\n",
      "Test loss at epoch (594):  0.03840662\n",
      "Train loss at epoch (595):  0.0689905\n",
      "Test loss at epoch (595):  0.03829691\n",
      "Train loss at epoch (596):  0.069201514\n",
      "Test loss at epoch (596):  0.038358856\n",
      "Train loss at epoch (597):  0.06912246\n",
      "Test loss at epoch (597):  0.038405403\n",
      "Train loss at epoch (598):  0.06893169\n",
      "Test loss at epoch (598):  0.03834833\n",
      "Train loss at epoch (599):  0.06902575\n",
      "Test loss at epoch (599):  0.03837205\n",
      "Train loss at epoch (600):  0.06896091\n",
      "Test loss at epoch (600):  0.03828952\n",
      "Train loss at epoch (601):  0.068639725\n",
      "Test loss at epoch (601):  0.038339965\n",
      "Train loss at epoch (602):  0.06898913\n",
      "Test loss at epoch (602):  0.0383721\n",
      "Train loss at epoch (603):  0.068959095\n",
      "Test loss at epoch (603):  0.038172495\n",
      "Train loss at epoch (604):  0.06845139\n",
      "Test loss at epoch (604):  0.03818423\n",
      "Train loss at epoch (605):  0.06911808\n",
      "Test loss at epoch (605):  0.038188726\n",
      "Train loss at epoch (606):  0.06880446\n",
      "Test loss at epoch (606):  0.03821594\n",
      "Train loss at epoch (607):  0.06865918\n",
      "Test loss at epoch (607):  0.038140815\n",
      "Train loss at epoch (608):  0.06878856\n",
      "Test loss at epoch (608):  0.038149305\n",
      "Train loss at epoch (609):  0.06883367\n",
      "Test loss at epoch (609):  0.038148526\n",
      "Train loss at epoch (610):  0.06860071\n",
      "Test loss at epoch (610):  0.03797692\n",
      "Train loss at epoch (611):  0.06872881\n",
      "Test loss at epoch (611):  0.037953254\n",
      "Train loss at epoch (612):  0.06893633\n",
      "Test loss at epoch (612):  0.03802044\n",
      "Train loss at epoch (613):  0.068925925\n",
      "Test loss at epoch (613):  0.03796397\n",
      "Train loss at epoch (614):  0.068714745\n",
      "Test loss at epoch (614):  0.037886955\n",
      "Train loss at epoch (615):  0.06837468\n",
      "Test loss at epoch (615):  0.03799202\n",
      "Train loss at epoch (616):  0.068800054\n",
      "Test loss at epoch (616):  0.03794189\n",
      "Train loss at epoch (617):  0.06832803\n",
      "Test loss at epoch (617):  0.03792603\n",
      "Train loss at epoch (618):  0.06848123\n",
      "Test loss at epoch (618):  0.037891053\n",
      "Train loss at epoch (619):  0.06818662\n",
      "Test loss at epoch (619):  0.037909266\n",
      "Train loss at epoch (620):  0.06880961\n",
      "Test loss at epoch (620):  0.037953027\n",
      "Train loss at epoch (621):  0.06900318\n",
      "Test loss at epoch (621):  0.03789813\n",
      "Train loss at epoch (622):  0.06871687\n",
      "Test loss at epoch (622):  0.03794782\n",
      "Train loss at epoch (623):  0.06870278\n",
      "Test loss at epoch (623):  0.037867203\n",
      "Train loss at epoch (624):  0.06793013\n",
      "Test loss at epoch (624):  0.0379504\n",
      "Train loss at epoch (625):  0.06864044\n",
      "Test loss at epoch (625):  0.037864566\n",
      "Train loss at epoch (626):  0.068511136\n",
      "Test loss at epoch (626):  0.037837815\n",
      "Train loss at epoch (627):  0.06831\n",
      "Test loss at epoch (627):  0.037784323\n",
      "Train loss at epoch (628):  0.068230264\n",
      "Test loss at epoch (628):  0.03776211\n",
      "Train loss at epoch (629):  0.06832959\n",
      "Test loss at epoch (629):  0.03769958\n",
      "Train loss at epoch (630):  0.06815298\n",
      "Test loss at epoch (630):  0.037880667\n",
      "Train loss at epoch (631):  0.068281546\n",
      "Test loss at epoch (631):  0.037761617\n",
      "Train loss at epoch (632):  0.0680407\n",
      "Test loss at epoch (632):  0.037586544\n",
      "Train loss at epoch (633):  0.068512686\n",
      "Test loss at epoch (633):  0.037617043\n",
      "Train loss at epoch (634):  0.06876103\n",
      "Test loss at epoch (634):  0.037593815\n",
      "Train loss at epoch (635):  0.06822099\n",
      "Test loss at epoch (635):  0.037654858\n",
      "Train loss at epoch (636):  0.06814331\n",
      "Test loss at epoch (636):  0.03777976\n",
      "Train loss at epoch (637):  0.06817839\n",
      "Test loss at epoch (637):  0.037792355\n",
      "Train loss at epoch (638):  0.06829663\n",
      "Test loss at epoch (638):  0.03770567\n",
      "Train loss at epoch (639):  0.06832561\n",
      "Test loss at epoch (639):  0.037626687\n",
      "Train loss at epoch (640):  0.06801316\n",
      "Test loss at epoch (640):  0.03766341\n",
      "Train loss at epoch (641):  0.06808971\n",
      "Test loss at epoch (641):  0.03763371\n",
      "Train loss at epoch (642):  0.06794513\n",
      "Test loss at epoch (642):  0.037610892\n",
      "Train loss at epoch (643):  0.06809413\n",
      "Test loss at epoch (643):  0.037715748\n",
      "Train loss at epoch (644):  0.06796317\n",
      "Test loss at epoch (644):  0.037592225\n",
      "Train loss at epoch (645):  0.06813404\n",
      "Test loss at epoch (645):  0.037561636\n",
      "Train loss at epoch (646):  0.067741975\n",
      "Test loss at epoch (646):  0.037552863\n",
      "Train loss at epoch (647):  0.06820666\n",
      "Test loss at epoch (647):  0.03747132\n",
      "Train loss at epoch (648):  0.06829085\n",
      "Test loss at epoch (648):  0.037394077\n",
      "Train loss at epoch (649):  0.068283804\n",
      "Test loss at epoch (649):  0.037458707\n",
      "Train loss at epoch (650):  0.067938164\n",
      "Test loss at epoch (650):  0.037438102\n",
      "Train loss at epoch (651):  0.06779942\n",
      "Test loss at epoch (651):  0.037416574\n",
      "Train loss at epoch (652):  0.06750441\n",
      "Test loss at epoch (652):  0.037476756\n",
      "Train loss at epoch (653):  0.06801842\n",
      "Test loss at epoch (653):  0.037425827\n",
      "Train loss at epoch (654):  0.068128\n",
      "Test loss at epoch (654):  0.037470657\n",
      "Train loss at epoch (655):  0.06802912\n",
      "Test loss at epoch (655):  0.037512157\n",
      "Train loss at epoch (656):  0.067768335\n",
      "Test loss at epoch (656):  0.037482332\n",
      "Train loss at epoch (657):  0.06789383\n",
      "Test loss at epoch (657):  0.037341725\n",
      "Train loss at epoch (658):  0.067607015\n",
      "Test loss at epoch (658):  0.03733954\n",
      "Train loss at epoch (659):  0.06786714\n",
      "Test loss at epoch (659):  0.037376866\n",
      "Train loss at epoch (660):  0.06749022\n",
      "Test loss at epoch (660):  0.037263997\n",
      "Train loss at epoch (661):  0.06779234\n",
      "Test loss at epoch (661):  0.03734549\n",
      "Train loss at epoch (662):  0.06789653\n",
      "Test loss at epoch (662):  0.037252974\n",
      "Train loss at epoch (663):  0.067457795\n",
      "Test loss at epoch (663):  0.0373341\n",
      "Train loss at epoch (664):  0.06772077\n",
      "Test loss at epoch (664):  0.037201937\n",
      "Train loss at epoch (665):  0.06744294\n",
      "Test loss at epoch (665):  0.03719967\n",
      "Train loss at epoch (666):  0.067670316\n",
      "Test loss at epoch (666):  0.03714001\n",
      "Train loss at epoch (667):  0.06786895\n",
      "Test loss at epoch (667):  0.037145227\n",
      "Train loss at epoch (668):  0.067516424\n",
      "Test loss at epoch (668):  0.03722179\n",
      "Train loss at epoch (669):  0.06766755\n",
      "Test loss at epoch (669):  0.037180774\n",
      "Train loss at epoch (670):  0.06759656\n",
      "Test loss at epoch (670):  0.037217334\n",
      "Train loss at epoch (671):  0.06742115\n",
      "Test loss at epoch (671):  0.037293546\n",
      "Train loss at epoch (672):  0.0673585\n",
      "Test loss at epoch (672):  0.037158992\n",
      "Train loss at epoch (673):  0.0675037\n",
      "Test loss at epoch (673):  0.037193287\n",
      "Train loss at epoch (674):  0.06749528\n",
      "Test loss at epoch (674):  0.03729006\n",
      "Train loss at epoch (675):  0.067523606\n",
      "Test loss at epoch (675):  0.037119806\n",
      "Train loss at epoch (676):  0.067477375\n",
      "Test loss at epoch (676):  0.03715696\n",
      "Train loss at epoch (677):  0.06735899\n",
      "Test loss at epoch (677):  0.037124883\n",
      "Train loss at epoch (678):  0.06756461\n",
      "Test loss at epoch (678):  0.037133504\n",
      "Train loss at epoch (679):  0.067571275\n",
      "Test loss at epoch (679):  0.03711561\n",
      "Train loss at epoch (680):  0.06728699\n",
      "Test loss at epoch (680):  0.037058365\n",
      "Train loss at epoch (681):  0.06734581\n",
      "Test loss at epoch (681):  0.037145846\n",
      "Train loss at epoch (682):  0.06756319\n",
      "Test loss at epoch (682):  0.037013527\n",
      "Train loss at epoch (683):  0.06735427\n",
      "Test loss at epoch (683):  0.03702155\n",
      "Train loss at epoch (684):  0.06733814\n",
      "Test loss at epoch (684):  0.0369326\n",
      "Train loss at epoch (685):  0.067403294\n",
      "Test loss at epoch (685):  0.036987636\n",
      "Train loss at epoch (686):  0.067436084\n",
      "Test loss at epoch (686):  0.03697723\n",
      "Train loss at epoch (687):  0.067025036\n",
      "Test loss at epoch (687):  0.03693728\n",
      "Train loss at epoch (688):  0.06734484\n",
      "Test loss at epoch (688):  0.036905263\n",
      "Train loss at epoch (689):  0.06733381\n",
      "Test loss at epoch (689):  0.036922153\n",
      "Train loss at epoch (690):  0.067076184\n",
      "Test loss at epoch (690):  0.03686854\n",
      "Train loss at epoch (691):  0.06718783\n",
      "Test loss at epoch (691):  0.03687084\n",
      "Train loss at epoch (692):  0.06706658\n",
      "Test loss at epoch (692):  0.036877774\n",
      "Train loss at epoch (693):  0.06688097\n",
      "Test loss at epoch (693):  0.03690003\n",
      "Train loss at epoch (694):  0.067116186\n",
      "Test loss at epoch (694):  0.036934566\n",
      "Train loss at epoch (695):  0.0671375\n",
      "Test loss at epoch (695):  0.03680789\n",
      "Train loss at epoch (696):  0.06722877\n",
      "Test loss at epoch (696):  0.036961563\n",
      "Train loss at epoch (697):  0.06733309\n",
      "Test loss at epoch (697):  0.036860667\n",
      "Train loss at epoch (698):  0.066608116\n",
      "Test loss at epoch (698):  0.0368727\n",
      "Train loss at epoch (699):  0.066805586\n",
      "Test loss at epoch (699):  0.036914002\n",
      "Train loss at epoch (700):  0.06701345\n",
      "Test loss at epoch (700):  0.036852248\n",
      "Train loss at epoch (701):  0.06696871\n",
      "Test loss at epoch (701):  0.036932368\n",
      "Train loss at epoch (702):  0.06725512\n",
      "Test loss at epoch (702):  0.036881246\n",
      "Train loss at epoch (703):  0.067302786\n",
      "Test loss at epoch (703):  0.036811013\n",
      "Train loss at epoch (704):  0.06709504\n",
      "Test loss at epoch (704):  0.036844734\n",
      "Train loss at epoch (705):  0.0665576\n",
      "Test loss at epoch (705):  0.036740083\n",
      "Train loss at epoch (706):  0.06695388\n",
      "Test loss at epoch (706):  0.036693558\n",
      "Train loss at epoch (707):  0.06661237\n",
      "Test loss at epoch (707):  0.036671188\n",
      "Train loss at epoch (708):  0.06705436\n",
      "Test loss at epoch (708):  0.036681753\n",
      "Train loss at epoch (709):  0.06677743\n",
      "Test loss at epoch (709):  0.03673628\n",
      "Train loss at epoch (710):  0.06722202\n",
      "Test loss at epoch (710):  0.036626365\n",
      "Train loss at epoch (711):  0.06687786\n",
      "Test loss at epoch (711):  0.036558397\n",
      "Train loss at epoch (712):  0.06648941\n",
      "Test loss at epoch (712):  0.03660487\n",
      "Train loss at epoch (713):  0.06705738\n",
      "Test loss at epoch (713):  0.03656835\n",
      "Train loss at epoch (714):  0.06687808\n",
      "Test loss at epoch (714):  0.03666719\n",
      "Train loss at epoch (715):  0.06642279\n",
      "Test loss at epoch (715):  0.036594164\n",
      "Train loss at epoch (716):  0.0664442\n",
      "Test loss at epoch (716):  0.036607254\n",
      "Train loss at epoch (717):  0.06675325\n",
      "Test loss at epoch (717):  0.03671012\n",
      "Train loss at epoch (718):  0.06704258\n",
      "Test loss at epoch (718):  0.03661574\n",
      "Train loss at epoch (719):  0.066741824\n",
      "Test loss at epoch (719):  0.03662846\n",
      "Train loss at epoch (720):  0.06699307\n",
      "Test loss at epoch (720):  0.036645237\n",
      "Train loss at epoch (721):  0.06689666\n",
      "Test loss at epoch (721):  0.03656863\n",
      "Train loss at epoch (722):  0.06692006\n",
      "Test loss at epoch (722):  0.036582\n",
      "Train loss at epoch (723):  0.06649809\n",
      "Test loss at epoch (723):  0.036463503\n",
      "Train loss at epoch (724):  0.0666345\n",
      "Test loss at epoch (724):  0.036464613\n",
      "Train loss at epoch (725):  0.06670397\n",
      "Test loss at epoch (725):  0.03652756\n",
      "Train loss at epoch (726):  0.06653712\n",
      "Test loss at epoch (726):  0.03649747\n",
      "Train loss at epoch (727):  0.06644185\n",
      "Test loss at epoch (727):  0.036476288\n",
      "Train loss at epoch (728):  0.066333175\n",
      "Test loss at epoch (728):  0.03643933\n",
      "Train loss at epoch (729):  0.066334166\n",
      "Test loss at epoch (729):  0.03633193\n",
      "Train loss at epoch (730):  0.066579536\n",
      "Test loss at epoch (730):  0.036509793\n",
      "Train loss at epoch (731):  0.066543855\n",
      "Test loss at epoch (731):  0.03634933\n",
      "Train loss at epoch (732):  0.066791356\n",
      "Test loss at epoch (732):  0.036402073\n",
      "Train loss at epoch (733):  0.066436574\n",
      "Test loss at epoch (733):  0.036300153\n",
      "Train loss at epoch (734):  0.06654386\n",
      "Test loss at epoch (734):  0.036405943\n",
      "Train loss at epoch (735):  0.0665914\n",
      "Test loss at epoch (735):  0.036333628\n",
      "Train loss at epoch (736):  0.06610064\n",
      "Test loss at epoch (736):  0.036392584\n",
      "Train loss at epoch (737):  0.06620878\n",
      "Test loss at epoch (737):  0.036328856\n",
      "Train loss at epoch (738):  0.06632569\n",
      "Test loss at epoch (738):  0.036342815\n",
      "Train loss at epoch (739):  0.066519\n",
      "Test loss at epoch (739):  0.03638863\n",
      "Train loss at epoch (740):  0.06681124\n",
      "Test loss at epoch (740):  0.036391832\n",
      "Train loss at epoch (741):  0.06655903\n",
      "Test loss at epoch (741):  0.036352526\n",
      "Train loss at epoch (742):  0.06651898\n",
      "Test loss at epoch (742):  0.03635206\n",
      "Train loss at epoch (743):  0.066435635\n",
      "Test loss at epoch (743):  0.03628981\n",
      "Train loss at epoch (744):  0.06615928\n",
      "Test loss at epoch (744):  0.036366682\n",
      "Train loss at epoch (745):  0.066455185\n",
      "Test loss at epoch (745):  0.03625513\n",
      "Train loss at epoch (746):  0.06638408\n",
      "Test loss at epoch (746):  0.036283437\n",
      "Train loss at epoch (747):  0.066331886\n",
      "Test loss at epoch (747):  0.03643551\n",
      "Train loss at epoch (748):  0.06608221\n",
      "Test loss at epoch (748):  0.03634035\n",
      "Train loss at epoch (749):  0.066429816\n",
      "Test loss at epoch (749):  0.036251474\n",
      "Train loss at epoch (750):  0.06611733\n",
      "Test loss at epoch (750):  0.03629009\n",
      "Train loss at epoch (751):  0.0660127\n",
      "Test loss at epoch (751):  0.036257014\n",
      "Train loss at epoch (752):  0.066296615\n",
      "Test loss at epoch (752):  0.036270186\n",
      "Train loss at epoch (753):  0.06616289\n",
      "Test loss at epoch (753):  0.036312014\n",
      "Train loss at epoch (754):  0.06635948\n",
      "Test loss at epoch (754):  0.036232967\n",
      "Train loss at epoch (755):  0.066549525\n",
      "Test loss at epoch (755):  0.036260433\n",
      "Train loss at epoch (756):  0.06617101\n",
      "Test loss at epoch (756):  0.03619209\n",
      "Train loss at epoch (757):  0.06582332\n",
      "Test loss at epoch (757):  0.03628297\n",
      "Train loss at epoch (758):  0.06609246\n",
      "Test loss at epoch (758):  0.036196195\n",
      "Train loss at epoch (759):  0.06578027\n",
      "Test loss at epoch (759):  0.036150645\n",
      "Train loss at epoch (760):  0.066398814\n",
      "Test loss at epoch (760):  0.03619642\n",
      "Train loss at epoch (761):  0.06588732\n",
      "Test loss at epoch (761):  0.036129672\n",
      "Train loss at epoch (762):  0.065752\n",
      "Test loss at epoch (762):  0.03609168\n",
      "Train loss at epoch (763):  0.065965585\n",
      "Test loss at epoch (763):  0.036163736\n",
      "Train loss at epoch (764):  0.0664918\n",
      "Test loss at epoch (764):  0.036139153\n",
      "Train loss at epoch (765):  0.066036984\n",
      "Test loss at epoch (765):  0.03605973\n",
      "Train loss at epoch (766):  0.06602153\n",
      "Test loss at epoch (766):  0.036067568\n",
      "Train loss at epoch (767):  0.06616859\n",
      "Test loss at epoch (767):  0.03612034\n",
      "Train loss at epoch (768):  0.0660059\n",
      "Test loss at epoch (768):  0.036133897\n",
      "Train loss at epoch (769):  0.06571606\n",
      "Test loss at epoch (769):  0.036051333\n",
      "Train loss at epoch (770):  0.065883815\n",
      "Test loss at epoch (770):  0.036035754\n",
      "Train loss at epoch (771):  0.066002674\n",
      "Test loss at epoch (771):  0.03606862\n",
      "Train loss at epoch (772):  0.06578272\n",
      "Test loss at epoch (772):  0.03597054\n",
      "Train loss at epoch (773):  0.06582507\n",
      "Test loss at epoch (773):  0.035940144\n",
      "Train loss at epoch (774):  0.0658717\n",
      "Test loss at epoch (774):  0.035970982\n",
      "Train loss at epoch (775):  0.066111356\n",
      "Test loss at epoch (775):  0.035957128\n",
      "Train loss at epoch (776):  0.065826714\n",
      "Test loss at epoch (776):  0.035925616\n",
      "Train loss at epoch (777):  0.06582181\n",
      "Test loss at epoch (777):  0.03588743\n",
      "Train loss at epoch (778):  0.06593227\n",
      "Test loss at epoch (778):  0.035939377\n",
      "Train loss at epoch (779):  0.06582293\n",
      "Test loss at epoch (779):  0.035921313\n",
      "Train loss at epoch (780):  0.065728\n",
      "Test loss at epoch (780):  0.0359645\n",
      "Train loss at epoch (781):  0.06593891\n",
      "Test loss at epoch (781):  0.035910867\n",
      "Train loss at epoch (782):  0.06571341\n",
      "Test loss at epoch (782):  0.0358464\n",
      "Train loss at epoch (783):  0.065820225\n",
      "Test loss at epoch (783):  0.03587324\n",
      "Train loss at epoch (784):  0.06582505\n",
      "Test loss at epoch (784):  0.035853826\n",
      "Train loss at epoch (785):  0.06598584\n",
      "Test loss at epoch (785):  0.035887484\n",
      "Train loss at epoch (786):  0.06565344\n",
      "Test loss at epoch (786):  0.035889603\n",
      "Train loss at epoch (787):  0.06571872\n",
      "Test loss at epoch (787):  0.03594082\n",
      "Train loss at epoch (788):  0.065636255\n",
      "Test loss at epoch (788):  0.035967447\n",
      "Train loss at epoch (789):  0.06576997\n",
      "Test loss at epoch (789):  0.03594004\n",
      "Train loss at epoch (790):  0.06567296\n",
      "Test loss at epoch (790):  0.03595793\n",
      "Train loss at epoch (791):  0.06569656\n",
      "Test loss at epoch (791):  0.03594356\n",
      "Train loss at epoch (792):  0.06571369\n",
      "Test loss at epoch (792):  0.03587919\n",
      "Train loss at epoch (793):  0.06558737\n",
      "Test loss at epoch (793):  0.035910618\n",
      "Train loss at epoch (794):  0.065762416\n",
      "Test loss at epoch (794):  0.035840243\n",
      "Train loss at epoch (795):  0.065541334\n",
      "Test loss at epoch (795):  0.035808302\n",
      "Train loss at epoch (796):  0.06549105\n",
      "Test loss at epoch (796):  0.03577656\n",
      "Train loss at epoch (797):  0.06579717\n",
      "Test loss at epoch (797):  0.035889335\n",
      "Train loss at epoch (798):  0.06566272\n",
      "Test loss at epoch (798):  0.03581437\n",
      "Train loss at epoch (799):  0.065602615\n",
      "Test loss at epoch (799):  0.03591813\n",
      "Train loss at epoch (800):  0.06533378\n",
      "Test loss at epoch (800):  0.035882164\n",
      "Train loss at epoch (801):  0.065726064\n",
      "Test loss at epoch (801):  0.03585884\n",
      "Train loss at epoch (802):  0.06596951\n",
      "Test loss at epoch (802):  0.035895515\n",
      "Train loss at epoch (803):  0.0655152\n",
      "Test loss at epoch (803):  0.03584829\n",
      "Train loss at epoch (804):  0.06571104\n",
      "Test loss at epoch (804):  0.035854165\n",
      "Train loss at epoch (805):  0.06597438\n",
      "Test loss at epoch (805):  0.035784498\n",
      "Train loss at epoch (806):  0.06529032\n",
      "Test loss at epoch (806):  0.035821103\n",
      "Train loss at epoch (807):  0.06552466\n",
      "Test loss at epoch (807):  0.035844624\n",
      "Train loss at epoch (808):  0.06540813\n",
      "Test loss at epoch (808):  0.03592219\n",
      "Train loss at epoch (809):  0.06567124\n",
      "Test loss at epoch (809):  0.035825234\n",
      "Train loss at epoch (810):  0.065458834\n",
      "Test loss at epoch (810):  0.035824336\n",
      "Train loss at epoch (811):  0.06529849\n",
      "Test loss at epoch (811):  0.03572162\n",
      "Train loss at epoch (812):  0.06542281\n",
      "Test loss at epoch (812):  0.035652842\n",
      "Train loss at epoch (813):  0.06596325\n",
      "Test loss at epoch (813):  0.035648596\n",
      "Train loss at epoch (814):  0.06593099\n",
      "Test loss at epoch (814):  0.035624456\n",
      "Train loss at epoch (815):  0.06541055\n",
      "Test loss at epoch (815):  0.03569913\n",
      "Train loss at epoch (816):  0.065609194\n",
      "Test loss at epoch (816):  0.035611466\n",
      "Train loss at epoch (817):  0.06532293\n",
      "Test loss at epoch (817):  0.035611372\n",
      "Train loss at epoch (818):  0.065109566\n",
      "Test loss at epoch (818):  0.035710633\n",
      "Train loss at epoch (819):  0.06534856\n",
      "Test loss at epoch (819):  0.035595205\n",
      "Train loss at epoch (820):  0.06536037\n",
      "Test loss at epoch (820):  0.035604663\n",
      "Train loss at epoch (821):  0.065754496\n",
      "Test loss at epoch (821):  0.03554702\n",
      "Train loss at epoch (822):  0.065612555\n",
      "Test loss at epoch (822):  0.03556298\n",
      "Train loss at epoch (823):  0.06556105\n",
      "Test loss at epoch (823):  0.035610657\n",
      "Train loss at epoch (824):  0.065619186\n",
      "Test loss at epoch (824):  0.035628676\n",
      "Train loss at epoch (825):  0.06525494\n",
      "Test loss at epoch (825):  0.03562713\n",
      "Train loss at epoch (826):  0.06525133\n",
      "Test loss at epoch (826):  0.03557222\n",
      "Train loss at epoch (827):  0.06517262\n",
      "Test loss at epoch (827):  0.035553254\n",
      "Train loss at epoch (828):  0.06514907\n",
      "Test loss at epoch (828):  0.03561102\n",
      "Train loss at epoch (829):  0.065747455\n",
      "Test loss at epoch (829):  0.035529014\n",
      "Train loss at epoch (830):  0.06514143\n",
      "Test loss at epoch (830):  0.03564572\n",
      "Train loss at epoch (831):  0.06522241\n",
      "Test loss at epoch (831):  0.03555723\n",
      "Train loss at epoch (832):  0.06517872\n",
      "Test loss at epoch (832):  0.035663243\n",
      "Train loss at epoch (833):  0.06533317\n",
      "Test loss at epoch (833):  0.035540476\n",
      "Train loss at epoch (834):  0.065422505\n",
      "Test loss at epoch (834):  0.035606768\n",
      "Train loss at epoch (835):  0.06531638\n",
      "Test loss at epoch (835):  0.035620067\n",
      "Train loss at epoch (836):  0.065180264\n",
      "Test loss at epoch (836):  0.03561075\n",
      "Train loss at epoch (837):  0.064954184\n",
      "Test loss at epoch (837):  0.03560161\n",
      "Train loss at epoch (838):  0.06513578\n",
      "Test loss at epoch (838):  0.035599686\n",
      "Train loss at epoch (839):  0.06536339\n",
      "Test loss at epoch (839):  0.035519835\n",
      "Train loss at epoch (840):  0.065236025\n",
      "Test loss at epoch (840):  0.035502166\n",
      "Train loss at epoch (841):  0.065053314\n",
      "Test loss at epoch (841):  0.03550121\n",
      "Train loss at epoch (842):  0.06509185\n",
      "Test loss at epoch (842):  0.035471205\n",
      "Train loss at epoch (843):  0.06499095\n",
      "Test loss at epoch (843):  0.035491977\n",
      "Train loss at epoch (844):  0.06532526\n",
      "Test loss at epoch (844):  0.035429187\n",
      "Train loss at epoch (845):  0.06496968\n",
      "Test loss at epoch (845):  0.035495345\n",
      "Train loss at epoch (846):  0.065419674\n",
      "Test loss at epoch (846):  0.035456304\n",
      "Train loss at epoch (847):  0.06480559\n",
      "Test loss at epoch (847):  0.035548083\n",
      "Train loss at epoch (848):  0.06505461\n",
      "Test loss at epoch (848):  0.035423625\n",
      "Train loss at epoch (849):  0.064808525\n",
      "Test loss at epoch (849):  0.03540798\n",
      "Train loss at epoch (850):  0.06498545\n",
      "Test loss at epoch (850):  0.035365406\n",
      "Train loss at epoch (851):  0.06464881\n",
      "Test loss at epoch (851):  0.035406824\n",
      "Train loss at epoch (852):  0.06514612\n",
      "Test loss at epoch (852):  0.03540324\n",
      "Train loss at epoch (853):  0.065077715\n",
      "Test loss at epoch (853):  0.035319712\n",
      "Train loss at epoch (854):  0.065275684\n",
      "Test loss at epoch (854):  0.035409987\n",
      "Train loss at epoch (855):  0.06496164\n",
      "Test loss at epoch (855):  0.03535227\n",
      "Train loss at epoch (856):  0.065092206\n",
      "Test loss at epoch (856):  0.035330072\n",
      "Train loss at epoch (857):  0.06492579\n",
      "Test loss at epoch (857):  0.035338476\n",
      "Train loss at epoch (858):  0.06499094\n",
      "Test loss at epoch (858):  0.035337392\n",
      "Train loss at epoch (859):  0.06497487\n",
      "Test loss at epoch (859):  0.035348393\n",
      "Train loss at epoch (860):  0.06499022\n",
      "Test loss at epoch (860):  0.0354626\n",
      "Train loss at epoch (861):  0.06471679\n",
      "Test loss at epoch (861):  0.03531266\n",
      "Train loss at epoch (862):  0.06531296\n",
      "Test loss at epoch (862):  0.035369553\n",
      "Train loss at epoch (863):  0.06478958\n",
      "Test loss at epoch (863):  0.03538164\n",
      "Train loss at epoch (864):  0.06493879\n",
      "Test loss at epoch (864):  0.03532322\n",
      "Train loss at epoch (865):  0.06476198\n",
      "Test loss at epoch (865):  0.035334636\n",
      "Train loss at epoch (866):  0.0651662\n",
      "Test loss at epoch (866):  0.0353991\n",
      "Train loss at epoch (867):  0.06494312\n",
      "Test loss at epoch (867):  0.035312627\n",
      "Train loss at epoch (868):  0.064766474\n",
      "Test loss at epoch (868):  0.035342883\n",
      "Train loss at epoch (869):  0.06504142\n",
      "Test loss at epoch (869):  0.03526279\n",
      "Train loss at epoch (870):  0.06462157\n",
      "Test loss at epoch (870):  0.035273455\n",
      "Train loss at epoch (871):  0.064890854\n",
      "Test loss at epoch (871):  0.035295997\n",
      "Train loss at epoch (872):  0.06485351\n",
      "Test loss at epoch (872):  0.035248924\n",
      "Train loss at epoch (873):  0.064822815\n",
      "Test loss at epoch (873):  0.035288084\n",
      "Train loss at epoch (874):  0.06470158\n",
      "Test loss at epoch (874):  0.035229653\n",
      "Train loss at epoch (875):  0.06493485\n",
      "Test loss at epoch (875):  0.0352722\n",
      "Train loss at epoch (876):  0.06458079\n",
      "Test loss at epoch (876):  0.03523372\n",
      "Train loss at epoch (877):  0.06464637\n",
      "Test loss at epoch (877):  0.03520212\n",
      "Train loss at epoch (878):  0.06511248\n",
      "Test loss at epoch (878):  0.035151217\n",
      "Train loss at epoch (879):  0.06477842\n",
      "Test loss at epoch (879):  0.03524147\n",
      "Train loss at epoch (880):  0.06486787\n",
      "Test loss at epoch (880):  0.0351443\n",
      "Train loss at epoch (881):  0.064890884\n",
      "Test loss at epoch (881):  0.035101745\n",
      "Train loss at epoch (882):  0.064780936\n",
      "Test loss at epoch (882):  0.035103004\n",
      "Train loss at epoch (883):  0.064559005\n",
      "Test loss at epoch (883):  0.035148017\n",
      "Train loss at epoch (884):  0.064800985\n",
      "Test loss at epoch (884):  0.035094827\n",
      "Train loss at epoch (885):  0.064745754\n",
      "Test loss at epoch (885):  0.03513035\n",
      "Train loss at epoch (886):  0.06495172\n",
      "Test loss at epoch (886):  0.035122324\n",
      "Train loss at epoch (887):  0.0646748\n",
      "Test loss at epoch (887):  0.035060786\n",
      "Train loss at epoch (888):  0.06447754\n",
      "Test loss at epoch (888):  0.035119038\n",
      "Train loss at epoch (889):  0.06480921\n",
      "Test loss at epoch (889):  0.03508157\n",
      "Train loss at epoch (890):  0.06475798\n",
      "Test loss at epoch (890):  0.0350964\n",
      "Train loss at epoch (891):  0.06493948\n",
      "Test loss at epoch (891):  0.035079647\n",
      "Train loss at epoch (892):  0.06486366\n",
      "Test loss at epoch (892):  0.035009097\n",
      "Train loss at epoch (893):  0.06470025\n",
      "Test loss at epoch (893):  0.03504637\n",
      "Train loss at epoch (894):  0.06481402\n",
      "Test loss at epoch (894):  0.03507939\n",
      "Train loss at epoch (895):  0.06462868\n",
      "Test loss at epoch (895):  0.03504776\n",
      "Train loss at epoch (896):  0.06455151\n",
      "Test loss at epoch (896):  0.035018157\n",
      "Train loss at epoch (897):  0.065000296\n",
      "Test loss at epoch (897):  0.035032496\n",
      "Train loss at epoch (898):  0.064302444\n",
      "Test loss at epoch (898):  0.035040487\n",
      "Train loss at epoch (899):  0.0646793\n",
      "Test loss at epoch (899):  0.03501488\n",
      "Train loss at epoch (900):  0.06460549\n",
      "Test loss at epoch (900):  0.035085127\n",
      "Train loss at epoch (901):  0.06478909\n",
      "Test loss at epoch (901):  0.035026424\n",
      "Train loss at epoch (902):  0.06428716\n",
      "Test loss at epoch (902):  0.035046857\n",
      "Train loss at epoch (903):  0.06451793\n",
      "Test loss at epoch (903):  0.03507337\n",
      "Train loss at epoch (904):  0.06479342\n",
      "Test loss at epoch (904):  0.0350011\n",
      "Train loss at epoch (905):  0.06447638\n",
      "Test loss at epoch (905):  0.03497166\n",
      "Train loss at epoch (906):  0.064384386\n",
      "Test loss at epoch (906):  0.034967277\n",
      "Train loss at epoch (907):  0.064156294\n",
      "Test loss at epoch (907):  0.03498545\n",
      "Train loss at epoch (908):  0.06479011\n",
      "Test loss at epoch (908):  0.03496139\n",
      "Train loss at epoch (909):  0.064496845\n",
      "Test loss at epoch (909):  0.034925252\n",
      "Train loss at epoch (910):  0.06469514\n",
      "Test loss at epoch (910):  0.03493393\n",
      "Train loss at epoch (911):  0.064627334\n",
      "Test loss at epoch (911):  0.034928467\n",
      "Train loss at epoch (912):  0.06447327\n",
      "Test loss at epoch (912):  0.034939\n",
      "Train loss at epoch (913):  0.06468555\n",
      "Test loss at epoch (913):  0.03492047\n",
      "Train loss at epoch (914):  0.064701125\n",
      "Test loss at epoch (914):  0.03495218\n",
      "Train loss at epoch (915):  0.064344\n",
      "Test loss at epoch (915):  0.03500653\n",
      "Train loss at epoch (916):  0.06404756\n",
      "Test loss at epoch (916):  0.03492475\n",
      "Train loss at epoch (917):  0.06475064\n",
      "Test loss at epoch (917):  0.034920122\n",
      "Train loss at epoch (918):  0.064742945\n",
      "Test loss at epoch (918):  0.03489388\n",
      "Train loss at epoch (919):  0.06423568\n",
      "Test loss at epoch (919):  0.034857195\n",
      "Train loss at epoch (920):  0.06422707\n",
      "Test loss at epoch (920):  0.034860462\n",
      "Train loss at epoch (921):  0.06461403\n",
      "Test loss at epoch (921):  0.034903266\n",
      "Train loss at epoch (922):  0.064255446\n",
      "Test loss at epoch (922):  0.03489752\n",
      "Train loss at epoch (923):  0.06424411\n",
      "Test loss at epoch (923):  0.034901958\n",
      "Train loss at epoch (924):  0.06426344\n",
      "Test loss at epoch (924):  0.034865867\n",
      "Train loss at epoch (925):  0.06463104\n",
      "Test loss at epoch (925):  0.03485052\n",
      "Train loss at epoch (926):  0.06450198\n",
      "Test loss at epoch (926):  0.034905158\n",
      "Train loss at epoch (927):  0.06424646\n",
      "Test loss at epoch (927):  0.034933556\n",
      "Train loss at epoch (928):  0.06425943\n",
      "Test loss at epoch (928):  0.034878556\n",
      "Train loss at epoch (929):  0.06461617\n",
      "Test loss at epoch (929):  0.03484699\n",
      "Train loss at epoch (930):  0.06443767\n",
      "Test loss at epoch (930):  0.034925174\n",
      "Train loss at epoch (931):  0.06444486\n",
      "Test loss at epoch (931):  0.034905024\n",
      "Train loss at epoch (932):  0.064188026\n",
      "Test loss at epoch (932):  0.03490477\n",
      "Train loss at epoch (933):  0.06454689\n",
      "Test loss at epoch (933):  0.034847118\n",
      "Train loss at epoch (934):  0.06439587\n",
      "Test loss at epoch (934):  0.034910534\n",
      "Train loss at epoch (935):  0.064128146\n",
      "Test loss at epoch (935):  0.034920707\n",
      "Train loss at epoch (936):  0.06463251\n",
      "Test loss at epoch (936):  0.034854818\n",
      "Train loss at epoch (937):  0.06448671\n",
      "Test loss at epoch (937):  0.034885693\n",
      "Train loss at epoch (938):  0.0639773\n",
      "Test loss at epoch (938):  0.03488161\n",
      "Train loss at epoch (939):  0.06418115\n",
      "Test loss at epoch (939):  0.034946475\n",
      "Train loss at epoch (940):  0.06447672\n",
      "Test loss at epoch (940):  0.034875996\n",
      "Train loss at epoch (941):  0.06438715\n",
      "Test loss at epoch (941):  0.034880437\n",
      "Train loss at epoch (942):  0.064722344\n",
      "Test loss at epoch (942):  0.03484006\n",
      "Train loss at epoch (943):  0.0643826\n",
      "Test loss at epoch (943):  0.034868687\n",
      "Train loss at epoch (944):  0.06438639\n",
      "Test loss at epoch (944):  0.034806862\n",
      "Train loss at epoch (945):  0.06465178\n",
      "Test loss at epoch (945):  0.034839932\n",
      "Train loss at epoch (946):  0.06417589\n",
      "Test loss at epoch (946):  0.034873817\n",
      "Train loss at epoch (947):  0.06436439\n",
      "Test loss at epoch (947):  0.034877084\n",
      "Train loss at epoch (948):  0.06425174\n",
      "Test loss at epoch (948):  0.034936406\n",
      "Train loss at epoch (949):  0.06464268\n",
      "Test loss at epoch (949):  0.034842186\n",
      "Train loss at epoch (950):  0.06401498\n",
      "Test loss at epoch (950):  0.034801804\n",
      "Train loss at epoch (951):  0.063991345\n",
      "Test loss at epoch (951):  0.034784917\n",
      "Train loss at epoch (952):  0.06406607\n",
      "Test loss at epoch (952):  0.03476099\n",
      "Train loss at epoch (953):  0.06427155\n",
      "Test loss at epoch (953):  0.034773126\n",
      "Train loss at epoch (954):  0.06413779\n",
      "Test loss at epoch (954):  0.034764435\n",
      "Train loss at epoch (955):  0.06405396\n",
      "Test loss at epoch (955):  0.034787007\n",
      "Train loss at epoch (956):  0.06429783\n",
      "Test loss at epoch (956):  0.03478234\n",
      "Train loss at epoch (957):  0.06416143\n",
      "Test loss at epoch (957):  0.03483676\n",
      "Train loss at epoch (958):  0.06436944\n",
      "Test loss at epoch (958):  0.034764476\n",
      "Train loss at epoch (959):  0.06397093\n",
      "Test loss at epoch (959):  0.034742966\n",
      "Train loss at epoch (960):  0.064078726\n",
      "Test loss at epoch (960):  0.03474617\n",
      "Train loss at epoch (961):  0.06425503\n",
      "Test loss at epoch (961):  0.034741297\n",
      "Train loss at epoch (962):  0.06389204\n",
      "Test loss at epoch (962):  0.034721307\n",
      "Train loss at epoch (963):  0.06400428\n",
      "Test loss at epoch (963):  0.034710653\n",
      "Train loss at epoch (964):  0.063810214\n",
      "Test loss at epoch (964):  0.034766167\n",
      "Train loss at epoch (965):  0.064294375\n",
      "Test loss at epoch (965):  0.03472671\n",
      "Train loss at epoch (966):  0.06396847\n",
      "Test loss at epoch (966):  0.03474185\n",
      "Train loss at epoch (967):  0.064182565\n",
      "Test loss at epoch (967):  0.034709483\n",
      "Train loss at epoch (968):  0.06391365\n",
      "Test loss at epoch (968):  0.03474862\n",
      "Train loss at epoch (969):  0.06382256\n",
      "Test loss at epoch (969):  0.034726076\n",
      "Train loss at epoch (970):  0.064326145\n",
      "Test loss at epoch (970):  0.034702662\n",
      "Train loss at epoch (971):  0.06389284\n",
      "Test loss at epoch (971):  0.034705043\n",
      "Train loss at epoch (972):  0.06387314\n",
      "Test loss at epoch (972):  0.034716498\n",
      "Train loss at epoch (973):  0.063779205\n",
      "Test loss at epoch (973):  0.034696456\n",
      "Train loss at epoch (974):  0.06413811\n",
      "Test loss at epoch (974):  0.03465577\n",
      "Train loss at epoch (975):  0.06385016\n",
      "Test loss at epoch (975):  0.03471472\n",
      "Train loss at epoch (976):  0.06423748\n",
      "Test loss at epoch (976):  0.03466533\n",
      "Train loss at epoch (977):  0.06430184\n",
      "Test loss at epoch (977):  0.034674317\n",
      "Train loss at epoch (978):  0.0640984\n",
      "Test loss at epoch (978):  0.03463387\n",
      "Train loss at epoch (979):  0.063966595\n",
      "Test loss at epoch (979):  0.034604736\n",
      "Train loss at epoch (980):  0.0634391\n",
      "Test loss at epoch (980):  0.034629326\n",
      "Train loss at epoch (981):  0.06409318\n",
      "Test loss at epoch (981):  0.03462321\n",
      "Train loss at epoch (982):  0.064057656\n",
      "Test loss at epoch (982):  0.03461535\n",
      "Train loss at epoch (983):  0.06379606\n",
      "Test loss at epoch (983):  0.034641653\n",
      "Train loss at epoch (984):  0.06397617\n",
      "Test loss at epoch (984):  0.03462521\n",
      "Train loss at epoch (985):  0.063567184\n",
      "Test loss at epoch (985):  0.034578323\n",
      "Train loss at epoch (986):  0.0639544\n",
      "Test loss at epoch (986):  0.03457476\n",
      "Train loss at epoch (987):  0.06394701\n",
      "Test loss at epoch (987):  0.0345496\n",
      "Train loss at epoch (988):  0.06395633\n",
      "Test loss at epoch (988):  0.03455312\n",
      "Train loss at epoch (989):  0.06415515\n",
      "Test loss at epoch (989):  0.03454879\n",
      "Train loss at epoch (990):  0.06374337\n",
      "Test loss at epoch (990):  0.034512676\n",
      "Train loss at epoch (991):  0.06426612\n",
      "Test loss at epoch (991):  0.0345498\n",
      "Train loss at epoch (992):  0.063692324\n",
      "Test loss at epoch (992):  0.034571417\n",
      "Train loss at epoch (993):  0.06401766\n",
      "Test loss at epoch (993):  0.03453415\n",
      "Train loss at epoch (994):  0.06380939\n",
      "Test loss at epoch (994):  0.034540646\n",
      "Train loss at epoch (995):  0.063980386\n",
      "Test loss at epoch (995):  0.034486484\n",
      "Train loss at epoch (996):  0.06434803\n",
      "Test loss at epoch (996):  0.034481\n",
      "Train loss at epoch (997):  0.06379173\n",
      "Test loss at epoch (997):  0.03445501\n",
      "Train loss at epoch (998):  0.063978076\n",
      "Test loss at epoch (998):  0.034477443\n",
      "Train loss at epoch (999):  0.06389271\n",
      "Test loss at epoch (999):  0.034482196\n",
      "Train loss at epoch (1000):  0.063756995\n",
      "Test loss at epoch (1000):  0.03452618\n",
      "Train loss at epoch (1001):  0.06407957\n",
      "Test loss at epoch (1001):  0.034528345\n",
      "Train loss at epoch (1002):  0.06403216\n",
      "Test loss at epoch (1002):  0.034532066\n",
      "Train loss at epoch (1003):  0.0641964\n",
      "Test loss at epoch (1003):  0.034519378\n",
      "Train loss at epoch (1004):  0.06387181\n",
      "Test loss at epoch (1004):  0.03455515\n",
      "Train loss at epoch (1005):  0.06424918\n",
      "Test loss at epoch (1005):  0.034585025\n",
      "Train loss at epoch (1006):  0.063689224\n",
      "Test loss at epoch (1006):  0.034552198\n",
      "Train loss at epoch (1007):  0.06382669\n",
      "Test loss at epoch (1007):  0.034536626\n",
      "Train loss at epoch (1008):  0.06416746\n",
      "Test loss at epoch (1008):  0.034545574\n",
      "Train loss at epoch (1009):  0.06360255\n",
      "Test loss at epoch (1009):  0.034532625\n",
      "Train loss at epoch (1010):  0.06360245\n",
      "Test loss at epoch (1010):  0.034500252\n",
      "Train loss at epoch (1011):  0.0635639\n",
      "Test loss at epoch (1011):  0.03448056\n",
      "Train loss at epoch (1012):  0.06383779\n",
      "Test loss at epoch (1012):  0.034495007\n",
      "Train loss at epoch (1013):  0.06381582\n",
      "Test loss at epoch (1013):  0.0344566\n",
      "Train loss at epoch (1014):  0.06394899\n",
      "Test loss at epoch (1014):  0.034523714\n",
      "Train loss at epoch (1015):  0.0639705\n",
      "Test loss at epoch (1015):  0.034521192\n",
      "Train loss at epoch (1016):  0.06395924\n",
      "Test loss at epoch (1016):  0.0345228\n",
      "Train loss at epoch (1017):  0.06374921\n",
      "Test loss at epoch (1017):  0.034512922\n",
      "Train loss at epoch (1018):  0.06377556\n",
      "Test loss at epoch (1018):  0.03451851\n",
      "Train loss at epoch (1019):  0.06391552\n",
      "Test loss at epoch (1019):  0.03449971\n",
      "Train loss at epoch (1020):  0.06380122\n",
      "Test loss at epoch (1020):  0.034543484\n",
      "Train loss at epoch (1021):  0.06392541\n",
      "Test loss at epoch (1021):  0.03451468\n",
      "Train loss at epoch (1022):  0.063937746\n",
      "Test loss at epoch (1022):  0.034514263\n",
      "Train loss at epoch (1023):  0.06386007\n",
      "Test loss at epoch (1023):  0.034535058\n",
      "Train loss at epoch (1024):  0.06373711\n",
      "Test loss at epoch (1024):  0.034492273\n",
      "Train loss at epoch (1025):  0.06380254\n",
      "Test loss at epoch (1025):  0.034456465\n",
      "Train loss at epoch (1026):  0.06424118\n",
      "Test loss at epoch (1026):  0.034494787\n",
      "Train loss at epoch (1027):  0.06368697\n",
      "Test loss at epoch (1027):  0.03448349\n",
      "Train loss at epoch (1028):  0.063758016\n",
      "Test loss at epoch (1028):  0.03447802\n",
      "Train loss at epoch (1029):  0.06334975\n",
      "Test loss at epoch (1029):  0.034451496\n",
      "Train loss at epoch (1030):  0.06390019\n",
      "Test loss at epoch (1030):  0.03447439\n",
      "Train loss at epoch (1031):  0.06392142\n",
      "Test loss at epoch (1031):  0.034490436\n",
      "Train loss at epoch (1032):  0.063693516\n",
      "Test loss at epoch (1032):  0.034527387\n",
      "Train loss at epoch (1033):  0.0635775\n",
      "Test loss at epoch (1033):  0.034467317\n",
      "Train loss at epoch (1034):  0.063615896\n",
      "Test loss at epoch (1034):  0.03445338\n",
      "Train loss at epoch (1035):  0.06389451\n",
      "Test loss at epoch (1035):  0.034438595\n",
      "Train loss at epoch (1036):  0.06385012\n",
      "Test loss at epoch (1036):  0.034408316\n",
      "Train loss at epoch (1037):  0.063960254\n",
      "Test loss at epoch (1037):  0.03447249\n",
      "Train loss at epoch (1038):  0.06366922\n",
      "Test loss at epoch (1038):  0.0344329\n",
      "Train loss at epoch (1039):  0.06353148\n",
      "Test loss at epoch (1039):  0.034441486\n",
      "Train loss at epoch (1040):  0.0636976\n",
      "Test loss at epoch (1040):  0.03445021\n",
      "Train loss at epoch (1041):  0.063796684\n",
      "Test loss at epoch (1041):  0.03438148\n",
      "Train loss at epoch (1042):  0.063593045\n",
      "Test loss at epoch (1042):  0.034399617\n",
      "Train loss at epoch (1043):  0.06382758\n",
      "Test loss at epoch (1043):  0.034447238\n",
      "Train loss at epoch (1044):  0.0638404\n",
      "Test loss at epoch (1044):  0.034397293\n",
      "Train loss at epoch (1045):  0.06347832\n",
      "Test loss at epoch (1045):  0.034415837\n",
      "Train loss at epoch (1046):  0.06367786\n",
      "Test loss at epoch (1046):  0.034386296\n",
      "Train loss at epoch (1047):  0.06361083\n",
      "Test loss at epoch (1047):  0.034349065\n",
      "Train loss at epoch (1048):  0.06369344\n",
      "Test loss at epoch (1048):  0.034405664\n",
      "Train loss at epoch (1049):  0.063194945\n",
      "Test loss at epoch (1049):  0.034367178\n",
      "Train loss at epoch (1050):  0.06380199\n",
      "Test loss at epoch (1050):  0.034343835\n",
      "Train loss at epoch (1051):  0.06342146\n",
      "Test loss at epoch (1051):  0.034436632\n",
      "Train loss at epoch (1052):  0.06363868\n",
      "Test loss at epoch (1052):  0.034392707\n",
      "Train loss at epoch (1053):  0.06354889\n",
      "Test loss at epoch (1053):  0.03439185\n",
      "Train loss at epoch (1054):  0.06324933\n",
      "Test loss at epoch (1054):  0.03442214\n",
      "Train loss at epoch (1055):  0.06354258\n",
      "Test loss at epoch (1055):  0.034405652\n",
      "Train loss at epoch (1056):  0.063526616\n",
      "Test loss at epoch (1056):  0.03440973\n",
      "Train loss at epoch (1057):  0.063530676\n",
      "Test loss at epoch (1057):  0.034401044\n",
      "Train loss at epoch (1058):  0.06338734\n",
      "Test loss at epoch (1058):  0.034389485\n",
      "Train loss at epoch (1059):  0.063768364\n",
      "Test loss at epoch (1059):  0.034405194\n",
      "Train loss at epoch (1060):  0.0638175\n",
      "Test loss at epoch (1060):  0.03441128\n",
      "Train loss at epoch (1061):  0.06376524\n",
      "Test loss at epoch (1061):  0.034384523\n",
      "Train loss at epoch (1062):  0.063678786\n",
      "Test loss at epoch (1062):  0.03437428\n",
      "Train loss at epoch (1063):  0.06372054\n",
      "Test loss at epoch (1063):  0.03435899\n",
      "Train loss at epoch (1064):  0.06361208\n",
      "Test loss at epoch (1064):  0.03437173\n",
      "Train loss at epoch (1065):  0.06363846\n",
      "Test loss at epoch (1065):  0.034352783\n",
      "Train loss at epoch (1066):  0.06366634\n",
      "Test loss at epoch (1066):  0.034335822\n",
      "Train loss at epoch (1067):  0.06314371\n",
      "Test loss at epoch (1067):  0.0343829\n",
      "Train loss at epoch (1068):  0.063603975\n",
      "Test loss at epoch (1068):  0.034387246\n",
      "Train loss at epoch (1069):  0.063690074\n",
      "Test loss at epoch (1069):  0.034394823\n",
      "Train loss at epoch (1070):  0.06357232\n",
      "Test loss at epoch (1070):  0.03437178\n",
      "Train loss at epoch (1071):  0.06367052\n",
      "Test loss at epoch (1071):  0.03437726\n",
      "Train loss at epoch (1072):  0.063395046\n",
      "Test loss at epoch (1072):  0.03440121\n",
      "Train loss at epoch (1073):  0.063566215\n",
      "Test loss at epoch (1073):  0.03436362\n",
      "Train loss at epoch (1074):  0.06325747\n",
      "Test loss at epoch (1074):  0.034326263\n",
      "Train loss at epoch (1075):  0.06366245\n",
      "Test loss at epoch (1075):  0.034340173\n",
      "Train loss at epoch (1076):  0.06355896\n",
      "Test loss at epoch (1076):  0.0343594\n",
      "Train loss at epoch (1077):  0.06309287\n",
      "Test loss at epoch (1077):  0.034343217\n",
      "Train loss at epoch (1078):  0.06359189\n",
      "Test loss at epoch (1078):  0.034326583\n",
      "Train loss at epoch (1079):  0.06346454\n",
      "Test loss at epoch (1079):  0.034301177\n",
      "Train loss at epoch (1080):  0.063824214\n",
      "Test loss at epoch (1080):  0.034305748\n",
      "Train loss at epoch (1081):  0.063449815\n",
      "Test loss at epoch (1081):  0.034281198\n",
      "Train loss at epoch (1082):  0.063828595\n",
      "Test loss at epoch (1082):  0.034314405\n",
      "Train loss at epoch (1083):  0.0637417\n",
      "Test loss at epoch (1083):  0.034319136\n",
      "Train loss at epoch (1084):  0.06353641\n",
      "Test loss at epoch (1084):  0.034303915\n",
      "Train loss at epoch (1085):  0.063288875\n",
      "Test loss at epoch (1085):  0.03433255\n",
      "Train loss at epoch (1086):  0.0634061\n",
      "Test loss at epoch (1086):  0.03429358\n",
      "Train loss at epoch (1087):  0.06338736\n",
      "Test loss at epoch (1087):  0.034278046\n",
      "Train loss at epoch (1088):  0.0635057\n",
      "Test loss at epoch (1088):  0.034269918\n",
      "Train loss at epoch (1089):  0.0634389\n",
      "Test loss at epoch (1089):  0.034294747\n",
      "Train loss at epoch (1090):  0.06331298\n",
      "Test loss at epoch (1090):  0.034328938\n",
      "Train loss at epoch (1091):  0.063641876\n",
      "Test loss at epoch (1091):  0.034273833\n",
      "Train loss at epoch (1092):  0.06346429\n",
      "Test loss at epoch (1092):  0.034290563\n",
      "Train loss at epoch (1093):  0.06360491\n",
      "Test loss at epoch (1093):  0.034280825\n",
      "Train loss at epoch (1094):  0.06360187\n",
      "Test loss at epoch (1094):  0.034264352\n",
      "Train loss at epoch (1095):  0.06321588\n",
      "Test loss at epoch (1095):  0.034236565\n",
      "Train loss at epoch (1096):  0.06347571\n",
      "Test loss at epoch (1096):  0.03426068\n",
      "Train loss at epoch (1097):  0.06363468\n",
      "Test loss at epoch (1097):  0.03426477\n",
      "Train loss at epoch (1098):  0.062897995\n",
      "Test loss at epoch (1098):  0.034257084\n",
      "Train loss at epoch (1099):  0.06346953\n",
      "Test loss at epoch (1099):  0.03421844\n",
      "Train loss at epoch (1100):  0.06357187\n",
      "Test loss at epoch (1100):  0.03422826\n",
      "Train loss at epoch (1101):  0.06316907\n",
      "Test loss at epoch (1101):  0.034200877\n",
      "Train loss at epoch (1102):  0.063543715\n",
      "Test loss at epoch (1102):  0.034232955\n",
      "Train loss at epoch (1103):  0.06326736\n",
      "Test loss at epoch (1103):  0.034211487\n",
      "Train loss at epoch (1104):  0.06320904\n",
      "Test loss at epoch (1104):  0.03421487\n",
      "Train loss at epoch (1105):  0.06358901\n",
      "Test loss at epoch (1105):  0.034205187\n",
      "Train loss at epoch (1106):  0.06331157\n",
      "Test loss at epoch (1106):  0.034227878\n",
      "Train loss at epoch (1107):  0.063854136\n",
      "Test loss at epoch (1107):  0.03422992\n",
      "Train loss at epoch (1108):  0.06334672\n",
      "Test loss at epoch (1108):  0.034232896\n",
      "Train loss at epoch (1109):  0.063359074\n",
      "Test loss at epoch (1109):  0.034219358\n",
      "Train loss at epoch (1110):  0.063697174\n",
      "Test loss at epoch (1110):  0.034250442\n",
      "Train loss at epoch (1111):  0.06352594\n",
      "Test loss at epoch (1111):  0.034251798\n",
      "Train loss at epoch (1112):  0.06359053\n",
      "Test loss at epoch (1112):  0.03425917\n",
      "Train loss at epoch (1113):  0.06334341\n",
      "Test loss at epoch (1113):  0.034247767\n",
      "Train loss at epoch (1114):  0.063263826\n",
      "Test loss at epoch (1114):  0.034208067\n",
      "Train loss at epoch (1115):  0.06329322\n",
      "Test loss at epoch (1115):  0.034221534\n",
      "Train loss at epoch (1116):  0.06338452\n",
      "Test loss at epoch (1116):  0.03422941\n",
      "Train loss at epoch (1117):  0.06341104\n",
      "Test loss at epoch (1117):  0.034192916\n",
      "Train loss at epoch (1118):  0.06331693\n",
      "Test loss at epoch (1118):  0.034248643\n",
      "Train loss at epoch (1119):  0.0630663\n",
      "Test loss at epoch (1119):  0.034226444\n",
      "Train loss at epoch (1120):  0.06353385\n",
      "Test loss at epoch (1120):  0.034205236\n",
      "Train loss at epoch (1121):  0.063763574\n",
      "Test loss at epoch (1121):  0.034205638\n",
      "Train loss at epoch (1122):  0.063516945\n",
      "Test loss at epoch (1122):  0.03418951\n",
      "Train loss at epoch (1123):  0.06331728\n",
      "Test loss at epoch (1123):  0.03415596\n",
      "Train loss at epoch (1124):  0.06349282\n",
      "Test loss at epoch (1124):  0.03417282\n",
      "Train loss at epoch (1125):  0.06319156\n",
      "Test loss at epoch (1125):  0.034172107\n",
      "Train loss at epoch (1126):  0.06346071\n",
      "Test loss at epoch (1126):  0.03420129\n",
      "Train loss at epoch (1127):  0.063635066\n",
      "Test loss at epoch (1127):  0.034272645\n",
      "Train loss at epoch (1128):  0.063182995\n",
      "Test loss at epoch (1128):  0.03418551\n",
      "Train loss at epoch (1129):  0.063030526\n",
      "Test loss at epoch (1129):  0.034154527\n",
      "Train loss at epoch (1130):  0.06347447\n",
      "Test loss at epoch (1130):  0.03414309\n",
      "Train loss at epoch (1131):  0.063270114\n",
      "Test loss at epoch (1131):  0.034187835\n",
      "Train loss at epoch (1132):  0.063394144\n",
      "Test loss at epoch (1132):  0.034126088\n",
      "Train loss at epoch (1133):  0.0634576\n",
      "Test loss at epoch (1133):  0.034152772\n",
      "Train loss at epoch (1134):  0.063260004\n",
      "Test loss at epoch (1134):  0.034138095\n",
      "Train loss at epoch (1135):  0.062943354\n",
      "Test loss at epoch (1135):  0.034165513\n",
      "Train loss at epoch (1136):  0.06308879\n",
      "Test loss at epoch (1136):  0.034107216\n",
      "Train loss at epoch (1137):  0.06312813\n",
      "Test loss at epoch (1137):  0.034139108\n",
      "Train loss at epoch (1138):  0.063523486\n",
      "Test loss at epoch (1138):  0.034154516\n",
      "Train loss at epoch (1139):  0.06349833\n",
      "Test loss at epoch (1139):  0.034144368\n",
      "Train loss at epoch (1140):  0.063129194\n",
      "Test loss at epoch (1140):  0.034127314\n",
      "Train loss at epoch (1141):  0.06334118\n",
      "Test loss at epoch (1141):  0.034144424\n",
      "Train loss at epoch (1142):  0.06343224\n",
      "Test loss at epoch (1142):  0.034128767\n",
      "Train loss at epoch (1143):  0.06343919\n",
      "Test loss at epoch (1143):  0.034144282\n",
      "Train loss at epoch (1144):  0.063167006\n",
      "Test loss at epoch (1144):  0.03414388\n",
      "Train loss at epoch (1145):  0.06330785\n",
      "Test loss at epoch (1145):  0.034176596\n",
      "Train loss at epoch (1146):  0.063161545\n",
      "Test loss at epoch (1146):  0.034138583\n",
      "Train loss at epoch (1147):  0.063041456\n",
      "Test loss at epoch (1147):  0.034219805\n",
      "Train loss at epoch (1148):  0.06359416\n",
      "Test loss at epoch (1148):  0.03417352\n",
      "Train loss at epoch (1149):  0.06324407\n",
      "Test loss at epoch (1149):  0.03417656\n",
      "Train loss at epoch (1150):  0.063240975\n",
      "Test loss at epoch (1150):  0.034166425\n",
      "Train loss at epoch (1151):  0.06306739\n",
      "Test loss at epoch (1151):  0.034163915\n",
      "Train loss at epoch (1152):  0.06323171\n",
      "Test loss at epoch (1152):  0.034154363\n",
      "Train loss at epoch (1153):  0.06322557\n",
      "Test loss at epoch (1153):  0.03416425\n",
      "Train loss at epoch (1154):  0.06331292\n",
      "Test loss at epoch (1154):  0.03416773\n",
      "Train loss at epoch (1155):  0.06341603\n",
      "Test loss at epoch (1155):  0.034119207\n",
      "Train loss at epoch (1156):  0.0633005\n",
      "Test loss at epoch (1156):  0.03414035\n",
      "Train loss at epoch (1157):  0.06321736\n",
      "Test loss at epoch (1157):  0.034124583\n",
      "Train loss at epoch (1158):  0.06319856\n",
      "Test loss at epoch (1158):  0.03411781\n",
      "Train loss at epoch (1159):  0.06330173\n",
      "Test loss at epoch (1159):  0.034145746\n",
      "Train loss at epoch (1160):  0.06323379\n",
      "Test loss at epoch (1160):  0.03414881\n",
      "Train loss at epoch (1161):  0.063274145\n",
      "Test loss at epoch (1161):  0.03414325\n",
      "Train loss at epoch (1162):  0.06325462\n",
      "Test loss at epoch (1162):  0.034152083\n",
      "Train loss at epoch (1163):  0.06331372\n",
      "Test loss at epoch (1163):  0.034125768\n",
      "Train loss at epoch (1164):  0.06328314\n",
      "Test loss at epoch (1164):  0.034124564\n",
      "Train loss at epoch (1165):  0.06339485\n",
      "Test loss at epoch (1165):  0.034113232\n",
      "Train loss at epoch (1166):  0.06320236\n",
      "Test loss at epoch (1166):  0.0341082\n",
      "Train loss at epoch (1167):  0.06320379\n",
      "Test loss at epoch (1167):  0.034104493\n",
      "Train loss at epoch (1168):  0.06300928\n",
      "Test loss at epoch (1168):  0.034111727\n",
      "Train loss at epoch (1169):  0.06321744\n",
      "Test loss at epoch (1169):  0.03411139\n",
      "Train loss at epoch (1170):  0.06306607\n",
      "Test loss at epoch (1170):  0.03411293\n",
      "Train loss at epoch (1171):  0.06332134\n",
      "Test loss at epoch (1171):  0.034094263\n",
      "Train loss at epoch (1172):  0.063362986\n",
      "Test loss at epoch (1172):  0.034077127\n",
      "Train loss at epoch (1173):  0.06335195\n",
      "Test loss at epoch (1173):  0.034134246\n",
      "Train loss at epoch (1174):  0.06287999\n",
      "Test loss at epoch (1174):  0.034140866\n",
      "Train loss at epoch (1175):  0.06335836\n",
      "Test loss at epoch (1175):  0.034126434\n",
      "Train loss at epoch (1176):  0.063363925\n",
      "Test loss at epoch (1176):  0.03411052\n",
      "Train loss at epoch (1177):  0.06317672\n",
      "Test loss at epoch (1177):  0.034111165\n",
      "Train loss at epoch (1178):  0.06336665\n",
      "Test loss at epoch (1178):  0.03412376\n",
      "Train loss at epoch (1179):  0.06331787\n",
      "Test loss at epoch (1179):  0.03411068\n",
      "Train loss at epoch (1180):  0.063374005\n",
      "Test loss at epoch (1180):  0.034083284\n",
      "Train loss at epoch (1181):  0.063224815\n",
      "Test loss at epoch (1181):  0.034098774\n",
      "Train loss at epoch (1182):  0.063250326\n",
      "Test loss at epoch (1182):  0.034107853\n",
      "Train loss at epoch (1183):  0.06322281\n",
      "Test loss at epoch (1183):  0.034130327\n",
      "Train loss at epoch (1184):  0.06348497\n",
      "Test loss at epoch (1184):  0.034133554\n",
      "Train loss at epoch (1185):  0.0631756\n",
      "Test loss at epoch (1185):  0.034112897\n",
      "Train loss at epoch (1186):  0.06278432\n",
      "Test loss at epoch (1186):  0.034095418\n",
      "Train loss at epoch (1187):  0.06316744\n",
      "Test loss at epoch (1187):  0.034085117\n",
      "Train loss at epoch (1188):  0.06332315\n",
      "Test loss at epoch (1188):  0.034107435\n",
      "Train loss at epoch (1189):  0.06320782\n",
      "Test loss at epoch (1189):  0.034069862\n",
      "Train loss at epoch (1190):  0.06301359\n",
      "Test loss at epoch (1190):  0.0340563\n",
      "Train loss at epoch (1191):  0.062890776\n",
      "Test loss at epoch (1191):  0.034082346\n",
      "Train loss at epoch (1192):  0.063451655\n",
      "Test loss at epoch (1192):  0.03404026\n",
      "Train loss at epoch (1193):  0.062942155\n",
      "Test loss at epoch (1193):  0.034023758\n",
      "Train loss at epoch (1194):  0.063348055\n",
      "Test loss at epoch (1194):  0.034034085\n",
      "Train loss at epoch (1195):  0.06308999\n",
      "Test loss at epoch (1195):  0.03399769\n",
      "Train loss at epoch (1196):  0.063136324\n",
      "Test loss at epoch (1196):  0.034032095\n",
      "Train loss at epoch (1197):  0.063229464\n",
      "Test loss at epoch (1197):  0.034050476\n",
      "Train loss at epoch (1198):  0.06299906\n",
      "Test loss at epoch (1198):  0.034041643\n",
      "Train loss at epoch (1199):  0.06328827\n",
      "Test loss at epoch (1199):  0.03403623\n",
      "Train loss at epoch (1200):  0.063424736\n",
      "Test loss at epoch (1200):  0.034024566\n",
      "Train loss at epoch (1201):  0.062979154\n",
      "Test loss at epoch (1201):  0.034028552\n",
      "Train loss at epoch (1202):  0.06321745\n",
      "Test loss at epoch (1202):  0.034043934\n",
      "Train loss at epoch (1203):  0.062881954\n",
      "Test loss at epoch (1203):  0.03404145\n",
      "Train loss at epoch (1204):  0.06297568\n",
      "Test loss at epoch (1204):  0.03403732\n",
      "Train loss at epoch (1205):  0.06322923\n",
      "Test loss at epoch (1205):  0.034040086\n",
      "Train loss at epoch (1206):  0.06308375\n",
      "Test loss at epoch (1206):  0.034054093\n",
      "Train loss at epoch (1207):  0.06328234\n",
      "Test loss at epoch (1207):  0.034070417\n",
      "Train loss at epoch (1208):  0.06299055\n",
      "Test loss at epoch (1208):  0.03407805\n",
      "Train loss at epoch (1209):  0.06300365\n",
      "Test loss at epoch (1209):  0.034086477\n",
      "Train loss at epoch (1210):  0.062996626\n",
      "Test loss at epoch (1210):  0.03405306\n",
      "Train loss at epoch (1211):  0.06307677\n",
      "Test loss at epoch (1211):  0.034062937\n",
      "Train loss at epoch (1212):  0.06297867\n",
      "Test loss at epoch (1212):  0.034069933\n",
      "Train loss at epoch (1213):  0.063361146\n",
      "Test loss at epoch (1213):  0.03404782\n",
      "Train loss at epoch (1214):  0.062897585\n",
      "Test loss at epoch (1214):  0.03405099\n",
      "Train loss at epoch (1215):  0.063265845\n",
      "Test loss at epoch (1215):  0.03407182\n",
      "Train loss at epoch (1216):  0.06354869\n",
      "Test loss at epoch (1216):  0.034053396\n",
      "Train loss at epoch (1217):  0.063194014\n",
      "Test loss at epoch (1217):  0.034046937\n",
      "Train loss at epoch (1218):  0.06350073\n",
      "Test loss at epoch (1218):  0.03404294\n",
      "Train loss at epoch (1219):  0.06347092\n",
      "Test loss at epoch (1219):  0.03404181\n",
      "Train loss at epoch (1220):  0.06358338\n",
      "Test loss at epoch (1220):  0.034063295\n",
      "Train loss at epoch (1221):  0.06280075\n",
      "Test loss at epoch (1221):  0.03405039\n",
      "Train loss at epoch (1222):  0.06304089\n",
      "Test loss at epoch (1222):  0.034024313\n",
      "Train loss at epoch (1223):  0.063217014\n",
      "Test loss at epoch (1223):  0.034073997\n",
      "Train loss at epoch (1224):  0.06300252\n",
      "Test loss at epoch (1224):  0.034044914\n",
      "Train loss at epoch (1225):  0.06310724\n",
      "Test loss at epoch (1225):  0.034000106\n",
      "Train loss at epoch (1226):  0.06303719\n",
      "Test loss at epoch (1226):  0.03398977\n",
      "Train loss at epoch (1227):  0.06314924\n",
      "Test loss at epoch (1227):  0.03402095\n",
      "Train loss at epoch (1228):  0.0632102\n",
      "Test loss at epoch (1228):  0.03401938\n",
      "Train loss at epoch (1229):  0.063141264\n",
      "Test loss at epoch (1229):  0.033993796\n",
      "Train loss at epoch (1230):  0.06338016\n",
      "Test loss at epoch (1230):  0.03398702\n",
      "Train loss at epoch (1231):  0.06338365\n",
      "Test loss at epoch (1231):  0.033991307\n",
      "Train loss at epoch (1232):  0.06328516\n",
      "Test loss at epoch (1232):  0.033976007\n",
      "Train loss at epoch (1233):  0.06324505\n",
      "Test loss at epoch (1233):  0.034000073\n",
      "Train loss at epoch (1234):  0.06320744\n",
      "Test loss at epoch (1234):  0.03396009\n",
      "Train loss at epoch (1235):  0.06322293\n",
      "Test loss at epoch (1235):  0.033994164\n",
      "Train loss at epoch (1236):  0.0631929\n",
      "Test loss at epoch (1236):  0.034003872\n",
      "Train loss at epoch (1237):  0.06308873\n",
      "Test loss at epoch (1237):  0.03398992\n",
      "Train loss at epoch (1238):  0.063130446\n",
      "Test loss at epoch (1238):  0.03398984\n",
      "Train loss at epoch (1239):  0.062976785\n",
      "Test loss at epoch (1239):  0.033995546\n",
      "Train loss at epoch (1240):  0.063049175\n",
      "Test loss at epoch (1240):  0.03399667\n",
      "Train loss at epoch (1241):  0.06268289\n",
      "Test loss at epoch (1241):  0.03398863\n",
      "Train loss at epoch (1242):  0.06303958\n",
      "Test loss at epoch (1242):  0.033998698\n",
      "Train loss at epoch (1243):  0.06299325\n",
      "Test loss at epoch (1243):  0.03400061\n",
      "Train loss at epoch (1244):  0.062920615\n",
      "Test loss at epoch (1244):  0.033992615\n",
      "Train loss at epoch (1245):  0.06311179\n",
      "Test loss at epoch (1245):  0.034009505\n",
      "Train loss at epoch (1246):  0.0630353\n",
      "Test loss at epoch (1246):  0.034001797\n",
      "Train loss at epoch (1247):  0.06330414\n",
      "Test loss at epoch (1247):  0.03399227\n",
      "Train loss at epoch (1248):  0.063155614\n",
      "Test loss at epoch (1248):  0.033995442\n",
      "Train loss at epoch (1249):  0.063186735\n",
      "Test loss at epoch (1249):  0.03397718\n",
      "Train loss at epoch (1250):  0.06290263\n",
      "Test loss at epoch (1250):  0.033963215\n",
      "Train loss at epoch (1251):  0.06309019\n",
      "Test loss at epoch (1251):  0.033977237\n",
      "Train loss at epoch (1252):  0.0629066\n",
      "Test loss at epoch (1252):  0.033956718\n",
      "Train loss at epoch (1253):  0.062896155\n",
      "Test loss at epoch (1253):  0.033985395\n",
      "Train loss at epoch (1254):  0.06294665\n",
      "Test loss at epoch (1254):  0.03399065\n",
      "Train loss at epoch (1255):  0.06298165\n",
      "Test loss at epoch (1255):  0.034006\n",
      "Train loss at epoch (1256):  0.06340096\n",
      "Test loss at epoch (1256):  0.033972338\n",
      "Train loss at epoch (1257):  0.063229665\n",
      "Test loss at epoch (1257):  0.033961922\n",
      "Train loss at epoch (1258):  0.06330115\n",
      "Test loss at epoch (1258):  0.03397644\n",
      "Train loss at epoch (1259):  0.06290775\n",
      "Test loss at epoch (1259):  0.03398809\n",
      "Train loss at epoch (1260):  0.06319618\n",
      "Test loss at epoch (1260):  0.033999782\n",
      "Train loss at epoch (1261):  0.06309175\n",
      "Test loss at epoch (1261):  0.03399095\n",
      "Train loss at epoch (1262):  0.062759034\n",
      "Test loss at epoch (1262):  0.03396109\n",
      "Train loss at epoch (1263):  0.06286601\n",
      "Test loss at epoch (1263):  0.033980735\n",
      "Train loss at epoch (1264):  0.062914774\n",
      "Test loss at epoch (1264):  0.033974525\n",
      "Train loss at epoch (1265):  0.06298268\n",
      "Test loss at epoch (1265):  0.03399804\n",
      "Train loss at epoch (1266):  0.06300125\n",
      "Test loss at epoch (1266):  0.034002494\n",
      "Train loss at epoch (1267):  0.06292685\n",
      "Test loss at epoch (1267):  0.033996075\n",
      "Train loss at epoch (1268):  0.06293105\n",
      "Test loss at epoch (1268):  0.03398507\n",
      "Train loss at epoch (1269):  0.06255282\n",
      "Test loss at epoch (1269):  0.033994243\n",
      "Train loss at epoch (1270):  0.06295825\n",
      "Test loss at epoch (1270):  0.033985425\n",
      "Train loss at epoch (1271):  0.062733084\n",
      "Test loss at epoch (1271):  0.03399119\n",
      "Train loss at epoch (1272):  0.06300367\n",
      "Test loss at epoch (1272):  0.0339895\n",
      "Train loss at epoch (1273):  0.063176796\n",
      "Test loss at epoch (1273):  0.03399184\n",
      "Train loss at epoch (1274):  0.06351178\n",
      "Test loss at epoch (1274):  0.0340137\n",
      "Train loss at epoch (1275):  0.06332018\n",
      "Test loss at epoch (1275):  0.034026287\n",
      "Train loss at epoch (1276):  0.063107654\n",
      "Test loss at epoch (1276):  0.033994347\n",
      "Train loss at epoch (1277):  0.06289236\n",
      "Test loss at epoch (1277):  0.0340037\n",
      "Train loss at epoch (1278):  0.06317673\n",
      "Test loss at epoch (1278):  0.03398706\n",
      "Train loss at epoch (1279):  0.06302655\n",
      "Test loss at epoch (1279):  0.03399183\n",
      "Train loss at epoch (1280):  0.06301973\n",
      "Test loss at epoch (1280):  0.03400022\n",
      "Train loss at epoch (1281):  0.06303131\n",
      "Test loss at epoch (1281):  0.033995558\n",
      "Train loss at epoch (1282):  0.06271696\n",
      "Test loss at epoch (1282):  0.03401037\n",
      "Train loss at epoch (1283):  0.063001595\n",
      "Test loss at epoch (1283):  0.034003925\n",
      "Train loss at epoch (1284):  0.06287253\n",
      "Test loss at epoch (1284):  0.03399663\n",
      "Train loss at epoch (1285):  0.0630144\n",
      "Test loss at epoch (1285):  0.034006868\n",
      "Train loss at epoch (1286):  0.063014746\n",
      "Test loss at epoch (1286):  0.033979606\n",
      "Train loss at epoch (1287):  0.06300673\n",
      "Test loss at epoch (1287):  0.033987135\n",
      "Train loss at epoch (1288):  0.062993884\n",
      "Test loss at epoch (1288):  0.03397321\n",
      "Train loss at epoch (1289):  0.06318071\n",
      "Test loss at epoch (1289):  0.033974815\n",
      "Train loss at epoch (1290):  0.062604226\n",
      "Test loss at epoch (1290):  0.03399261\n",
      "Train loss at epoch (1291):  0.06296418\n",
      "Test loss at epoch (1291):  0.03399555\n",
      "Train loss at epoch (1292):  0.06308424\n",
      "Test loss at epoch (1292):  0.033988692\n",
      "Train loss at epoch (1293):  0.063163996\n",
      "Test loss at epoch (1293):  0.033997662\n",
      "Train loss at epoch (1294):  0.06326339\n",
      "Test loss at epoch (1294):  0.033997618\n",
      "Train loss at epoch (1295):  0.06307522\n",
      "Test loss at epoch (1295):  0.03397888\n",
      "Train loss at epoch (1296):  0.0627218\n",
      "Test loss at epoch (1296):  0.0339806\n",
      "Train loss at epoch (1297):  0.06358403\n",
      "Test loss at epoch (1297):  0.03395574\n",
      "Train loss at epoch (1298):  0.06313024\n",
      "Test loss at epoch (1298):  0.033966232\n",
      "Train loss at epoch (1299):  0.063031785\n",
      "Test loss at epoch (1299):  0.033956338\n",
      "Train loss at epoch (1300):  0.06339795\n",
      "Test loss at epoch (1300):  0.03395509\n",
      "Train loss at epoch (1301):  0.06271417\n",
      "Test loss at epoch (1301):  0.033952117\n",
      "Train loss at epoch (1302):  0.06283101\n",
      "Test loss at epoch (1302):  0.03397818\n",
      "Train loss at epoch (1303):  0.06296552\n",
      "Test loss at epoch (1303):  0.033967823\n",
      "Train loss at epoch (1304):  0.06293885\n",
      "Test loss at epoch (1304):  0.033961684\n",
      "Train loss at epoch (1305):  0.063268155\n",
      "Test loss at epoch (1305):  0.03396242\n",
      "Train loss at epoch (1306):  0.06294532\n",
      "Test loss at epoch (1306):  0.033973377\n",
      "Train loss at epoch (1307):  0.06328306\n",
      "Test loss at epoch (1307):  0.033958208\n",
      "Train loss at epoch (1308):  0.06306617\n",
      "Test loss at epoch (1308):  0.033960637\n",
      "Train loss at epoch (1309):  0.06305002\n",
      "Test loss at epoch (1309):  0.03396074\n",
      "Train loss at epoch (1310):  0.06272528\n",
      "Test loss at epoch (1310):  0.033947937\n",
      "Train loss at epoch (1311):  0.063329756\n",
      "Test loss at epoch (1311):  0.03397027\n",
      "Train loss at epoch (1312):  0.06303789\n",
      "Test loss at epoch (1312):  0.033977803\n",
      "Train loss at epoch (1313):  0.06295402\n",
      "Test loss at epoch (1313):  0.03398384\n",
      "Train loss at epoch (1314):  0.06308975\n",
      "Test loss at epoch (1314):  0.033970226\n",
      "Train loss at epoch (1315):  0.062728055\n",
      "Test loss at epoch (1315):  0.033940405\n",
      "Train loss at epoch (1316):  0.06277024\n",
      "Test loss at epoch (1316):  0.033962548\n",
      "Train loss at epoch (1317):  0.06315399\n",
      "Test loss at epoch (1317):  0.033962514\n",
      "Train loss at epoch (1318):  0.06263354\n",
      "Test loss at epoch (1318):  0.033952605\n",
      "Train loss at epoch (1319):  0.06303095\n",
      "Test loss at epoch (1319):  0.033954162\n",
      "Train loss at epoch (1320):  0.06303945\n",
      "Test loss at epoch (1320):  0.033936325\n",
      "Train loss at epoch (1321):  0.06308424\n",
      "Test loss at epoch (1321):  0.033971258\n",
      "Train loss at epoch (1322):  0.06305419\n",
      "Test loss at epoch (1322):  0.03394751\n",
      "Train loss at epoch (1323):  0.0626654\n",
      "Test loss at epoch (1323):  0.033947945\n",
      "Train loss at epoch (1324):  0.0628834\n",
      "Test loss at epoch (1324):  0.03395612\n",
      "Train loss at epoch (1325):  0.063257925\n",
      "Test loss at epoch (1325):  0.033943053\n",
      "Train loss at epoch (1326):  0.063192345\n",
      "Test loss at epoch (1326):  0.033942986\n",
      "Train loss at epoch (1327):  0.06309116\n",
      "Test loss at epoch (1327):  0.033945125\n",
      "Train loss at epoch (1328):  0.06272036\n",
      "Test loss at epoch (1328):  0.03394615\n",
      "Train loss at epoch (1329):  0.063124105\n",
      "Test loss at epoch (1329):  0.033941496\n",
      "Train loss at epoch (1330):  0.06313183\n",
      "Test loss at epoch (1330):  0.033938564\n",
      "Train loss at epoch (1331):  0.06262872\n",
      "Test loss at epoch (1331):  0.033951428\n",
      "Train loss at epoch (1332):  0.06291944\n",
      "Test loss at epoch (1332):  0.033943605\n",
      "Train loss at epoch (1333):  0.06292776\n",
      "Test loss at epoch (1333):  0.03394704\n",
      "Train loss at epoch (1334):  0.06288357\n",
      "Test loss at epoch (1334):  0.033941798\n",
      "Train loss at epoch (1335):  0.06297731\n",
      "Test loss at epoch (1335):  0.033946544\n",
      "Train loss at epoch (1336):  0.06312034\n",
      "Test loss at epoch (1336):  0.03395387\n",
      "Train loss at epoch (1337):  0.06287477\n",
      "Test loss at epoch (1337):  0.033938054\n",
      "Train loss at epoch (1338):  0.063014485\n",
      "Test loss at epoch (1338):  0.0339286\n",
      "Train loss at epoch (1339):  0.06308527\n",
      "Test loss at epoch (1339):  0.033952054\n",
      "Train loss at epoch (1340):  0.063052334\n",
      "Test loss at epoch (1340):  0.033948887\n",
      "Train loss at epoch (1341):  0.06296048\n",
      "Test loss at epoch (1341):  0.033942435\n",
      "Train loss at epoch (1342):  0.06307253\n",
      "Test loss at epoch (1342):  0.033945706\n",
      "Train loss at epoch (1343):  0.06290854\n",
      "Test loss at epoch (1343):  0.0339596\n",
      "Train loss at epoch (1344):  0.06286214\n",
      "Test loss at epoch (1344):  0.033942807\n",
      "Train loss at epoch (1345):  0.063198105\n",
      "Test loss at epoch (1345):  0.03396224\n",
      "Train loss at epoch (1346):  0.06276928\n",
      "Test loss at epoch (1346):  0.033931185\n",
      "Train loss at epoch (1347):  0.06305519\n",
      "Test loss at epoch (1347):  0.033945147\n",
      "Train loss at epoch (1348):  0.06318085\n",
      "Test loss at epoch (1348):  0.03393883\n",
      "Train loss at epoch (1349):  0.063042\n",
      "Test loss at epoch (1349):  0.033922594\n",
      "Train loss at epoch (1350):  0.062717184\n",
      "Test loss at epoch (1350):  0.03394504\n",
      "Train loss at epoch (1351):  0.06318934\n",
      "Test loss at epoch (1351):  0.033931565\n",
      "Train loss at epoch (1352):  0.06258085\n",
      "Test loss at epoch (1352):  0.033932585\n",
      "Train loss at epoch (1353):  0.06316314\n",
      "Test loss at epoch (1353):  0.03393406\n",
      "Train loss at epoch (1354):  0.062717594\n",
      "Test loss at epoch (1354):  0.033916254\n",
      "Train loss at epoch (1355):  0.06287239\n",
      "Test loss at epoch (1355):  0.033938322\n",
      "Train loss at epoch (1356):  0.06324575\n",
      "Test loss at epoch (1356):  0.03391186\n",
      "Train loss at epoch (1357):  0.06299983\n",
      "Test loss at epoch (1357):  0.03393024\n",
      "Train loss at epoch (1358):  0.06295699\n",
      "Test loss at epoch (1358):  0.033928618\n",
      "Train loss at epoch (1359):  0.063007705\n",
      "Test loss at epoch (1359):  0.03392213\n",
      "Train loss at epoch (1360):  0.06289068\n",
      "Test loss at epoch (1360):  0.033930138\n",
      "Train loss at epoch (1361):  0.062825926\n",
      "Test loss at epoch (1361):  0.03388716\n",
      "Train loss at epoch (1362):  0.06310363\n",
      "Test loss at epoch (1362):  0.033916876\n",
      "Train loss at epoch (1363):  0.06344551\n",
      "Test loss at epoch (1363):  0.033912998\n",
      "Train loss at epoch (1364):  0.063052244\n",
      "Test loss at epoch (1364):  0.033902336\n",
      "Train loss at epoch (1365):  0.0629101\n",
      "Test loss at epoch (1365):  0.033916026\n",
      "Train loss at epoch (1366):  0.062344417\n",
      "Test loss at epoch (1366):  0.033915397\n",
      "Train loss at epoch (1367):  0.06300568\n",
      "Test loss at epoch (1367):  0.03393069\n",
      "Train loss at epoch (1368):  0.063347206\n",
      "Test loss at epoch (1368):  0.03392364\n",
      "Train loss at epoch (1369):  0.06317561\n",
      "Test loss at epoch (1369):  0.03392271\n",
      "Train loss at epoch (1370):  0.06310161\n",
      "Test loss at epoch (1370):  0.03392438\n",
      "Train loss at epoch (1371):  0.06324779\n",
      "Test loss at epoch (1371):  0.033931285\n",
      "Train loss at epoch (1372):  0.06317192\n",
      "Test loss at epoch (1372):  0.03392524\n",
      "Train loss at epoch (1373):  0.06281417\n",
      "Test loss at epoch (1373):  0.033925086\n",
      "Train loss at epoch (1374):  0.06278962\n",
      "Test loss at epoch (1374):  0.03392982\n",
      "Train loss at epoch (1375):  0.06282239\n",
      "Test loss at epoch (1375):  0.03393749\n",
      "Train loss at epoch (1376):  0.06316816\n",
      "Test loss at epoch (1376):  0.033923578\n",
      "Train loss at epoch (1377):  0.06283033\n",
      "Test loss at epoch (1377):  0.033926282\n",
      "Train loss at epoch (1378):  0.06325503\n",
      "Test loss at epoch (1378):  0.03391453\n",
      "Train loss at epoch (1379):  0.06297725\n",
      "Test loss at epoch (1379):  0.033942807\n",
      "Train loss at epoch (1380):  0.06311899\n",
      "Test loss at epoch (1380):  0.033912804\n",
      "Train loss at epoch (1381):  0.063006\n",
      "Test loss at epoch (1381):  0.033911612\n",
      "Train loss at epoch (1382):  0.062597\n",
      "Test loss at epoch (1382):  0.0339268\n",
      "Train loss at epoch (1383):  0.06293593\n",
      "Test loss at epoch (1383):  0.0339213\n",
      "Train loss at epoch (1384):  0.0629606\n",
      "Test loss at epoch (1384):  0.033915594\n",
      "Train loss at epoch (1385):  0.06316662\n",
      "Test loss at epoch (1385):  0.03392904\n",
      "Train loss at epoch (1386):  0.06301052\n",
      "Test loss at epoch (1386):  0.03392412\n",
      "Train loss at epoch (1387):  0.0626133\n",
      "Test loss at epoch (1387):  0.033934336\n",
      "Train loss at epoch (1388):  0.06312082\n",
      "Test loss at epoch (1388):  0.03391829\n",
      "Train loss at epoch (1389):  0.06269058\n",
      "Test loss at epoch (1389):  0.033927284\n",
      "Train loss at epoch (1390):  0.062650055\n",
      "Test loss at epoch (1390):  0.033927895\n",
      "Train loss at epoch (1391):  0.06310022\n",
      "Test loss at epoch (1391):  0.033932775\n",
      "Train loss at epoch (1392):  0.06290731\n",
      "Test loss at epoch (1392):  0.033924516\n",
      "Train loss at epoch (1393):  0.062959224\n",
      "Test loss at epoch (1393):  0.03393784\n",
      "Train loss at epoch (1394):  0.06311341\n",
      "Test loss at epoch (1394):  0.03392629\n",
      "Train loss at epoch (1395):  0.06314687\n",
      "Test loss at epoch (1395):  0.033916343\n",
      "Train loss at epoch (1396):  0.06317184\n",
      "Test loss at epoch (1396):  0.033942915\n",
      "Train loss at epoch (1397):  0.06271588\n",
      "Test loss at epoch (1397):  0.03394292\n",
      "Train loss at epoch (1398):  0.06301143\n",
      "Test loss at epoch (1398):  0.033923842\n",
      "Train loss at epoch (1399):  0.062879436\n",
      "Test loss at epoch (1399):  0.03392065\n",
      "Train loss at epoch (1400):  0.06288964\n",
      "Test loss at epoch (1400):  0.03393225\n",
      "Train loss at epoch (1401):  0.06299103\n",
      "Test loss at epoch (1401):  0.033929776\n",
      "Train loss at epoch (1402):  0.06280765\n",
      "Test loss at epoch (1402):  0.033928797\n",
      "Train loss at epoch (1403):  0.062896326\n",
      "Test loss at epoch (1403):  0.033919197\n",
      "Train loss at epoch (1404):  0.06291874\n",
      "Test loss at epoch (1404):  0.033940643\n",
      "Train loss at epoch (1405):  0.062670335\n",
      "Test loss at epoch (1405):  0.03393422\n",
      "Train loss at epoch (1406):  0.06285636\n",
      "Test loss at epoch (1406):  0.033916026\n",
      "Train loss at epoch (1407):  0.06294119\n",
      "Test loss at epoch (1407):  0.033913624\n",
      "Train loss at epoch (1408):  0.06272752\n",
      "Test loss at epoch (1408):  0.033920057\n",
      "Train loss at epoch (1409):  0.0628146\n",
      "Test loss at epoch (1409):  0.03391998\n",
      "Train loss at epoch (1410):  0.06318934\n",
      "Test loss at epoch (1410):  0.033900324\n",
      "Train loss at epoch (1411):  0.06293083\n",
      "Test loss at epoch (1411):  0.03392604\n",
      "Train loss at epoch (1412):  0.06283468\n",
      "Test loss at epoch (1412):  0.033930045\n",
      "Train loss at epoch (1413):  0.06283247\n",
      "Test loss at epoch (1413):  0.03392733\n",
      "Train loss at epoch (1414):  0.06282458\n",
      "Test loss at epoch (1414):  0.033916913\n",
      "Train loss at epoch (1415):  0.06317518\n",
      "Test loss at epoch (1415):  0.03390885\n",
      "Train loss at epoch (1416):  0.06310959\n",
      "Test loss at epoch (1416):  0.033918016\n",
      "Train loss at epoch (1417):  0.06323738\n",
      "Test loss at epoch (1417):  0.03391825\n",
      "Train loss at epoch (1418):  0.06338056\n",
      "Test loss at epoch (1418):  0.033909317\n",
      "Train loss at epoch (1419):  0.06328714\n",
      "Test loss at epoch (1419):  0.03392084\n",
      "Train loss at epoch (1420):  0.062979005\n",
      "Test loss at epoch (1420):  0.03392762\n",
      "Train loss at epoch (1421):  0.06289499\n",
      "Test loss at epoch (1421):  0.033920012\n",
      "Train loss at epoch (1422):  0.06276606\n",
      "Test loss at epoch (1422):  0.03392929\n",
      "Train loss at epoch (1423):  0.0625537\n",
      "Test loss at epoch (1423):  0.033921525\n",
      "Train loss at epoch (1424):  0.06331709\n",
      "Test loss at epoch (1424):  0.03390524\n",
      "Train loss at epoch (1425):  0.06276722\n",
      "Test loss at epoch (1425):  0.03392302\n",
      "Train loss at epoch (1426):  0.06289417\n",
      "Test loss at epoch (1426):  0.033926558\n",
      "Train loss at epoch (1427):  0.06276436\n",
      "Test loss at epoch (1427):  0.033929646\n",
      "Train loss at epoch (1428):  0.062637724\n",
      "Test loss at epoch (1428):  0.033917055\n",
      "Train loss at epoch (1429):  0.06291242\n",
      "Test loss at epoch (1429):  0.033902876\n",
      "Train loss at epoch (1430):  0.06293533\n",
      "Test loss at epoch (1430):  0.03390871\n",
      "Train loss at epoch (1431):  0.06321905\n",
      "Test loss at epoch (1431):  0.033920735\n",
      "Train loss at epoch (1432):  0.06331324\n",
      "Test loss at epoch (1432):  0.033926256\n",
      "Train loss at epoch (1433):  0.062943526\n",
      "Test loss at epoch (1433):  0.033922803\n",
      "Train loss at epoch (1434):  0.06287864\n",
      "Test loss at epoch (1434):  0.033934522\n",
      "Train loss at epoch (1435):  0.06300384\n",
      "Test loss at epoch (1435):  0.033911027\n",
      "Train loss at epoch (1436):  0.06298249\n",
      "Test loss at epoch (1436):  0.033909548\n",
      "Train loss at epoch (1437):  0.06283623\n",
      "Test loss at epoch (1437):  0.033919085\n",
      "Train loss at epoch (1438):  0.06275381\n",
      "Test loss at epoch (1438):  0.033906076\n",
      "Train loss at epoch (1439):  0.06295023\n",
      "Test loss at epoch (1439):  0.033917848\n",
      "Train loss at epoch (1440):  0.062937066\n",
      "Test loss at epoch (1440):  0.0339198\n",
      "Train loss at epoch (1441):  0.062969625\n",
      "Test loss at epoch (1441):  0.033907693\n",
      "Train loss at epoch (1442):  0.063233614\n",
      "Test loss at epoch (1442):  0.03392316\n",
      "Train loss at epoch (1443):  0.06290078\n",
      "Test loss at epoch (1443):  0.03391064\n",
      "Train loss at epoch (1444):  0.06268933\n",
      "Test loss at epoch (1444):  0.033910953\n",
      "Train loss at epoch (1445):  0.06295966\n",
      "Test loss at epoch (1445):  0.033925265\n",
      "Train loss at epoch (1446):  0.06328738\n",
      "Test loss at epoch (1446):  0.033930764\n",
      "Train loss at epoch (1447):  0.06303382\n",
      "Test loss at epoch (1447):  0.033920042\n",
      "Train loss at epoch (1448):  0.062926315\n",
      "Test loss at epoch (1448):  0.033920396\n",
      "Train loss at epoch (1449):  0.063174605\n",
      "Test loss at epoch (1449):  0.033914566\n",
      "Train loss at epoch (1450):  0.062970884\n",
      "Test loss at epoch (1450):  0.03392902\n",
      "Train loss at epoch (1451):  0.063037395\n",
      "Test loss at epoch (1451):  0.033912867\n",
      "Train loss at epoch (1452):  0.06311723\n",
      "Test loss at epoch (1452):  0.033920895\n",
      "Train loss at epoch (1453):  0.06287235\n",
      "Test loss at epoch (1453):  0.033936422\n",
      "Train loss at epoch (1454):  0.06289341\n",
      "Test loss at epoch (1454):  0.033902686\n",
      "Train loss at epoch (1455):  0.062629215\n",
      "Test loss at epoch (1455):  0.033922438\n",
      "Train loss at epoch (1456):  0.06329087\n",
      "Test loss at epoch (1456):  0.03392331\n",
      "Train loss at epoch (1457):  0.06282585\n",
      "Test loss at epoch (1457):  0.033925764\n",
      "Train loss at epoch (1458):  0.06273664\n",
      "Test loss at epoch (1458):  0.033922024\n",
      "Train loss at epoch (1459):  0.063014284\n",
      "Test loss at epoch (1459):  0.033908997\n",
      "Train loss at epoch (1460):  0.06316816\n",
      "Test loss at epoch (1460):  0.033930887\n",
      "Train loss at epoch (1461):  0.062570214\n",
      "Test loss at epoch (1461):  0.033905752\n",
      "Train loss at epoch (1462):  0.06332919\n",
      "Test loss at epoch (1462):  0.03392127\n",
      "Train loss at epoch (1463):  0.06309833\n",
      "Test loss at epoch (1463):  0.033922024\n",
      "Train loss at epoch (1464):  0.062868744\n",
      "Test loss at epoch (1464):  0.033925522\n",
      "Train loss at epoch (1465):  0.06288739\n",
      "Test loss at epoch (1465):  0.033913355\n",
      "Train loss at epoch (1466):  0.06333885\n",
      "Test loss at epoch (1466):  0.03391626\n",
      "Train loss at epoch (1467):  0.06321059\n",
      "Test loss at epoch (1467):  0.033904582\n",
      "Train loss at epoch (1468):  0.06336165\n",
      "Test loss at epoch (1468):  0.033913188\n",
      "Train loss at epoch (1469):  0.06325958\n",
      "Test loss at epoch (1469):  0.03391557\n",
      "Train loss at epoch (1470):  0.063144214\n",
      "Test loss at epoch (1470):  0.033920612\n",
      "Train loss at epoch (1471):  0.06321093\n",
      "Test loss at epoch (1471):  0.033934828\n",
      "Train loss at epoch (1472):  0.063369036\n",
      "Test loss at epoch (1472):  0.033922624\n",
      "Train loss at epoch (1473):  0.06285393\n",
      "Test loss at epoch (1473):  0.033930033\n",
      "Train loss at epoch (1474):  0.06300889\n",
      "Test loss at epoch (1474):  0.03391012\n",
      "Train loss at epoch (1475):  0.062763855\n",
      "Test loss at epoch (1475):  0.03392497\n",
      "Train loss at epoch (1476):  0.06280702\n",
      "Test loss at epoch (1476):  0.033925414\n",
      "Train loss at epoch (1477):  0.06305953\n",
      "Test loss at epoch (1477):  0.033925954\n",
      "Train loss at epoch (1478):  0.063111596\n",
      "Test loss at epoch (1478):  0.03391441\n",
      "Train loss at epoch (1479):  0.06306586\n",
      "Test loss at epoch (1479):  0.033924293\n",
      "Train loss at epoch (1480):  0.063290246\n",
      "Test loss at epoch (1480):  0.033929046\n",
      "Train loss at epoch (1481):  0.06273521\n",
      "Test loss at epoch (1481):  0.033912946\n",
      "Train loss at epoch (1482):  0.06321807\n",
      "Test loss at epoch (1482):  0.033930965\n",
      "Train loss at epoch (1483):  0.06294716\n",
      "Test loss at epoch (1483):  0.03391911\n",
      "Train loss at epoch (1484):  0.06303681\n",
      "Test loss at epoch (1484):  0.03391326\n",
      "Train loss at epoch (1485):  0.063134156\n",
      "Test loss at epoch (1485):  0.033908807\n",
      "Train loss at epoch (1486):  0.06293533\n",
      "Test loss at epoch (1486):  0.033921886\n",
      "Train loss at epoch (1487):  0.063045494\n",
      "Test loss at epoch (1487):  0.033924047\n",
      "Train loss at epoch (1488):  0.0629561\n",
      "Test loss at epoch (1488):  0.033923775\n",
      "Train loss at epoch (1489):  0.063336246\n",
      "Test loss at epoch (1489):  0.033917483\n",
      "Train loss at epoch (1490):  0.06300599\n",
      "Test loss at epoch (1490):  0.03393804\n",
      "Train loss at epoch (1491):  0.06283879\n",
      "Test loss at epoch (1491):  0.03390935\n",
      "Train loss at epoch (1492):  0.063116096\n",
      "Test loss at epoch (1492):  0.033916038\n",
      "Train loss at epoch (1493):  0.06294981\n",
      "Test loss at epoch (1493):  0.03392616\n",
      "Train loss at epoch (1494):  0.06299495\n",
      "Test loss at epoch (1494):  0.03392738\n",
      "Train loss at epoch (1495):  0.06291013\n",
      "Test loss at epoch (1495):  0.033914987\n",
      "Train loss at epoch (1496):  0.06292645\n",
      "Test loss at epoch (1496):  0.03391758\n",
      "Train loss at epoch (1497):  0.062857054\n",
      "Test loss at epoch (1497):  0.033907093\n",
      "Train loss at epoch (1498):  0.06307122\n",
      "Test loss at epoch (1498):  0.03391527\n",
      "Train loss at epoch (1499):  0.06279206\n",
      "Test loss at epoch (1499):  0.03389862\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "EPOCHS = 1500\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "cached_decoder = CachedTransformerDecoder(cfg)\n",
    "optimizer = SGD(cached_decoder.parameters(), lr=1e-4, momentum=0.9)\n",
    "lr_scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "trainer = Trainer(\n",
    "    train_data_loader=train_data_loader,\n",
    "    test_data_loader=test_data_loader,\n",
    "    # optimizer=AdamW(decoder.parameters(), lr=1e-5),\n",
    "    optimizer=optimizer,\n",
    "    model=cached_decoder,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    epochs=EPOCHS\n",
    ")\n",
    "\n",
    "trainer.train(loss_fn=nn.L1Loss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, n_tokens, inputs):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_len = inputs.size(-2)\n",
    "        for i in range(n_tokens):\n",
    "            inputs = torch.cat((inputs, model.predict(inputs)), dim=-2)\n",
    "        return inputs[:, input_len:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_sample, y_sample = test_dataset[np.random.randint(0, TEST_SIZE)]\n",
    "predictions = cached_decoder.predict(x_sample.unsqueeze(0)[:, :SOURCE_SEQ_LEN, :], TARGET_SEQ_LEN)\n",
    "labels = torch.cat((x_sample, y_sample[-1:]))\n",
    "\n",
    "plot_circle(\n",
    "    input=labels.detach().numpy()[0:],\n",
    "    prediction=torch.cat((labels[0:SOURCE_SEQ_LEN, :], predictions[0])).detach().numpy()[0:],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'decoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m x_sample, y_sample \u001b[38;5;241m=\u001b[39m test_dataset[np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, TEST_SIZE)]\n\u001b[0;32m----> 2\u001b[0m predictions \u001b[38;5;241m=\u001b[39m predict(\u001b[43mdecoder\u001b[49m, TARGET_SEQ_LEN, x_sample\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)[:, :SOURCE_SEQ_LEN, :])\n\u001b[1;32m      3\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((x_sample, y_sample[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:]))\n\u001b[1;32m      5\u001b[0m plot_circle(\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mlabels\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m:],\n\u001b[1;32m      7\u001b[0m     prediction\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcat((labels[\u001b[38;5;241m0\u001b[39m:SOURCE_SEQ_LEN, :], predictions[\u001b[38;5;241m0\u001b[39m]))\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m:],\n\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'decoder' is not defined"
     ]
    }
   ],
   "source": [
    "x_sample, y_sample = test_dataset[np.random.randint(0, TEST_SIZE)]\n",
    "predictions = predict(decoder, TARGET_SEQ_LEN, x_sample.unsqueeze(0)[:, :SOURCE_SEQ_LEN, :])\n",
    "labels = torch.cat((x_sample, y_sample[-1:]))\n",
    "\n",
    "plot_circle(\n",
    "    input=labels.detach().numpy()[0:],\n",
    "    prediction=torch.cat((labels[0:SOURCE_SEQ_LEN, :], predictions[0])).detach().numpy()[0:],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached:  0.003320588705362752 0.0007953278805429409\n",
      "Cacheless:  0.0041436104653403165 0.0007108229989071066\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "cached_time = []\n",
    "with torch.no_grad():\n",
    "    for i in range(1000):\n",
    "        s = time.perf_counter()\n",
    "        x_sample, y_sample = test_dataset[np.random.randint(0, TEST_SIZE)]\n",
    "        predictions = cached_decoder.predict(x_sample.unsqueeze(0)[:, :SOURCE_SEQ_LEN, :], TARGET_SEQ_LEN)\n",
    "        cached_time.append(time.perf_counter() - s)\n",
    "\n",
    "cacheless_time = []\n",
    "for i in range(1000):\n",
    "    s = time.perf_counter()\n",
    "    predictions = predict(decoder, TARGET_SEQ_LEN, x_sample.unsqueeze(0)[:, :SOURCE_SEQ_LEN, :])\n",
    "    labels = torch.cat((x_sample, y_sample[-1:]))\n",
    "    cacheless_time.append(time.perf_counter() - s)\n",
    "\n",
    "print(\"Cached: \", np.array(cached_time).mean(), np.array(cached_time).std())\n",
    "print(\"Cacheless: \", np.array(cacheless_time).mean(), np.array(cacheless_time).std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
