{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW, Optimizer, SGD\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from scratchers.vanilla_transformer.attn import Attention\n",
    "from scratchers.transformer_config import TransformerConfig\n",
    "\n",
    "from scratchers.encoder_decoder_transformer import Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_SEQ_LEN = 12\n",
    "TARGET_SEQ_LEN = 12\n",
    "\n",
    "DATA_SIZE = 2000\n",
    "TRAIN_SIZE = int(DATA_SIZE * 0.8)\n",
    "TEST_SIZE = DATA_SIZE - TRAIN_SIZE\n",
    "\n",
    "# radii = np.random.ranf((DATA_SIZE)) # * 9 + 1\n",
    "radii = np.ones((DATA_SIZE))\n",
    "starting_radian = np.random.ranf((DATA_SIZE)) * 2 * np.pi\n",
    "directions = np.random.randint(2, size=DATA_SIZE) * 2 - 1\n",
    "\n",
    "def to_cartesian(\n",
    "    radius: float,\n",
    "    start_radian: float, \n",
    "    direction: int\n",
    "):\n",
    "    delta = 2 * np.pi / (SOURCE_SEQ_LEN + TARGET_SEQ_LEN)\n",
    "    seq = np.array([\n",
    "        np.array([\n",
    "            radius * np.cos(start_radian + (i * direction * delta)),\n",
    "            radius * np.sin(start_radian + (i * direction * delta))\n",
    "        ])\n",
    "        for i in range(SOURCE_SEQ_LEN + TARGET_SEQ_LEN)\n",
    "    ])\n",
    "    source_seq = seq[:SOURCE_SEQ_LEN]\n",
    "    target_seq = seq[SOURCE_SEQ_LEN:]\n",
    "    return source_seq, target_seq\n",
    "\n",
    "def make_circles_data():\n",
    "    X = np.empty((DATA_SIZE, (SOURCE_SEQ_LEN), 2))\n",
    "    Y = np.empty((DATA_SIZE, (SOURCE_SEQ_LEN), 2))\n",
    "    for idx, (radius, start_radian, direction) in enumerate(zip(radii, starting_radian, directions)):\n",
    "        x, y = to_cartesian(radius, start_radian, direction)\n",
    "        X[idx, :, :] = x\n",
    "        Y[idx, :, :] = y\n",
    "\n",
    "    return (\n",
    "        torch.from_numpy(X[:TRAIN_SIZE]).float(), \n",
    "        torch.from_numpy(Y[:TRAIN_SIZE]).float(),\n",
    "        torch.from_numpy(X[TRAIN_SIZE:]).float(), \n",
    "        torch.from_numpy(Y[TRAIN_SIZE:]).float()\n",
    "    )\n",
    "\n",
    "X_train, y_train, X_test, y_test = make_circles_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_circle(input, prediction, figsize=(6, 6)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    colors = cm.rainbow(np.linspace(0, 1, len(input)))\n",
    "    plt.scatter(\n",
    "        input[:, 0], input[:, 1], label=\"input\", marker=\"*\", s=250, alpha=0.5, color=colors\n",
    "    )\n",
    "    plt.scatter(prediction[:, 0], prediction[:, 1], label=\"prediction\", color=colors)\n",
    "    plt.legend()\n",
    "\n",
    "plot_circle(X_train[4], y_train[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = TransformerConfig(\n",
    "    input_size=2,\n",
    "    attn_d_k=64,\n",
    "    transformer_proj_dim=128,\n",
    "    dropout=0.2,\n",
    "    nlayers=3,\n",
    "    is_self_attn=False,\n",
    "    max_seq_len=TARGET_SEQ_LEN + SOURCE_SEQ_LEN,\n",
    "    nheads=2,\n",
    "    pre_layer_norm=True,\n",
    "    use_cache=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_data_loader: DataLoader,\n",
    "        test_data_loader: DataLoader,\n",
    "        optimizer: Optimizer,\n",
    "        model: nn.Module,\n",
    "        lr_scheduler = None,\n",
    "        epochs: int = 10,\n",
    "    ):\n",
    "        self.train_data_loader = train_data_loader\n",
    "        self.test_data_loader = test_data_loader\n",
    "        self.optimizer = optimizer\n",
    "        self.model = model\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.epochs = epochs\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "      return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    def train(self, loss_fn: callable):\n",
    "        self.model.to(self.device)\n",
    "        for epoch in range(self.epochs):\n",
    "            losses = []\n",
    "            self.model.train()\n",
    "            for x_batch, y_batch in self.train_data_loader:\n",
    "                out = self.model(x_batch, torch.cat((x_batch[:, -1:, :], y_batch[:, :-1, :]), dim=-2))\n",
    "                loss = loss_fn(out, y_batch)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                losses.append(loss.detach().cpu().numpy())\n",
    "\n",
    "            if self.lr_scheduler is not None:\n",
    "                self.lr_scheduler.step()\n",
    "\n",
    "            print(f\"Train loss at epoch ({epoch}): \", np.array(losses).mean())\n",
    "\n",
    "            with torch.no_grad():\n",
    "                self.model.eval()\n",
    "                test_losses = []\n",
    "                for x_batch, y_batch in self.test_data_loader:\n",
    "                    out = self.model(x_batch, torch.cat((x_batch[:, -1:, :], y_batch[:, :-1, :]), dim=-2))\n",
    "                    loss = loss_fn(out, y_batch)\n",
    "                    test_losses.append(loss.detach().cpu().numpy())\n",
    "\n",
    "                print(f\"Test loss at epoch ({epoch}): \", np.array(test_losses).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss at epoch (0):  1.2317655\n",
      "Test loss at epoch (0):  0.9702111\n",
      "Train loss at epoch (1):  0.79569405\n",
      "Test loss at epoch (1):  0.64623326\n",
      "Train loss at epoch (2):  0.6412567\n",
      "Test loss at epoch (2):  0.6002227\n",
      "Train loss at epoch (3):  0.59938544\n",
      "Test loss at epoch (3):  0.5557897\n",
      "Train loss at epoch (4):  0.551968\n",
      "Test loss at epoch (4):  0.49557793\n",
      "Train loss at epoch (5):  0.51059216\n",
      "Test loss at epoch (5):  0.45352432\n",
      "Train loss at epoch (6):  0.48009655\n",
      "Test loss at epoch (6):  0.42491102\n",
      "Train loss at epoch (7):  0.4486423\n",
      "Test loss at epoch (7):  0.399666\n",
      "Train loss at epoch (8):  0.42541182\n",
      "Test loss at epoch (8):  0.36939925\n",
      "Train loss at epoch (9):  0.40783888\n",
      "Test loss at epoch (9):  0.35187095\n",
      "Train loss at epoch (10):  0.3924217\n",
      "Test loss at epoch (10):  0.3436376\n",
      "Train loss at epoch (11):  0.38283885\n",
      "Test loss at epoch (11):  0.32793137\n",
      "Train loss at epoch (12):  0.37004268\n",
      "Test loss at epoch (12):  0.3119199\n",
      "Train loss at epoch (13):  0.35977152\n",
      "Test loss at epoch (13):  0.30885804\n",
      "Train loss at epoch (14):  0.35176185\n",
      "Test loss at epoch (14):  0.30141383\n",
      "Train loss at epoch (15):  0.34255615\n",
      "Test loss at epoch (15):  0.2910658\n",
      "Train loss at epoch (16):  0.3384845\n",
      "Test loss at epoch (16):  0.28288138\n",
      "Train loss at epoch (17):  0.32980597\n",
      "Test loss at epoch (17):  0.2734226\n",
      "Train loss at epoch (18):  0.32532072\n",
      "Test loss at epoch (18):  0.26595375\n",
      "Train loss at epoch (19):  0.31948116\n",
      "Test loss at epoch (19):  0.26037335\n",
      "Train loss at epoch (20):  0.31471494\n",
      "Test loss at epoch (20):  0.2600705\n",
      "Train loss at epoch (21):  0.30929306\n",
      "Test loss at epoch (21):  0.254484\n",
      "Train loss at epoch (22):  0.30250576\n",
      "Test loss at epoch (22):  0.25132567\n",
      "Train loss at epoch (23):  0.30067718\n",
      "Test loss at epoch (23):  0.24050397\n",
      "Train loss at epoch (24):  0.29431322\n",
      "Test loss at epoch (24):  0.24048823\n",
      "Train loss at epoch (25):  0.2894332\n",
      "Test loss at epoch (25):  0.23416084\n",
      "Train loss at epoch (26):  0.28667945\n",
      "Test loss at epoch (26):  0.23059079\n",
      "Train loss at epoch (27):  0.28190035\n",
      "Test loss at epoch (27):  0.2220928\n",
      "Train loss at epoch (28):  0.2779227\n",
      "Test loss at epoch (28):  0.21966957\n",
      "Train loss at epoch (29):  0.2748458\n",
      "Test loss at epoch (29):  0.22065029\n",
      "Train loss at epoch (30):  0.26994273\n",
      "Test loss at epoch (30):  0.2137573\n",
      "Train loss at epoch (31):  0.26789185\n",
      "Test loss at epoch (31):  0.21164446\n",
      "Train loss at epoch (32):  0.26158863\n",
      "Test loss at epoch (32):  0.20356011\n",
      "Train loss at epoch (33):  0.25863525\n",
      "Test loss at epoch (33):  0.20298804\n",
      "Train loss at epoch (34):  0.25745502\n",
      "Test loss at epoch (34):  0.2006089\n",
      "Train loss at epoch (35):  0.25380817\n",
      "Test loss at epoch (35):  0.19340871\n",
      "Train loss at epoch (36):  0.25039464\n",
      "Test loss at epoch (36):  0.19256546\n",
      "Train loss at epoch (37):  0.24833737\n",
      "Test loss at epoch (37):  0.18911952\n",
      "Train loss at epoch (38):  0.24481781\n",
      "Test loss at epoch (38):  0.1872494\n",
      "Train loss at epoch (39):  0.24216025\n",
      "Test loss at epoch (39):  0.18302828\n",
      "Train loss at epoch (40):  0.23971486\n",
      "Test loss at epoch (40):  0.18171293\n",
      "Train loss at epoch (41):  0.2375447\n",
      "Test loss at epoch (41):  0.17980532\n",
      "Train loss at epoch (42):  0.23750183\n",
      "Test loss at epoch (42):  0.17725629\n",
      "Train loss at epoch (43):  0.23365125\n",
      "Test loss at epoch (43):  0.17229135\n",
      "Train loss at epoch (44):  0.23270774\n",
      "Test loss at epoch (44):  0.1712898\n",
      "Train loss at epoch (45):  0.23035707\n",
      "Test loss at epoch (45):  0.17013343\n",
      "Train loss at epoch (46):  0.22666015\n",
      "Test loss at epoch (46):  0.16952422\n",
      "Train loss at epoch (47):  0.22504666\n",
      "Test loss at epoch (47):  0.167904\n",
      "Train loss at epoch (48):  0.22189468\n",
      "Test loss at epoch (48):  0.1641297\n",
      "Train loss at epoch (49):  0.22420838\n",
      "Test loss at epoch (49):  0.16304998\n",
      "Train loss at epoch (50):  0.22145261\n",
      "Test loss at epoch (50):  0.1610782\n",
      "Train loss at epoch (51):  0.22008243\n",
      "Test loss at epoch (51):  0.16014372\n",
      "Train loss at epoch (52):  0.2183643\n",
      "Test loss at epoch (52):  0.15754774\n",
      "Train loss at epoch (53):  0.21642473\n",
      "Test loss at epoch (53):  0.15466888\n",
      "Train loss at epoch (54):  0.21483514\n",
      "Test loss at epoch (54):  0.1546568\n",
      "Train loss at epoch (55):  0.21299244\n",
      "Test loss at epoch (55):  0.15306813\n",
      "Train loss at epoch (56):  0.21179049\n",
      "Test loss at epoch (56):  0.15082687\n",
      "Train loss at epoch (57):  0.21014021\n",
      "Test loss at epoch (57):  0.1516313\n",
      "Train loss at epoch (58):  0.20847538\n",
      "Test loss at epoch (58):  0.14900585\n",
      "Train loss at epoch (59):  0.20720945\n",
      "Test loss at epoch (59):  0.14812799\n",
      "Train loss at epoch (60):  0.20528156\n",
      "Test loss at epoch (60):  0.1449174\n",
      "Train loss at epoch (61):  0.20512141\n",
      "Test loss at epoch (61):  0.14622134\n",
      "Train loss at epoch (62):  0.20402373\n",
      "Test loss at epoch (62):  0.14297682\n",
      "Train loss at epoch (63):  0.20217572\n",
      "Test loss at epoch (63):  0.14564343\n",
      "Train loss at epoch (64):  0.2013325\n",
      "Test loss at epoch (64):  0.14215198\n",
      "Train loss at epoch (65):  0.20178366\n",
      "Test loss at epoch (65):  0.14021553\n",
      "Train loss at epoch (66):  0.19882753\n",
      "Test loss at epoch (66):  0.14377838\n",
      "Train loss at epoch (67):  0.19768475\n",
      "Test loss at epoch (67):  0.14110944\n",
      "Train loss at epoch (68):  0.19626369\n",
      "Test loss at epoch (68):  0.13760105\n",
      "Train loss at epoch (69):  0.19636211\n",
      "Test loss at epoch (69):  0.1390543\n",
      "Train loss at epoch (70):  0.19531585\n",
      "Test loss at epoch (70):  0.13727903\n",
      "Train loss at epoch (71):  0.19418305\n",
      "Test loss at epoch (71):  0.13752824\n",
      "Train loss at epoch (72):  0.19341388\n",
      "Test loss at epoch (72):  0.13590606\n",
      "Train loss at epoch (73):  0.19101518\n",
      "Test loss at epoch (73):  0.13496616\n",
      "Train loss at epoch (74):  0.19066724\n",
      "Test loss at epoch (74):  0.13459623\n",
      "Train loss at epoch (75):  0.19043906\n",
      "Test loss at epoch (75):  0.13496695\n",
      "Train loss at epoch (76):  0.18856867\n",
      "Test loss at epoch (76):  0.13303557\n",
      "Train loss at epoch (77):  0.18874398\n",
      "Test loss at epoch (77):  0.13328815\n",
      "Train loss at epoch (78):  0.18741839\n",
      "Test loss at epoch (78):  0.12936701\n",
      "Train loss at epoch (79):  0.18705231\n",
      "Test loss at epoch (79):  0.13012075\n",
      "Train loss at epoch (80):  0.1856973\n",
      "Test loss at epoch (80):  0.1311476\n",
      "Train loss at epoch (81):  0.1836881\n",
      "Test loss at epoch (81):  0.12984341\n",
      "Train loss at epoch (82):  0.18508206\n",
      "Test loss at epoch (82):  0.12997293\n",
      "Train loss at epoch (83):  0.18277301\n",
      "Test loss at epoch (83):  0.12545504\n",
      "Train loss at epoch (84):  0.18199228\n",
      "Test loss at epoch (84):  0.12682843\n",
      "Train loss at epoch (85):  0.18090422\n",
      "Test loss at epoch (85):  0.12588671\n",
      "Train loss at epoch (86):  0.18041316\n",
      "Test loss at epoch (86):  0.12632214\n",
      "Train loss at epoch (87):  0.1800078\n",
      "Test loss at epoch (87):  0.12432269\n",
      "Train loss at epoch (88):  0.1797847\n",
      "Test loss at epoch (88):  0.124235176\n",
      "Train loss at epoch (89):  0.17914917\n",
      "Test loss at epoch (89):  0.1246517\n",
      "Train loss at epoch (90):  0.17844845\n",
      "Test loss at epoch (90):  0.12418117\n",
      "Train loss at epoch (91):  0.17711443\n",
      "Test loss at epoch (91):  0.1220193\n",
      "Train loss at epoch (92):  0.17581281\n",
      "Test loss at epoch (92):  0.12081152\n",
      "Train loss at epoch (93):  0.17734137\n",
      "Test loss at epoch (93):  0.118222445\n",
      "Train loss at epoch (94):  0.17555915\n",
      "Test loss at epoch (94):  0.11897919\n",
      "Train loss at epoch (95):  0.17439772\n",
      "Test loss at epoch (95):  0.120321766\n",
      "Train loss at epoch (96):  0.17424065\n",
      "Test loss at epoch (96):  0.11872627\n",
      "Train loss at epoch (97):  0.17355472\n",
      "Test loss at epoch (97):  0.119370505\n",
      "Train loss at epoch (98):  0.17320386\n",
      "Test loss at epoch (98):  0.1184141\n",
      "Train loss at epoch (99):  0.17185427\n",
      "Test loss at epoch (99):  0.11906991\n",
      "Train loss at epoch (100):  0.17174318\n",
      "Test loss at epoch (100):  0.11693927\n",
      "Train loss at epoch (101):  0.17135723\n",
      "Test loss at epoch (101):  0.11567549\n",
      "Train loss at epoch (102):  0.16925427\n",
      "Test loss at epoch (102):  0.11793328\n",
      "Train loss at epoch (103):  0.16935436\n",
      "Test loss at epoch (103):  0.11994967\n",
      "Train loss at epoch (104):  0.16864793\n",
      "Test loss at epoch (104):  0.11518851\n",
      "Train loss at epoch (105):  0.1687692\n",
      "Test loss at epoch (105):  0.11524042\n",
      "Train loss at epoch (106):  0.16661151\n",
      "Test loss at epoch (106):  0.114973776\n",
      "Train loss at epoch (107):  0.1666145\n",
      "Test loss at epoch (107):  0.11522176\n",
      "Train loss at epoch (108):  0.16545448\n",
      "Test loss at epoch (108):  0.11255763\n",
      "Train loss at epoch (109):  0.16520537\n",
      "Test loss at epoch (109):  0.11321138\n",
      "Train loss at epoch (110):  0.16338362\n",
      "Test loss at epoch (110):  0.111562565\n",
      "Train loss at epoch (111):  0.16524845\n",
      "Test loss at epoch (111):  0.11347672\n",
      "Train loss at epoch (112):  0.16376486\n",
      "Test loss at epoch (112):  0.11217076\n",
      "Train loss at epoch (113):  0.16403443\n",
      "Test loss at epoch (113):  0.11119689\n",
      "Train loss at epoch (114):  0.16318265\n",
      "Test loss at epoch (114):  0.10968754\n",
      "Train loss at epoch (115):  0.16267188\n",
      "Test loss at epoch (115):  0.11068213\n",
      "Train loss at epoch (116):  0.16253321\n",
      "Test loss at epoch (116):  0.11082986\n",
      "Train loss at epoch (117):  0.16162054\n",
      "Test loss at epoch (117):  0.11194936\n",
      "Train loss at epoch (118):  0.16168451\n",
      "Test loss at epoch (118):  0.110729426\n",
      "Train loss at epoch (119):  0.16014025\n",
      "Test loss at epoch (119):  0.111016385\n",
      "Train loss at epoch (120):  0.15989307\n",
      "Test loss at epoch (120):  0.10880929\n",
      "Train loss at epoch (121):  0.15952447\n",
      "Test loss at epoch (121):  0.10917502\n",
      "Train loss at epoch (122):  0.15883128\n",
      "Test loss at epoch (122):  0.108960815\n",
      "Train loss at epoch (123):  0.15851758\n",
      "Test loss at epoch (123):  0.10911429\n",
      "Train loss at epoch (124):  0.15717694\n",
      "Test loss at epoch (124):  0.106194265\n",
      "Train loss at epoch (125):  0.15796953\n",
      "Test loss at epoch (125):  0.107542604\n",
      "Train loss at epoch (126):  0.15717\n",
      "Test loss at epoch (126):  0.106898874\n",
      "Train loss at epoch (127):  0.15642682\n",
      "Test loss at epoch (127):  0.10702408\n",
      "Train loss at epoch (128):  0.15528145\n",
      "Test loss at epoch (128):  0.10454826\n",
      "Train loss at epoch (129):  0.15570582\n",
      "Test loss at epoch (129):  0.105688654\n",
      "Train loss at epoch (130):  0.15475999\n",
      "Test loss at epoch (130):  0.10433856\n",
      "Train loss at epoch (131):  0.15376513\n",
      "Test loss at epoch (131):  0.104428485\n",
      "Train loss at epoch (132):  0.15458407\n",
      "Test loss at epoch (132):  0.106140256\n",
      "Train loss at epoch (133):  0.15361053\n",
      "Test loss at epoch (133):  0.104209974\n",
      "Train loss at epoch (134):  0.15143093\n",
      "Test loss at epoch (134):  0.10484317\n",
      "Train loss at epoch (135):  0.15152688\n",
      "Test loss at epoch (135):  0.10363155\n",
      "Train loss at epoch (136):  0.15233183\n",
      "Test loss at epoch (136):  0.104657836\n",
      "Train loss at epoch (137):  0.15093586\n",
      "Test loss at epoch (137):  0.101812825\n",
      "Train loss at epoch (138):  0.15218179\n",
      "Test loss at epoch (138):  0.10223035\n",
      "Train loss at epoch (139):  0.1512057\n",
      "Test loss at epoch (139):  0.101823166\n",
      "Train loss at epoch (140):  0.15107946\n",
      "Test loss at epoch (140):  0.102668345\n",
      "Train loss at epoch (141):  0.15009616\n",
      "Test loss at epoch (141):  0.10127898\n",
      "Train loss at epoch (142):  0.15062484\n",
      "Test loss at epoch (142):  0.10161581\n",
      "Train loss at epoch (143):  0.14959821\n",
      "Test loss at epoch (143):  0.09895739\n",
      "Train loss at epoch (144):  0.14895935\n",
      "Test loss at epoch (144):  0.09951956\n",
      "Train loss at epoch (145):  0.14805011\n",
      "Test loss at epoch (145):  0.1012771\n",
      "Train loss at epoch (146):  0.1473721\n",
      "Test loss at epoch (146):  0.09920484\n",
      "Train loss at epoch (147):  0.1473303\n",
      "Test loss at epoch (147):  0.09870751\n",
      "Train loss at epoch (148):  0.14688209\n",
      "Test loss at epoch (148):  0.10036711\n",
      "Train loss at epoch (149):  0.14606282\n",
      "Test loss at epoch (149):  0.099352\n",
      "Train loss at epoch (150):  0.14549354\n",
      "Test loss at epoch (150):  0.09731729\n",
      "Train loss at epoch (151):  0.14522608\n",
      "Test loss at epoch (151):  0.0981927\n",
      "Train loss at epoch (152):  0.1459916\n",
      "Test loss at epoch (152):  0.09865142\n",
      "Train loss at epoch (153):  0.1451878\n",
      "Test loss at epoch (153):  0.09746647\n",
      "Train loss at epoch (154):  0.14568163\n",
      "Test loss at epoch (154):  0.09714264\n",
      "Train loss at epoch (155):  0.14549854\n",
      "Test loss at epoch (155):  0.095967464\n",
      "Train loss at epoch (156):  0.14450657\n",
      "Test loss at epoch (156):  0.09486207\n",
      "Train loss at epoch (157):  0.14301372\n",
      "Test loss at epoch (157):  0.09601319\n",
      "Train loss at epoch (158):  0.1427583\n",
      "Test loss at epoch (158):  0.097498894\n",
      "Train loss at epoch (159):  0.14349028\n",
      "Test loss at epoch (159):  0.09660696\n",
      "Train loss at epoch (160):  0.14216115\n",
      "Test loss at epoch (160):  0.09468486\n",
      "Train loss at epoch (161):  0.14207166\n",
      "Test loss at epoch (161):  0.0974336\n",
      "Train loss at epoch (162):  0.14249955\n",
      "Test loss at epoch (162):  0.09488694\n",
      "Train loss at epoch (163):  0.1411191\n",
      "Test loss at epoch (163):  0.09430022\n",
      "Train loss at epoch (164):  0.14042374\n",
      "Test loss at epoch (164):  0.09360516\n",
      "Train loss at epoch (165):  0.14046478\n",
      "Test loss at epoch (165):  0.094039194\n",
      "Train loss at epoch (166):  0.14102934\n",
      "Test loss at epoch (166):  0.09340132\n",
      "Train loss at epoch (167):  0.13991264\n",
      "Test loss at epoch (167):  0.092532605\n",
      "Train loss at epoch (168):  0.13941629\n",
      "Test loss at epoch (168):  0.09327988\n",
      "Train loss at epoch (169):  0.14005534\n",
      "Test loss at epoch (169):  0.092848115\n",
      "Train loss at epoch (170):  0.13898084\n",
      "Test loss at epoch (170):  0.09314012\n",
      "Train loss at epoch (171):  0.13945088\n",
      "Test loss at epoch (171):  0.09481286\n",
      "Train loss at epoch (172):  0.13753526\n",
      "Test loss at epoch (172):  0.091165975\n",
      "Train loss at epoch (173):  0.13876027\n",
      "Test loss at epoch (173):  0.09318068\n",
      "Train loss at epoch (174):  0.13838813\n",
      "Test loss at epoch (174):  0.09170125\n",
      "Train loss at epoch (175):  0.13750044\n",
      "Test loss at epoch (175):  0.0927302\n",
      "Train loss at epoch (176):  0.13777576\n",
      "Test loss at epoch (176):  0.09155721\n",
      "Train loss at epoch (177):  0.13651036\n",
      "Test loss at epoch (177):  0.08979599\n",
      "Train loss at epoch (178):  0.13734137\n",
      "Test loss at epoch (178):  0.09081298\n",
      "Train loss at epoch (179):  0.13724214\n",
      "Test loss at epoch (179):  0.09149606\n",
      "Train loss at epoch (180):  0.13685614\n",
      "Test loss at epoch (180):  0.091024466\n",
      "Train loss at epoch (181):  0.13605163\n",
      "Test loss at epoch (181):  0.09022156\n",
      "Train loss at epoch (182):  0.13560544\n",
      "Test loss at epoch (182):  0.09041484\n",
      "Train loss at epoch (183):  0.1352826\n",
      "Test loss at epoch (183):  0.09042057\n",
      "Train loss at epoch (184):  0.13523279\n",
      "Test loss at epoch (184):  0.08996941\n",
      "Train loss at epoch (185):  0.13492571\n",
      "Test loss at epoch (185):  0.08996695\n",
      "Train loss at epoch (186):  0.134202\n",
      "Test loss at epoch (186):  0.088253796\n",
      "Train loss at epoch (187):  0.13393816\n",
      "Test loss at epoch (187):  0.08861331\n",
      "Train loss at epoch (188):  0.13391133\n",
      "Test loss at epoch (188):  0.08885253\n",
      "Train loss at epoch (189):  0.13331613\n",
      "Test loss at epoch (189):  0.08807093\n",
      "Train loss at epoch (190):  0.13399623\n",
      "Test loss at epoch (190):  0.08712164\n",
      "Train loss at epoch (191):  0.133473\n",
      "Test loss at epoch (191):  0.08904475\n",
      "Train loss at epoch (192):  0.13204499\n",
      "Test loss at epoch (192):  0.08561178\n",
      "Train loss at epoch (193):  0.13235894\n",
      "Test loss at epoch (193):  0.08790096\n",
      "Train loss at epoch (194):  0.13187064\n",
      "Test loss at epoch (194):  0.08608861\n",
      "Train loss at epoch (195):  0.1318896\n",
      "Test loss at epoch (195):  0.0866844\n",
      "Train loss at epoch (196):  0.13087997\n",
      "Test loss at epoch (196):  0.08782596\n",
      "Train loss at epoch (197):  0.13116091\n",
      "Test loss at epoch (197):  0.087356865\n",
      "Train loss at epoch (198):  0.1318288\n",
      "Test loss at epoch (198):  0.08615109\n",
      "Train loss at epoch (199):  0.13026963\n",
      "Test loss at epoch (199):  0.08479926\n",
      "Train loss at epoch (200):  0.1305235\n",
      "Test loss at epoch (200):  0.085575595\n",
      "Train loss at epoch (201):  0.1302378\n",
      "Test loss at epoch (201):  0.08577197\n",
      "Train loss at epoch (202):  0.13005958\n",
      "Test loss at epoch (202):  0.08512234\n",
      "Train loss at epoch (203):  0.12978117\n",
      "Test loss at epoch (203):  0.08566496\n",
      "Train loss at epoch (204):  0.1288627\n",
      "Test loss at epoch (204):  0.084249794\n",
      "Train loss at epoch (205):  0.12924443\n",
      "Test loss at epoch (205):  0.08449851\n",
      "Train loss at epoch (206):  0.12828614\n",
      "Test loss at epoch (206):  0.083825216\n",
      "Train loss at epoch (207):  0.1287001\n",
      "Test loss at epoch (207):  0.08461223\n",
      "Train loss at epoch (208):  0.12825176\n",
      "Test loss at epoch (208):  0.08469566\n",
      "Train loss at epoch (209):  0.1279336\n",
      "Test loss at epoch (209):  0.08276991\n",
      "Train loss at epoch (210):  0.12810348\n",
      "Test loss at epoch (210):  0.083175935\n",
      "Train loss at epoch (211):  0.12771562\n",
      "Test loss at epoch (211):  0.08505841\n",
      "Train loss at epoch (212):  0.12799451\n",
      "Test loss at epoch (212):  0.08286943\n",
      "Train loss at epoch (213):  0.12692693\n",
      "Test loss at epoch (213):  0.082768485\n",
      "Train loss at epoch (214):  0.12736458\n",
      "Test loss at epoch (214):  0.08155287\n",
      "Train loss at epoch (215):  0.12655531\n",
      "Test loss at epoch (215):  0.081797294\n",
      "Train loss at epoch (216):  0.12613274\n",
      "Test loss at epoch (216):  0.08285876\n",
      "Train loss at epoch (217):  0.1266671\n",
      "Test loss at epoch (217):  0.081831135\n",
      "Train loss at epoch (218):  0.12703851\n",
      "Test loss at epoch (218):  0.08198687\n",
      "Train loss at epoch (219):  0.12549815\n",
      "Test loss at epoch (219):  0.08118745\n",
      "Train loss at epoch (220):  0.12554811\n",
      "Test loss at epoch (220):  0.08236751\n",
      "Train loss at epoch (221):  0.12515892\n",
      "Test loss at epoch (221):  0.08217974\n",
      "Train loss at epoch (222):  0.1248802\n",
      "Test loss at epoch (222):  0.08070368\n",
      "Train loss at epoch (223):  0.12462941\n",
      "Test loss at epoch (223):  0.08234306\n",
      "Train loss at epoch (224):  0.12569532\n",
      "Test loss at epoch (224):  0.079584226\n",
      "Train loss at epoch (225):  0.12324714\n",
      "Test loss at epoch (225):  0.08044398\n",
      "Train loss at epoch (226):  0.123914704\n",
      "Test loss at epoch (226):  0.08034693\n",
      "Train loss at epoch (227):  0.12391527\n",
      "Test loss at epoch (227):  0.08105819\n",
      "Train loss at epoch (228):  0.12307901\n",
      "Test loss at epoch (228):  0.07895534\n",
      "Train loss at epoch (229):  0.12301818\n",
      "Test loss at epoch (229):  0.08001902\n",
      "Train loss at epoch (230):  0.12315404\n",
      "Test loss at epoch (230):  0.07871845\n",
      "Train loss at epoch (231):  0.122339375\n",
      "Test loss at epoch (231):  0.08049317\n",
      "Train loss at epoch (232):  0.12269252\n",
      "Test loss at epoch (232):  0.078244165\n",
      "Train loss at epoch (233):  0.12203407\n",
      "Test loss at epoch (233):  0.07855906\n",
      "Train loss at epoch (234):  0.12165718\n",
      "Test loss at epoch (234):  0.078200206\n",
      "Train loss at epoch (235):  0.121653505\n",
      "Test loss at epoch (235):  0.079599425\n",
      "Train loss at epoch (236):  0.12053489\n",
      "Test loss at epoch (236):  0.07928095\n",
      "Train loss at epoch (237):  0.12188436\n",
      "Test loss at epoch (237):  0.07824073\n",
      "Train loss at epoch (238):  0.121628135\n",
      "Test loss at epoch (238):  0.077963665\n",
      "Train loss at epoch (239):  0.120956175\n",
      "Test loss at epoch (239):  0.0777357\n",
      "Train loss at epoch (240):  0.121226475\n",
      "Test loss at epoch (240):  0.078269176\n",
      "Train loss at epoch (241):  0.12053345\n",
      "Test loss at epoch (241):  0.07862519\n",
      "Train loss at epoch (242):  0.12092297\n",
      "Test loss at epoch (242):  0.076495156\n",
      "Train loss at epoch (243):  0.120672666\n",
      "Test loss at epoch (243):  0.07677365\n",
      "Train loss at epoch (244):  0.12068457\n",
      "Test loss at epoch (244):  0.07606542\n",
      "Train loss at epoch (245):  0.12020776\n",
      "Test loss at epoch (245):  0.07657045\n",
      "Train loss at epoch (246):  0.119324714\n",
      "Test loss at epoch (246):  0.07677059\n",
      "Train loss at epoch (247):  0.11950091\n",
      "Test loss at epoch (247):  0.075551085\n",
      "Train loss at epoch (248):  0.118808135\n",
      "Test loss at epoch (248):  0.0771361\n",
      "Train loss at epoch (249):  0.11788947\n",
      "Test loss at epoch (249):  0.075917915\n",
      "Train loss at epoch (250):  0.118903875\n",
      "Test loss at epoch (250):  0.07598231\n",
      "Train loss at epoch (251):  0.11840517\n",
      "Test loss at epoch (251):  0.075920545\n",
      "Train loss at epoch (252):  0.11741081\n",
      "Test loss at epoch (252):  0.07642623\n",
      "Train loss at epoch (253):  0.11789286\n",
      "Test loss at epoch (253):  0.074860446\n",
      "Train loss at epoch (254):  0.11848899\n",
      "Test loss at epoch (254):  0.07518512\n",
      "Train loss at epoch (255):  0.1184415\n",
      "Test loss at epoch (255):  0.07461124\n",
      "Train loss at epoch (256):  0.11763367\n",
      "Test loss at epoch (256):  0.074314326\n",
      "Train loss at epoch (257):  0.11785607\n",
      "Test loss at epoch (257):  0.07554154\n",
      "Train loss at epoch (258):  0.11738502\n",
      "Test loss at epoch (258):  0.074675\n",
      "Train loss at epoch (259):  0.11729531\n",
      "Test loss at epoch (259):  0.07422309\n",
      "Train loss at epoch (260):  0.116798215\n",
      "Test loss at epoch (260):  0.07365769\n",
      "Train loss at epoch (261):  0.11605753\n",
      "Test loss at epoch (261):  0.074162014\n",
      "Train loss at epoch (262):  0.11633932\n",
      "Test loss at epoch (262):  0.07429924\n",
      "Train loss at epoch (263):  0.11581019\n",
      "Test loss at epoch (263):  0.07367031\n",
      "Train loss at epoch (264):  0.11625619\n",
      "Test loss at epoch (264):  0.07394064\n",
      "Train loss at epoch (265):  0.11560873\n",
      "Test loss at epoch (265):  0.072882816\n",
      "Train loss at epoch (266):  0.11617201\n",
      "Test loss at epoch (266):  0.0728561\n",
      "Train loss at epoch (267):  0.11527101\n",
      "Test loss at epoch (267):  0.07275109\n",
      "Train loss at epoch (268):  0.115715936\n",
      "Test loss at epoch (268):  0.0731202\n",
      "Train loss at epoch (269):  0.11497169\n",
      "Test loss at epoch (269):  0.07383098\n",
      "Train loss at epoch (270):  0.11586487\n",
      "Test loss at epoch (270):  0.07252482\n",
      "Train loss at epoch (271):  0.11557173\n",
      "Test loss at epoch (271):  0.07264876\n",
      "Train loss at epoch (272):  0.11450313\n",
      "Test loss at epoch (272):  0.07237336\n",
      "Train loss at epoch (273):  0.11465195\n",
      "Test loss at epoch (273):  0.07225653\n",
      "Train loss at epoch (274):  0.11479521\n",
      "Test loss at epoch (274):  0.07307119\n",
      "Train loss at epoch (275):  0.114726745\n",
      "Test loss at epoch (275):  0.07214067\n",
      "Train loss at epoch (276):  0.1137003\n",
      "Test loss at epoch (276):  0.07217849\n",
      "Train loss at epoch (277):  0.114224836\n",
      "Test loss at epoch (277):  0.072103396\n",
      "Train loss at epoch (278):  0.113976195\n",
      "Test loss at epoch (278):  0.07134353\n",
      "Train loss at epoch (279):  0.11413633\n",
      "Test loss at epoch (279):  0.07050404\n",
      "Train loss at epoch (280):  0.11307483\n",
      "Test loss at epoch (280):  0.07104091\n",
      "Train loss at epoch (281):  0.11368688\n",
      "Test loss at epoch (281):  0.072233714\n",
      "Train loss at epoch (282):  0.1138601\n",
      "Test loss at epoch (282):  0.07123787\n",
      "Train loss at epoch (283):  0.11265274\n",
      "Test loss at epoch (283):  0.07231418\n",
      "Train loss at epoch (284):  0.113166206\n",
      "Test loss at epoch (284):  0.07187172\n",
      "Train loss at epoch (285):  0.11272088\n",
      "Test loss at epoch (285):  0.07154497\n",
      "Train loss at epoch (286):  0.11144021\n",
      "Test loss at epoch (286):  0.0704873\n",
      "Train loss at epoch (287):  0.112676196\n",
      "Test loss at epoch (287):  0.07060234\n",
      "Train loss at epoch (288):  0.112781234\n",
      "Test loss at epoch (288):  0.069786236\n",
      "Train loss at epoch (289):  0.11222326\n",
      "Test loss at epoch (289):  0.07150003\n",
      "Train loss at epoch (290):  0.111201\n",
      "Test loss at epoch (290):  0.06941719\n",
      "Train loss at epoch (291):  0.111243196\n",
      "Test loss at epoch (291):  0.07017625\n",
      "Train loss at epoch (292):  0.111436136\n",
      "Test loss at epoch (292):  0.070636675\n",
      "Train loss at epoch (293):  0.11211824\n",
      "Test loss at epoch (293):  0.06940499\n",
      "Train loss at epoch (294):  0.11169645\n",
      "Test loss at epoch (294):  0.06901158\n",
      "Train loss at epoch (295):  0.11112772\n",
      "Test loss at epoch (295):  0.07040142\n",
      "Train loss at epoch (296):  0.111475416\n",
      "Test loss at epoch (296):  0.06881535\n",
      "Train loss at epoch (297):  0.11043901\n",
      "Test loss at epoch (297):  0.068636894\n",
      "Train loss at epoch (298):  0.11101227\n",
      "Test loss at epoch (298):  0.068045005\n",
      "Train loss at epoch (299):  0.11048102\n",
      "Test loss at epoch (299):  0.06933354\n",
      "Train loss at epoch (300):  0.11099705\n",
      "Test loss at epoch (300):  0.067054495\n",
      "Train loss at epoch (301):  0.110216536\n",
      "Test loss at epoch (301):  0.06798118\n",
      "Train loss at epoch (302):  0.11013983\n",
      "Test loss at epoch (302):  0.069063805\n",
      "Train loss at epoch (303):  0.109804906\n",
      "Test loss at epoch (303):  0.06803902\n",
      "Train loss at epoch (304):  0.10967482\n",
      "Test loss at epoch (304):  0.06724015\n",
      "Train loss at epoch (305):  0.110280246\n",
      "Test loss at epoch (305):  0.06875727\n",
      "Train loss at epoch (306):  0.10991124\n",
      "Test loss at epoch (306):  0.06716086\n",
      "Train loss at epoch (307):  0.10977291\n",
      "Test loss at epoch (307):  0.067846015\n",
      "Train loss at epoch (308):  0.109968096\n",
      "Test loss at epoch (308):  0.067735754\n",
      "Train loss at epoch (309):  0.10989908\n",
      "Test loss at epoch (309):  0.06607784\n",
      "Train loss at epoch (310):  0.1099366\n",
      "Test loss at epoch (310):  0.066407405\n",
      "Train loss at epoch (311):  0.10827767\n",
      "Test loss at epoch (311):  0.0677486\n",
      "Train loss at epoch (312):  0.10872054\n",
      "Test loss at epoch (312):  0.068067744\n",
      "Train loss at epoch (313):  0.10871192\n",
      "Test loss at epoch (313):  0.065983795\n",
      "Train loss at epoch (314):  0.108433284\n",
      "Test loss at epoch (314):  0.06793714\n",
      "Train loss at epoch (315):  0.108875126\n",
      "Test loss at epoch (315):  0.06640265\n",
      "Train loss at epoch (316):  0.10801847\n",
      "Test loss at epoch (316):  0.06676149\n",
      "Train loss at epoch (317):  0.10786159\n",
      "Test loss at epoch (317):  0.06707346\n",
      "Train loss at epoch (318):  0.10878928\n",
      "Test loss at epoch (318):  0.065982774\n",
      "Train loss at epoch (319):  0.1073186\n",
      "Test loss at epoch (319):  0.06621679\n",
      "Train loss at epoch (320):  0.107027516\n",
      "Test loss at epoch (320):  0.06683734\n",
      "Train loss at epoch (321):  0.107418686\n",
      "Test loss at epoch (321):  0.06535243\n",
      "Train loss at epoch (322):  0.10659562\n",
      "Test loss at epoch (322):  0.06608811\n",
      "Train loss at epoch (323):  0.106390715\n",
      "Test loss at epoch (323):  0.06595247\n",
      "Train loss at epoch (324):  0.107087776\n",
      "Test loss at epoch (324):  0.06645508\n",
      "Train loss at epoch (325):  0.10674993\n",
      "Test loss at epoch (325):  0.06500301\n",
      "Train loss at epoch (326):  0.107421234\n",
      "Test loss at epoch (326):  0.065693334\n",
      "Train loss at epoch (327):  0.10632344\n",
      "Test loss at epoch (327):  0.064916134\n",
      "Train loss at epoch (328):  0.10674192\n",
      "Test loss at epoch (328):  0.06547956\n",
      "Train loss at epoch (329):  0.10651788\n",
      "Test loss at epoch (329):  0.06424678\n",
      "Train loss at epoch (330):  0.107686415\n",
      "Test loss at epoch (330):  0.06539511\n",
      "Train loss at epoch (331):  0.10603425\n",
      "Test loss at epoch (331):  0.06507736\n",
      "Train loss at epoch (332):  0.10627171\n",
      "Test loss at epoch (332):  0.06472238\n",
      "Train loss at epoch (333):  0.10605939\n",
      "Test loss at epoch (333):  0.064910546\n",
      "Train loss at epoch (334):  0.106303416\n",
      "Test loss at epoch (334):  0.06425108\n",
      "Train loss at epoch (335):  0.10529198\n",
      "Test loss at epoch (335):  0.063360736\n",
      "Train loss at epoch (336):  0.10528629\n",
      "Test loss at epoch (336):  0.0631252\n",
      "Train loss at epoch (337):  0.10552449\n",
      "Test loss at epoch (337):  0.064564064\n",
      "Train loss at epoch (338):  0.104968876\n",
      "Test loss at epoch (338):  0.06381604\n",
      "Train loss at epoch (339):  0.105185755\n",
      "Test loss at epoch (339):  0.06261744\n",
      "Train loss at epoch (340):  0.10500269\n",
      "Test loss at epoch (340):  0.06317131\n",
      "Train loss at epoch (341):  0.104732566\n",
      "Test loss at epoch (341):  0.062484574\n",
      "Train loss at epoch (342):  0.10476269\n",
      "Test loss at epoch (342):  0.06349314\n",
      "Train loss at epoch (343):  0.105129994\n",
      "Test loss at epoch (343):  0.06409023\n",
      "Train loss at epoch (344):  0.104523614\n",
      "Test loss at epoch (344):  0.06332278\n",
      "Train loss at epoch (345):  0.10480619\n",
      "Test loss at epoch (345):  0.06249987\n",
      "Train loss at epoch (346):  0.103377506\n",
      "Test loss at epoch (346):  0.061784454\n",
      "Train loss at epoch (347):  0.10444657\n",
      "Test loss at epoch (347):  0.061874706\n",
      "Train loss at epoch (348):  0.10421007\n",
      "Test loss at epoch (348):  0.06284772\n",
      "Train loss at epoch (349):  0.1047341\n",
      "Test loss at epoch (349):  0.062837295\n",
      "Train loss at epoch (350):  0.10411142\n",
      "Test loss at epoch (350):  0.06314606\n",
      "Train loss at epoch (351):  0.10320955\n",
      "Test loss at epoch (351):  0.061742093\n",
      "Train loss at epoch (352):  0.10373231\n",
      "Test loss at epoch (352):  0.06249508\n",
      "Train loss at epoch (353):  0.1035852\n",
      "Test loss at epoch (353):  0.06230886\n",
      "Train loss at epoch (354):  0.10338489\n",
      "Test loss at epoch (354):  0.061227184\n",
      "Train loss at epoch (355):  0.10326147\n",
      "Test loss at epoch (355):  0.0630077\n",
      "Train loss at epoch (356):  0.10329243\n",
      "Test loss at epoch (356):  0.06101726\n",
      "Train loss at epoch (357):  0.10251248\n",
      "Test loss at epoch (357):  0.061928146\n",
      "Train loss at epoch (358):  0.10321716\n",
      "Test loss at epoch (358):  0.06244482\n",
      "Train loss at epoch (359):  0.102867275\n",
      "Test loss at epoch (359):  0.061423182\n",
      "Train loss at epoch (360):  0.102060154\n",
      "Test loss at epoch (360):  0.06217101\n",
      "Train loss at epoch (361):  0.10320926\n",
      "Test loss at epoch (361):  0.0596516\n",
      "Train loss at epoch (362):  0.10224263\n",
      "Test loss at epoch (362):  0.061034843\n",
      "Train loss at epoch (363):  0.10280986\n",
      "Test loss at epoch (363):  0.06129381\n",
      "Train loss at epoch (364):  0.10314917\n",
      "Test loss at epoch (364):  0.061064743\n",
      "Train loss at epoch (365):  0.101505265\n",
      "Test loss at epoch (365):  0.061578736\n",
      "Train loss at epoch (366):  0.10199377\n",
      "Test loss at epoch (366):  0.06046167\n",
      "Train loss at epoch (367):  0.10189642\n",
      "Test loss at epoch (367):  0.060855728\n",
      "Train loss at epoch (368):  0.10216892\n",
      "Test loss at epoch (368):  0.06147545\n",
      "Train loss at epoch (369):  0.10160756\n",
      "Test loss at epoch (369):  0.0616537\n",
      "Train loss at epoch (370):  0.1018637\n",
      "Test loss at epoch (370):  0.059366684\n",
      "Train loss at epoch (371):  0.101478904\n",
      "Test loss at epoch (371):  0.061091173\n",
      "Train loss at epoch (372):  0.1014465\n",
      "Test loss at epoch (372):  0.060309768\n",
      "Train loss at epoch (373):  0.10167453\n",
      "Test loss at epoch (373):  0.061139494\n",
      "Train loss at epoch (374):  0.10138559\n",
      "Test loss at epoch (374):  0.060231663\n",
      "Train loss at epoch (375):  0.101055756\n",
      "Test loss at epoch (375):  0.060578596\n",
      "Train loss at epoch (376):  0.1010236\n",
      "Test loss at epoch (376):  0.059199784\n",
      "Train loss at epoch (377):  0.100881666\n",
      "Test loss at epoch (377):  0.05983769\n",
      "Train loss at epoch (378):  0.10127593\n",
      "Test loss at epoch (378):  0.060257252\n",
      "Train loss at epoch (379):  0.10018356\n",
      "Test loss at epoch (379):  0.059736498\n",
      "Train loss at epoch (380):  0.10039074\n",
      "Test loss at epoch (380):  0.059760742\n",
      "Train loss at epoch (381):  0.10036171\n",
      "Test loss at epoch (381):  0.059394684\n",
      "Train loss at epoch (382):  0.100339875\n",
      "Test loss at epoch (382):  0.059570998\n",
      "Train loss at epoch (383):  0.10018358\n",
      "Test loss at epoch (383):  0.058657516\n",
      "Train loss at epoch (384):  0.10040343\n",
      "Test loss at epoch (384):  0.059197642\n",
      "Train loss at epoch (385):  0.1004326\n",
      "Test loss at epoch (385):  0.05958305\n",
      "Train loss at epoch (386):  0.09989457\n",
      "Test loss at epoch (386):  0.059322044\n",
      "Train loss at epoch (387):  0.09972329\n",
      "Test loss at epoch (387):  0.06014604\n",
      "Train loss at epoch (388):  0.09947371\n",
      "Test loss at epoch (388):  0.058771778\n",
      "Train loss at epoch (389):  0.10006903\n",
      "Test loss at epoch (389):  0.059404273\n",
      "Train loss at epoch (390):  0.09947879\n",
      "Test loss at epoch (390):  0.058806363\n",
      "Train loss at epoch (391):  0.09957116\n",
      "Test loss at epoch (391):  0.057264622\n",
      "Train loss at epoch (392):  0.09968272\n",
      "Test loss at epoch (392):  0.058840606\n",
      "Train loss at epoch (393):  0.09892731\n",
      "Test loss at epoch (393):  0.058719225\n",
      "Train loss at epoch (394):  0.09910757\n",
      "Test loss at epoch (394):  0.05870601\n",
      "Train loss at epoch (395):  0.09889241\n",
      "Test loss at epoch (395):  0.05719859\n",
      "Train loss at epoch (396):  0.09932101\n",
      "Test loss at epoch (396):  0.058086786\n",
      "Train loss at epoch (397):  0.09883017\n",
      "Test loss at epoch (397):  0.058090486\n",
      "Train loss at epoch (398):  0.09880377\n",
      "Test loss at epoch (398):  0.057835467\n",
      "Train loss at epoch (399):  0.09895069\n",
      "Test loss at epoch (399):  0.0594418\n",
      "Train loss at epoch (400):  0.098352626\n",
      "Test loss at epoch (400):  0.058939207\n",
      "Train loss at epoch (401):  0.09902069\n",
      "Test loss at epoch (401):  0.058251683\n",
      "Train loss at epoch (402):  0.098609164\n",
      "Test loss at epoch (402):  0.056827553\n",
      "Train loss at epoch (403):  0.09875338\n",
      "Test loss at epoch (403):  0.057659764\n",
      "Train loss at epoch (404):  0.09799444\n",
      "Test loss at epoch (404):  0.057382077\n",
      "Train loss at epoch (405):  0.0979977\n",
      "Test loss at epoch (405):  0.056628022\n",
      "Train loss at epoch (406):  0.098522015\n",
      "Test loss at epoch (406):  0.057238866\n",
      "Train loss at epoch (407):  0.0980992\n",
      "Test loss at epoch (407):  0.05672046\n",
      "Train loss at epoch (408):  0.09729277\n",
      "Test loss at epoch (408):  0.057227887\n",
      "Train loss at epoch (409):  0.09843446\n",
      "Test loss at epoch (409):  0.056999277\n",
      "Train loss at epoch (410):  0.09774407\n",
      "Test loss at epoch (410):  0.05745041\n",
      "Train loss at epoch (411):  0.09754613\n",
      "Test loss at epoch (411):  0.056838855\n",
      "Train loss at epoch (412):  0.09674485\n",
      "Test loss at epoch (412):  0.056562353\n",
      "Train loss at epoch (413):  0.09737278\n",
      "Test loss at epoch (413):  0.057399992\n",
      "Train loss at epoch (414):  0.09668953\n",
      "Test loss at epoch (414):  0.055516876\n",
      "Train loss at epoch (415):  0.09655393\n",
      "Test loss at epoch (415):  0.055727556\n",
      "Train loss at epoch (416):  0.09684107\n",
      "Test loss at epoch (416):  0.05645728\n",
      "Train loss at epoch (417):  0.09723507\n",
      "Test loss at epoch (417):  0.055991407\n",
      "Train loss at epoch (418):  0.096934706\n",
      "Test loss at epoch (418):  0.05516105\n",
      "Train loss at epoch (419):  0.096318595\n",
      "Test loss at epoch (419):  0.055641998\n",
      "Train loss at epoch (420):  0.09653922\n",
      "Test loss at epoch (420):  0.055299126\n",
      "Train loss at epoch (421):  0.096837796\n",
      "Test loss at epoch (421):  0.055634085\n",
      "Train loss at epoch (422):  0.09647842\n",
      "Test loss at epoch (422):  0.05554733\n",
      "Train loss at epoch (423):  0.09647371\n",
      "Test loss at epoch (423):  0.05665536\n",
      "Train loss at epoch (424):  0.096872255\n",
      "Test loss at epoch (424):  0.055776652\n",
      "Train loss at epoch (425):  0.09698929\n",
      "Test loss at epoch (425):  0.055857006\n",
      "Train loss at epoch (426):  0.096109375\n",
      "Test loss at epoch (426):  0.055342823\n",
      "Train loss at epoch (427):  0.09634654\n",
      "Test loss at epoch (427):  0.055603717\n",
      "Train loss at epoch (428):  0.09594075\n",
      "Test loss at epoch (428):  0.05589052\n",
      "Train loss at epoch (429):  0.09585115\n",
      "Test loss at epoch (429):  0.055678923\n",
      "Train loss at epoch (430):  0.095848314\n",
      "Test loss at epoch (430):  0.055269647\n",
      "Train loss at epoch (431):  0.09618228\n",
      "Test loss at epoch (431):  0.055056904\n",
      "Train loss at epoch (432):  0.09564784\n",
      "Test loss at epoch (432):  0.055542644\n",
      "Train loss at epoch (433):  0.095480636\n",
      "Test loss at epoch (433):  0.054033507\n",
      "Train loss at epoch (434):  0.09676822\n",
      "Test loss at epoch (434):  0.05549234\n",
      "Train loss at epoch (435):  0.0955051\n",
      "Test loss at epoch (435):  0.053681523\n",
      "Train loss at epoch (436):  0.09583925\n",
      "Test loss at epoch (436):  0.053585753\n",
      "Train loss at epoch (437):  0.095357105\n",
      "Test loss at epoch (437):  0.054598123\n",
      "Train loss at epoch (438):  0.09587472\n",
      "Test loss at epoch (438):  0.054984994\n",
      "Train loss at epoch (439):  0.095164694\n",
      "Test loss at epoch (439):  0.054693628\n",
      "Train loss at epoch (440):  0.09585841\n",
      "Test loss at epoch (440):  0.05464757\n",
      "Train loss at epoch (441):  0.09558551\n",
      "Test loss at epoch (441):  0.05480803\n",
      "Train loss at epoch (442):  0.09428061\n",
      "Test loss at epoch (442):  0.053529218\n",
      "Train loss at epoch (443):  0.095289305\n",
      "Test loss at epoch (443):  0.053503573\n",
      "Train loss at epoch (444):  0.09441416\n",
      "Test loss at epoch (444):  0.054365873\n",
      "Train loss at epoch (445):  0.09545706\n",
      "Test loss at epoch (445):  0.053664703\n",
      "Train loss at epoch (446):  0.09505227\n",
      "Test loss at epoch (446):  0.05340097\n",
      "Train loss at epoch (447):  0.09477924\n",
      "Test loss at epoch (447):  0.054883607\n",
      "Train loss at epoch (448):  0.09504291\n",
      "Test loss at epoch (448):  0.053189784\n",
      "Train loss at epoch (449):  0.094675064\n",
      "Test loss at epoch (449):  0.054111205\n",
      "Train loss at epoch (450):  0.09428128\n",
      "Test loss at epoch (450):  0.054241266\n",
      "Train loss at epoch (451):  0.094411165\n",
      "Test loss at epoch (451):  0.05460282\n",
      "Train loss at epoch (452):  0.094419904\n",
      "Test loss at epoch (452):  0.05404794\n",
      "Train loss at epoch (453):  0.09412194\n",
      "Test loss at epoch (453):  0.053991158\n",
      "Train loss at epoch (454):  0.09437884\n",
      "Test loss at epoch (454):  0.05324203\n",
      "Train loss at epoch (455):  0.09391106\n",
      "Test loss at epoch (455):  0.054185294\n",
      "Train loss at epoch (456):  0.09384733\n",
      "Test loss at epoch (456):  0.053133488\n",
      "Train loss at epoch (457):  0.09417451\n",
      "Test loss at epoch (457):  0.052964315\n",
      "Train loss at epoch (458):  0.0940014\n",
      "Test loss at epoch (458):  0.052688546\n",
      "Train loss at epoch (459):  0.09372273\n",
      "Test loss at epoch (459):  0.053458944\n",
      "Train loss at epoch (460):  0.09446985\n",
      "Test loss at epoch (460):  0.0532516\n",
      "Train loss at epoch (461):  0.09391082\n",
      "Test loss at epoch (461):  0.053296775\n",
      "Train loss at epoch (462):  0.09383074\n",
      "Test loss at epoch (462):  0.052859906\n",
      "Train loss at epoch (463):  0.09357353\n",
      "Test loss at epoch (463):  0.05263181\n",
      "Train loss at epoch (464):  0.09344656\n",
      "Test loss at epoch (464):  0.053327456\n",
      "Train loss at epoch (465):  0.09254122\n",
      "Test loss at epoch (465):  0.054348145\n",
      "Train loss at epoch (466):  0.094062164\n",
      "Test loss at epoch (466):  0.052366756\n",
      "Train loss at epoch (467):  0.09329246\n",
      "Test loss at epoch (467):  0.053204857\n",
      "Train loss at epoch (468):  0.09302551\n",
      "Test loss at epoch (468):  0.05253834\n",
      "Train loss at epoch (469):  0.09301638\n",
      "Test loss at epoch (469):  0.05325529\n",
      "Train loss at epoch (470):  0.092313364\n",
      "Test loss at epoch (470):  0.05187348\n",
      "Train loss at epoch (471):  0.09297461\n",
      "Test loss at epoch (471):  0.051789016\n",
      "Train loss at epoch (472):  0.09351404\n",
      "Test loss at epoch (472):  0.053328775\n",
      "Train loss at epoch (473):  0.09271382\n",
      "Test loss at epoch (473):  0.052873783\n",
      "Train loss at epoch (474):  0.09241939\n",
      "Test loss at epoch (474):  0.052577104\n",
      "Train loss at epoch (475):  0.09288761\n",
      "Test loss at epoch (475):  0.05179734\n",
      "Train loss at epoch (476):  0.09252007\n",
      "Test loss at epoch (476):  0.052299332\n",
      "Train loss at epoch (477):  0.093208365\n",
      "Test loss at epoch (477):  0.051603436\n",
      "Train loss at epoch (478):  0.092532076\n",
      "Test loss at epoch (478):  0.052648142\n",
      "Train loss at epoch (479):  0.093184106\n",
      "Test loss at epoch (479):  0.051469762\n",
      "Train loss at epoch (480):  0.091275126\n",
      "Test loss at epoch (480):  0.05267751\n",
      "Train loss at epoch (481):  0.0922607\n",
      "Test loss at epoch (481):  0.05228168\n",
      "Train loss at epoch (482):  0.09157962\n",
      "Test loss at epoch (482):  0.05109508\n",
      "Train loss at epoch (483):  0.09192148\n",
      "Test loss at epoch (483):  0.05104329\n",
      "Train loss at epoch (484):  0.091971025\n",
      "Test loss at epoch (484):  0.050662264\n",
      "Train loss at epoch (485):  0.09195079\n",
      "Test loss at epoch (485):  0.0507607\n",
      "Train loss at epoch (486):  0.09179631\n",
      "Test loss at epoch (486):  0.05111907\n",
      "Train loss at epoch (487):  0.09222139\n",
      "Test loss at epoch (487):  0.05162702\n",
      "Train loss at epoch (488):  0.09191975\n",
      "Test loss at epoch (488):  0.051548343\n",
      "Train loss at epoch (489):  0.09230593\n",
      "Test loss at epoch (489):  0.052035652\n",
      "Train loss at epoch (490):  0.09154252\n",
      "Test loss at epoch (490):  0.051699933\n",
      "Train loss at epoch (491):  0.09182588\n",
      "Test loss at epoch (491):  0.0516655\n",
      "Train loss at epoch (492):  0.091392204\n",
      "Test loss at epoch (492):  0.051790018\n",
      "Train loss at epoch (493):  0.09191818\n",
      "Test loss at epoch (493):  0.051733814\n",
      "Train loss at epoch (494):  0.09149488\n",
      "Test loss at epoch (494):  0.051047813\n",
      "Train loss at epoch (495):  0.091150485\n",
      "Test loss at epoch (495):  0.051728185\n",
      "Train loss at epoch (496):  0.09084746\n",
      "Test loss at epoch (496):  0.050335493\n",
      "Train loss at epoch (497):  0.09111481\n",
      "Test loss at epoch (497):  0.050535463\n",
      "Train loss at epoch (498):  0.090755716\n",
      "Test loss at epoch (498):  0.051062312\n",
      "Train loss at epoch (499):  0.09119565\n",
      "Test loss at epoch (499):  0.051308177\n",
      "Train loss at epoch (500):  0.09100664\n",
      "Test loss at epoch (500):  0.050664525\n",
      "Train loss at epoch (501):  0.090841085\n",
      "Test loss at epoch (501):  0.051637836\n",
      "Train loss at epoch (502):  0.09079155\n",
      "Test loss at epoch (502):  0.050697606\n",
      "Train loss at epoch (503):  0.08971652\n",
      "Test loss at epoch (503):  0.051238265\n",
      "Train loss at epoch (504):  0.09053136\n",
      "Test loss at epoch (504):  0.05005142\n",
      "Train loss at epoch (505):  0.09078904\n",
      "Test loss at epoch (505):  0.05034095\n",
      "Train loss at epoch (506):  0.09103911\n",
      "Test loss at epoch (506):  0.050697125\n",
      "Train loss at epoch (507):  0.09037245\n",
      "Test loss at epoch (507):  0.050131615\n",
      "Train loss at epoch (508):  0.09035225\n",
      "Test loss at epoch (508):  0.05002601\n",
      "Train loss at epoch (509):  0.09085325\n",
      "Test loss at epoch (509):  0.05072699\n",
      "Train loss at epoch (510):  0.09067746\n",
      "Test loss at epoch (510):  0.049790423\n",
      "Train loss at epoch (511):  0.08997465\n",
      "Test loss at epoch (511):  0.05033666\n",
      "Train loss at epoch (512):  0.090711504\n",
      "Test loss at epoch (512):  0.04952005\n",
      "Train loss at epoch (513):  0.09034921\n",
      "Test loss at epoch (513):  0.04917397\n",
      "Train loss at epoch (514):  0.089924574\n",
      "Test loss at epoch (514):  0.050480668\n",
      "Train loss at epoch (515):  0.09027012\n",
      "Test loss at epoch (515):  0.050887376\n",
      "Train loss at epoch (516):  0.090218864\n",
      "Test loss at epoch (516):  0.0498718\n",
      "Train loss at epoch (517):  0.089818925\n",
      "Test loss at epoch (517):  0.05056914\n",
      "Train loss at epoch (518):  0.089721605\n",
      "Test loss at epoch (518):  0.049560107\n",
      "Train loss at epoch (519):  0.089930534\n",
      "Test loss at epoch (519):  0.04933752\n",
      "Train loss at epoch (520):  0.089425035\n",
      "Test loss at epoch (520):  0.049481362\n",
      "Train loss at epoch (521):  0.08961995\n",
      "Test loss at epoch (521):  0.048645068\n",
      "Train loss at epoch (522):  0.08923615\n",
      "Test loss at epoch (522):  0.049170244\n",
      "Train loss at epoch (523):  0.08933965\n",
      "Test loss at epoch (523):  0.049406905\n",
      "Train loss at epoch (524):  0.08960405\n",
      "Test loss at epoch (524):  0.050611265\n",
      "Train loss at epoch (525):  0.088927135\n",
      "Test loss at epoch (525):  0.048865266\n",
      "Train loss at epoch (526):  0.08927019\n",
      "Test loss at epoch (526):  0.049898807\n",
      "Train loss at epoch (527):  0.08915378\n",
      "Test loss at epoch (527):  0.04896628\n",
      "Train loss at epoch (528):  0.089086734\n",
      "Test loss at epoch (528):  0.05010746\n",
      "Train loss at epoch (529):  0.08909699\n",
      "Test loss at epoch (529):  0.04920609\n",
      "Train loss at epoch (530):  0.088938825\n",
      "Test loss at epoch (530):  0.04940394\n",
      "Train loss at epoch (531):  0.08848062\n",
      "Test loss at epoch (531):  0.048732717\n",
      "Train loss at epoch (532):  0.08923766\n",
      "Test loss at epoch (532):  0.04927995\n",
      "Train loss at epoch (533):  0.08886166\n",
      "Test loss at epoch (533):  0.04997914\n",
      "Train loss at epoch (534):  0.088888496\n",
      "Test loss at epoch (534):  0.04885531\n",
      "Train loss at epoch (535):  0.088778585\n",
      "Test loss at epoch (535):  0.048398715\n",
      "Train loss at epoch (536):  0.0885381\n",
      "Test loss at epoch (536):  0.04871664\n",
      "Train loss at epoch (537):  0.088903435\n",
      "Test loss at epoch (537):  0.049961127\n",
      "Train loss at epoch (538):  0.0887667\n",
      "Test loss at epoch (538):  0.049666923\n",
      "Train loss at epoch (539):  0.088467225\n",
      "Test loss at epoch (539):  0.048147604\n",
      "Train loss at epoch (540):  0.089244574\n",
      "Test loss at epoch (540):  0.048255254\n",
      "Train loss at epoch (541):  0.088550255\n",
      "Test loss at epoch (541):  0.049239956\n",
      "Train loss at epoch (542):  0.08868004\n",
      "Test loss at epoch (542):  0.04890644\n",
      "Train loss at epoch (543):  0.08831648\n",
      "Test loss at epoch (543):  0.048029378\n",
      "Train loss at epoch (544):  0.08785787\n",
      "Test loss at epoch (544):  0.048080172\n",
      "Train loss at epoch (545):  0.087429434\n",
      "Test loss at epoch (545):  0.048019025\n",
      "Train loss at epoch (546):  0.0885684\n",
      "Test loss at epoch (546):  0.048017766\n",
      "Train loss at epoch (547):  0.08802222\n",
      "Test loss at epoch (547):  0.048240647\n",
      "Train loss at epoch (548):  0.08762443\n",
      "Test loss at epoch (548):  0.04830129\n",
      "Train loss at epoch (549):  0.087983295\n",
      "Test loss at epoch (549):  0.04831522\n",
      "Train loss at epoch (550):  0.08792722\n",
      "Test loss at epoch (550):  0.047707744\n",
      "Train loss at epoch (551):  0.08801581\n",
      "Test loss at epoch (551):  0.047887526\n",
      "Train loss at epoch (552):  0.08870828\n",
      "Test loss at epoch (552):  0.048028063\n",
      "Train loss at epoch (553):  0.08808919\n",
      "Test loss at epoch (553):  0.048900276\n",
      "Train loss at epoch (554):  0.08770406\n",
      "Test loss at epoch (554):  0.048963703\n",
      "Train loss at epoch (555):  0.08793133\n",
      "Test loss at epoch (555):  0.048563033\n",
      "Train loss at epoch (556):  0.08748199\n",
      "Test loss at epoch (556):  0.048074216\n",
      "Train loss at epoch (557):  0.08778066\n",
      "Test loss at epoch (557):  0.04834957\n",
      "Train loss at epoch (558):  0.08756404\n",
      "Test loss at epoch (558):  0.048629638\n",
      "Train loss at epoch (559):  0.088526815\n",
      "Test loss at epoch (559):  0.047870576\n",
      "Train loss at epoch (560):  0.087832294\n",
      "Test loss at epoch (560):  0.04779565\n",
      "Train loss at epoch (561):  0.08773374\n",
      "Test loss at epoch (561):  0.047433734\n",
      "Train loss at epoch (562):  0.08703254\n",
      "Test loss at epoch (562):  0.047812507\n",
      "Train loss at epoch (563):  0.087318555\n",
      "Test loss at epoch (563):  0.046653725\n",
      "Train loss at epoch (564):  0.087828636\n",
      "Test loss at epoch (564):  0.047863785\n",
      "Train loss at epoch (565):  0.08752989\n",
      "Test loss at epoch (565):  0.048077703\n",
      "Train loss at epoch (566):  0.08700567\n",
      "Test loss at epoch (566):  0.04752368\n",
      "Train loss at epoch (567):  0.08695507\n",
      "Test loss at epoch (567):  0.04775592\n",
      "Train loss at epoch (568):  0.08652818\n",
      "Test loss at epoch (568):  0.047308717\n",
      "Train loss at epoch (569):  0.08727848\n",
      "Test loss at epoch (569):  0.047376495\n",
      "Train loss at epoch (570):  0.08731112\n",
      "Test loss at epoch (570):  0.04716078\n",
      "Train loss at epoch (571):  0.086627856\n",
      "Test loss at epoch (571):  0.048064973\n",
      "Train loss at epoch (572):  0.08651861\n",
      "Test loss at epoch (572):  0.04685909\n",
      "Train loss at epoch (573):  0.08644017\n",
      "Test loss at epoch (573):  0.048198357\n",
      "Train loss at epoch (574):  0.08664158\n",
      "Test loss at epoch (574):  0.047313176\n",
      "Train loss at epoch (575):  0.08706261\n",
      "Test loss at epoch (575):  0.04794354\n",
      "Train loss at epoch (576):  0.08670508\n",
      "Test loss at epoch (576):  0.04781451\n",
      "Train loss at epoch (577):  0.086567774\n",
      "Test loss at epoch (577):  0.04673031\n",
      "Train loss at epoch (578):  0.08673017\n",
      "Test loss at epoch (578):  0.047454804\n",
      "Train loss at epoch (579):  0.08623454\n",
      "Test loss at epoch (579):  0.047627997\n",
      "Train loss at epoch (580):  0.086688526\n",
      "Test loss at epoch (580):  0.04673122\n",
      "Train loss at epoch (581):  0.086682275\n",
      "Test loss at epoch (581):  0.047634974\n",
      "Train loss at epoch (582):  0.08612172\n",
      "Test loss at epoch (582):  0.046565235\n",
      "Train loss at epoch (583):  0.086332045\n",
      "Test loss at epoch (583):  0.046902455\n",
      "Train loss at epoch (584):  0.0863499\n",
      "Test loss at epoch (584):  0.046732683\n",
      "Train loss at epoch (585):  0.08611524\n",
      "Test loss at epoch (585):  0.04680805\n",
      "Train loss at epoch (586):  0.085708685\n",
      "Test loss at epoch (586):  0.046578825\n",
      "Train loss at epoch (587):  0.085942656\n",
      "Test loss at epoch (587):  0.046581052\n",
      "Train loss at epoch (588):  0.085646555\n",
      "Test loss at epoch (588):  0.047783192\n",
      "Train loss at epoch (589):  0.086241804\n",
      "Test loss at epoch (589):  0.046592005\n",
      "Train loss at epoch (590):  0.08600453\n",
      "Test loss at epoch (590):  0.046198387\n",
      "Train loss at epoch (591):  0.08641677\n",
      "Test loss at epoch (591):  0.047157403\n",
      "Train loss at epoch (592):  0.08629617\n",
      "Test loss at epoch (592):  0.04643767\n",
      "Train loss at epoch (593):  0.08650151\n",
      "Test loss at epoch (593):  0.046384145\n",
      "Train loss at epoch (594):  0.08586909\n",
      "Test loss at epoch (594):  0.04727896\n",
      "Train loss at epoch (595):  0.08561514\n",
      "Test loss at epoch (595):  0.046529796\n",
      "Train loss at epoch (596):  0.08543429\n",
      "Test loss at epoch (596):  0.047730077\n",
      "Train loss at epoch (597):  0.08598146\n",
      "Test loss at epoch (597):  0.04634637\n",
      "Train loss at epoch (598):  0.08558753\n",
      "Test loss at epoch (598):  0.04636593\n",
      "Train loss at epoch (599):  0.08603419\n",
      "Test loss at epoch (599):  0.045780726\n",
      "Train loss at epoch (600):  0.08517173\n",
      "Test loss at epoch (600):  0.04615977\n",
      "Train loss at epoch (601):  0.085674055\n",
      "Test loss at epoch (601):  0.045941643\n",
      "Train loss at epoch (602):  0.08597105\n",
      "Test loss at epoch (602):  0.045670696\n",
      "Train loss at epoch (603):  0.085483104\n",
      "Test loss at epoch (603):  0.04619294\n",
      "Train loss at epoch (604):  0.085523404\n",
      "Test loss at epoch (604):  0.04642029\n",
      "Train loss at epoch (605):  0.08544722\n",
      "Test loss at epoch (605):  0.045879543\n",
      "Train loss at epoch (606):  0.08511825\n",
      "Test loss at epoch (606):  0.04660282\n",
      "Train loss at epoch (607):  0.08496763\n",
      "Test loss at epoch (607):  0.046063296\n",
      "Train loss at epoch (608):  0.08572641\n",
      "Test loss at epoch (608):  0.04699934\n",
      "Train loss at epoch (609):  0.08583825\n",
      "Test loss at epoch (609):  0.046117343\n",
      "Train loss at epoch (610):  0.085173674\n",
      "Test loss at epoch (610):  0.046167873\n",
      "Train loss at epoch (611):  0.08533449\n",
      "Test loss at epoch (611):  0.045443602\n",
      "Train loss at epoch (612):  0.08500718\n",
      "Test loss at epoch (612):  0.046682946\n",
      "Train loss at epoch (613):  0.084885046\n",
      "Test loss at epoch (613):  0.045964405\n",
      "Train loss at epoch (614):  0.085453995\n",
      "Test loss at epoch (614):  0.046306748\n",
      "Train loss at epoch (615):  0.08412822\n",
      "Test loss at epoch (615):  0.0451227\n",
      "Train loss at epoch (616):  0.085002\n",
      "Test loss at epoch (616):  0.045813356\n",
      "Train loss at epoch (617):  0.0849572\n",
      "Test loss at epoch (617):  0.045470316\n",
      "Train loss at epoch (618):  0.085359976\n",
      "Test loss at epoch (618):  0.04562664\n",
      "Train loss at epoch (619):  0.084515855\n",
      "Test loss at epoch (619):  0.04482425\n",
      "Train loss at epoch (620):  0.084774\n",
      "Test loss at epoch (620):  0.04505496\n",
      "Train loss at epoch (621):  0.084210046\n",
      "Test loss at epoch (621):  0.045139875\n",
      "Train loss at epoch (622):  0.08470318\n",
      "Test loss at epoch (622):  0.045506496\n",
      "Train loss at epoch (623):  0.0853716\n",
      "Test loss at epoch (623):  0.045422766\n",
      "Train loss at epoch (624):  0.08453293\n",
      "Test loss at epoch (624):  0.04581742\n",
      "Train loss at epoch (625):  0.08413388\n",
      "Test loss at epoch (625):  0.045254335\n",
      "Train loss at epoch (626):  0.08429487\n",
      "Test loss at epoch (626):  0.04482618\n",
      "Train loss at epoch (627):  0.08477526\n",
      "Test loss at epoch (627):  0.044616286\n",
      "Train loss at epoch (628):  0.08488805\n",
      "Test loss at epoch (628):  0.046138044\n",
      "Train loss at epoch (629):  0.08423001\n",
      "Test loss at epoch (629):  0.045208436\n",
      "Train loss at epoch (630):  0.08389124\n",
      "Test loss at epoch (630):  0.04585121\n",
      "Train loss at epoch (631):  0.08427665\n",
      "Test loss at epoch (631):  0.045572698\n",
      "Train loss at epoch (632):  0.08421458\n",
      "Test loss at epoch (632):  0.044440288\n",
      "Train loss at epoch (633):  0.08387867\n",
      "Test loss at epoch (633):  0.044618756\n",
      "Train loss at epoch (634):  0.084912896\n",
      "Test loss at epoch (634):  0.04568972\n",
      "Train loss at epoch (635):  0.084107645\n",
      "Test loss at epoch (635):  0.04510344\n",
      "Train loss at epoch (636):  0.08500909\n",
      "Test loss at epoch (636):  0.04518363\n",
      "Train loss at epoch (637):  0.084114365\n",
      "Test loss at epoch (637):  0.04483684\n",
      "Train loss at epoch (638):  0.08371977\n",
      "Test loss at epoch (638):  0.044793237\n",
      "Train loss at epoch (639):  0.08369152\n",
      "Test loss at epoch (639):  0.045482244\n",
      "Train loss at epoch (640):  0.08420689\n",
      "Test loss at epoch (640):  0.045374557\n",
      "Train loss at epoch (641):  0.08340992\n",
      "Test loss at epoch (641):  0.044295065\n",
      "Train loss at epoch (642):  0.0839539\n",
      "Test loss at epoch (642):  0.04436775\n",
      "Train loss at epoch (643):  0.083569534\n",
      "Test loss at epoch (643):  0.04453365\n",
      "Train loss at epoch (644):  0.084053524\n",
      "Test loss at epoch (644):  0.04499675\n",
      "Train loss at epoch (645):  0.08329304\n",
      "Test loss at epoch (645):  0.044184666\n",
      "Train loss at epoch (646):  0.08330957\n",
      "Test loss at epoch (646):  0.044683147\n",
      "Train loss at epoch (647):  0.0834632\n",
      "Test loss at epoch (647):  0.044431224\n",
      "Train loss at epoch (648):  0.083343826\n",
      "Test loss at epoch (648):  0.044920355\n",
      "Train loss at epoch (649):  0.08386394\n",
      "Test loss at epoch (649):  0.044572234\n",
      "Train loss at epoch (650):  0.08353087\n",
      "Test loss at epoch (650):  0.044464283\n",
      "Train loss at epoch (651):  0.084374815\n",
      "Test loss at epoch (651):  0.044202365\n",
      "Train loss at epoch (652):  0.08351073\n",
      "Test loss at epoch (652):  0.044653106\n",
      "Train loss at epoch (653):  0.083197206\n",
      "Test loss at epoch (653):  0.044787347\n",
      "Train loss at epoch (654):  0.08344844\n",
      "Test loss at epoch (654):  0.04465422\n",
      "Train loss at epoch (655):  0.083549626\n",
      "Test loss at epoch (655):  0.043835144\n",
      "Train loss at epoch (656):  0.08342842\n",
      "Test loss at epoch (656):  0.044405896\n",
      "Train loss at epoch (657):  0.08322438\n",
      "Test loss at epoch (657):  0.04407653\n",
      "Train loss at epoch (658):  0.08412573\n",
      "Test loss at epoch (658):  0.043609485\n",
      "Train loss at epoch (659):  0.08324292\n",
      "Test loss at epoch (659):  0.044557013\n",
      "Train loss at epoch (660):  0.08269683\n",
      "Test loss at epoch (660):  0.043186933\n",
      "Train loss at epoch (661):  0.083296604\n",
      "Test loss at epoch (661):  0.044281628\n",
      "Train loss at epoch (662):  0.08300957\n",
      "Test loss at epoch (662):  0.04438751\n",
      "Train loss at epoch (663):  0.08320281\n",
      "Test loss at epoch (663):  0.044242673\n",
      "Train loss at epoch (664):  0.08297476\n",
      "Test loss at epoch (664):  0.04374699\n",
      "Train loss at epoch (665):  0.08242771\n",
      "Test loss at epoch (665):  0.04430363\n",
      "Train loss at epoch (666):  0.08267777\n",
      "Test loss at epoch (666):  0.04446908\n",
      "Train loss at epoch (667):  0.08277697\n",
      "Test loss at epoch (667):  0.044347715\n",
      "Train loss at epoch (668):  0.08351905\n",
      "Test loss at epoch (668):  0.044651743\n",
      "Train loss at epoch (669):  0.08262882\n",
      "Test loss at epoch (669):  0.04413372\n",
      "Train loss at epoch (670):  0.08312517\n",
      "Test loss at epoch (670):  0.043868758\n",
      "Train loss at epoch (671):  0.08280567\n",
      "Test loss at epoch (671):  0.044171147\n",
      "Train loss at epoch (672):  0.083029956\n",
      "Test loss at epoch (672):  0.044387795\n",
      "Train loss at epoch (673):  0.082752295\n",
      "Test loss at epoch (673):  0.044645358\n",
      "Train loss at epoch (674):  0.0827859\n",
      "Test loss at epoch (674):  0.043671973\n",
      "Train loss at epoch (675):  0.08204714\n",
      "Test loss at epoch (675):  0.04402845\n",
      "Train loss at epoch (676):  0.08291591\n",
      "Test loss at epoch (676):  0.044843756\n",
      "Train loss at epoch (677):  0.082389124\n",
      "Test loss at epoch (677):  0.044118553\n",
      "Train loss at epoch (678):  0.08306216\n",
      "Test loss at epoch (678):  0.04373623\n",
      "Train loss at epoch (679):  0.08290533\n",
      "Test loss at epoch (679):  0.043752186\n",
      "Train loss at epoch (680):  0.08244976\n",
      "Test loss at epoch (680):  0.043510024\n",
      "Train loss at epoch (681):  0.0824316\n",
      "Test loss at epoch (681):  0.04381393\n",
      "Train loss at epoch (682):  0.08225889\n",
      "Test loss at epoch (682):  0.044548728\n",
      "Train loss at epoch (683):  0.08243558\n",
      "Test loss at epoch (683):  0.04346835\n",
      "Train loss at epoch (684):  0.08293041\n",
      "Test loss at epoch (684):  0.04415788\n",
      "Train loss at epoch (685):  0.08261434\n",
      "Test loss at epoch (685):  0.044314943\n",
      "Train loss at epoch (686):  0.08174353\n",
      "Test loss at epoch (686):  0.044152204\n",
      "Train loss at epoch (687):  0.08213497\n",
      "Test loss at epoch (687):  0.0430335\n",
      "Train loss at epoch (688):  0.08217573\n",
      "Test loss at epoch (688):  0.043531243\n",
      "Train loss at epoch (689):  0.08223148\n",
      "Test loss at epoch (689):  0.043040216\n",
      "Train loss at epoch (690):  0.08152277\n",
      "Test loss at epoch (690):  0.04257953\n",
      "Train loss at epoch (691):  0.0819068\n",
      "Test loss at epoch (691):  0.043141965\n",
      "Train loss at epoch (692):  0.082466945\n",
      "Test loss at epoch (692):  0.043969415\n",
      "Train loss at epoch (693):  0.08215463\n",
      "Test loss at epoch (693):  0.043198742\n",
      "Train loss at epoch (694):  0.08155739\n",
      "Test loss at epoch (694):  0.043015625\n",
      "Train loss at epoch (695):  0.082133465\n",
      "Test loss at epoch (695):  0.04272798\n",
      "Train loss at epoch (696):  0.082153484\n",
      "Test loss at epoch (696):  0.042701256\n",
      "Train loss at epoch (697):  0.08168797\n",
      "Test loss at epoch (697):  0.044115428\n",
      "Train loss at epoch (698):  0.08167748\n",
      "Test loss at epoch (698):  0.042457428\n",
      "Train loss at epoch (699):  0.08189679\n",
      "Test loss at epoch (699):  0.043179862\n",
      "Train loss at epoch (700):  0.0816722\n",
      "Test loss at epoch (700):  0.042854004\n",
      "Train loss at epoch (701):  0.08193889\n",
      "Test loss at epoch (701):  0.04283585\n",
      "Train loss at epoch (702):  0.0821724\n",
      "Test loss at epoch (702):  0.043135144\n",
      "Train loss at epoch (703):  0.080762975\n",
      "Test loss at epoch (703):  0.044360377\n",
      "Train loss at epoch (704):  0.08138594\n",
      "Test loss at epoch (704):  0.04371349\n",
      "Train loss at epoch (705):  0.08098351\n",
      "Test loss at epoch (705):  0.04247738\n",
      "Train loss at epoch (706):  0.08201969\n",
      "Test loss at epoch (706):  0.04375165\n",
      "Train loss at epoch (707):  0.08124668\n",
      "Test loss at epoch (707):  0.042829078\n",
      "Train loss at epoch (708):  0.08170248\n",
      "Test loss at epoch (708):  0.043162417\n",
      "Train loss at epoch (709):  0.0815977\n",
      "Test loss at epoch (709):  0.043227367\n",
      "Train loss at epoch (710):  0.08186609\n",
      "Test loss at epoch (710):  0.042756625\n",
      "Train loss at epoch (711):  0.0818827\n",
      "Test loss at epoch (711):  0.043481037\n",
      "Train loss at epoch (712):  0.08140553\n",
      "Test loss at epoch (712):  0.043451186\n",
      "Train loss at epoch (713):  0.08197374\n",
      "Test loss at epoch (713):  0.042377748\n",
      "Train loss at epoch (714):  0.081469044\n",
      "Test loss at epoch (714):  0.04321788\n",
      "Train loss at epoch (715):  0.08170172\n",
      "Test loss at epoch (715):  0.042687997\n",
      "Train loss at epoch (716):  0.08108337\n",
      "Test loss at epoch (716):  0.042229045\n",
      "Train loss at epoch (717):  0.08105668\n",
      "Test loss at epoch (717):  0.04233124\n",
      "Train loss at epoch (718):  0.0811288\n",
      "Test loss at epoch (718):  0.042617187\n",
      "Train loss at epoch (719):  0.08098415\n",
      "Test loss at epoch (719):  0.04343731\n",
      "Train loss at epoch (720):  0.08098461\n",
      "Test loss at epoch (720):  0.04266881\n",
      "Train loss at epoch (721):  0.0814159\n",
      "Test loss at epoch (721):  0.042930435\n",
      "Train loss at epoch (722):  0.08126572\n",
      "Test loss at epoch (722):  0.04224571\n",
      "Train loss at epoch (723):  0.08144777\n",
      "Test loss at epoch (723):  0.041959517\n",
      "Train loss at epoch (724):  0.08076836\n",
      "Test loss at epoch (724):  0.04280832\n",
      "Train loss at epoch (725):  0.0810789\n",
      "Test loss at epoch (725):  0.043114457\n",
      "Train loss at epoch (726):  0.080987744\n",
      "Test loss at epoch (726):  0.042589527\n",
      "Train loss at epoch (727):  0.0814265\n",
      "Test loss at epoch (727):  0.042741355\n",
      "Train loss at epoch (728):  0.08131695\n",
      "Test loss at epoch (728):  0.042735014\n",
      "Train loss at epoch (729):  0.081174694\n",
      "Test loss at epoch (729):  0.042753935\n",
      "Train loss at epoch (730):  0.08122504\n",
      "Test loss at epoch (730):  0.042973958\n",
      "Train loss at epoch (731):  0.081044875\n",
      "Test loss at epoch (731):  0.04293975\n",
      "Train loss at epoch (732):  0.08091966\n",
      "Test loss at epoch (732):  0.04265047\n",
      "Train loss at epoch (733):  0.081045344\n",
      "Test loss at epoch (733):  0.043016195\n",
      "Train loss at epoch (734):  0.08098494\n",
      "Test loss at epoch (734):  0.042178977\n",
      "Train loss at epoch (735):  0.08068451\n",
      "Test loss at epoch (735):  0.042718768\n",
      "Train loss at epoch (736):  0.080472\n",
      "Test loss at epoch (736):  0.04179007\n",
      "Train loss at epoch (737):  0.08087401\n",
      "Test loss at epoch (737):  0.04274118\n",
      "Train loss at epoch (738):  0.08038484\n",
      "Test loss at epoch (738):  0.043106202\n",
      "Train loss at epoch (739):  0.080627054\n",
      "Test loss at epoch (739):  0.04132667\n",
      "Train loss at epoch (740):  0.08115856\n",
      "Test loss at epoch (740):  0.04138218\n",
      "Train loss at epoch (741):  0.08103622\n",
      "Test loss at epoch (741):  0.041453168\n",
      "Train loss at epoch (742):  0.08018752\n",
      "Test loss at epoch (742):  0.042666044\n",
      "Train loss at epoch (743):  0.08024032\n",
      "Test loss at epoch (743):  0.042068724\n",
      "Train loss at epoch (744):  0.081052795\n",
      "Test loss at epoch (744):  0.04157957\n",
      "Train loss at epoch (745):  0.08070374\n",
      "Test loss at epoch (745):  0.04242223\n",
      "Train loss at epoch (746):  0.08041915\n",
      "Test loss at epoch (746):  0.041665353\n",
      "Train loss at epoch (747):  0.0805316\n",
      "Test loss at epoch (747):  0.042408958\n",
      "Train loss at epoch (748):  0.08073582\n",
      "Test loss at epoch (748):  0.0424884\n",
      "Train loss at epoch (749):  0.08048688\n",
      "Test loss at epoch (749):  0.04220534\n",
      "Train loss at epoch (750):  0.08027092\n",
      "Test loss at epoch (750):  0.041730978\n",
      "Train loss at epoch (751):  0.08038615\n",
      "Test loss at epoch (751):  0.04128596\n",
      "Train loss at epoch (752):  0.07946952\n",
      "Test loss at epoch (752):  0.042652044\n",
      "Train loss at epoch (753):  0.08027594\n",
      "Test loss at epoch (753):  0.041686025\n",
      "Train loss at epoch (754):  0.08043002\n",
      "Test loss at epoch (754):  0.04208902\n",
      "Train loss at epoch (755):  0.08025798\n",
      "Test loss at epoch (755):  0.041435912\n",
      "Train loss at epoch (756):  0.07971979\n",
      "Test loss at epoch (756):  0.04222177\n",
      "Train loss at epoch (757):  0.08057681\n",
      "Test loss at epoch (757):  0.041813437\n",
      "Train loss at epoch (758):  0.08002419\n",
      "Test loss at epoch (758):  0.042668976\n",
      "Train loss at epoch (759):  0.080469795\n",
      "Test loss at epoch (759):  0.041669093\n",
      "Train loss at epoch (760):  0.08024654\n",
      "Test loss at epoch (760):  0.041578647\n",
      "Train loss at epoch (761):  0.08030235\n",
      "Test loss at epoch (761):  0.041456968\n",
      "Train loss at epoch (762):  0.07979408\n",
      "Test loss at epoch (762):  0.04152433\n",
      "Train loss at epoch (763):  0.08016257\n",
      "Test loss at epoch (763):  0.042664632\n",
      "Train loss at epoch (764):  0.08002755\n",
      "Test loss at epoch (764):  0.04147933\n",
      "Train loss at epoch (765):  0.080305286\n",
      "Test loss at epoch (765):  0.041962855\n",
      "Train loss at epoch (766):  0.0799727\n",
      "Test loss at epoch (766):  0.042194698\n",
      "Train loss at epoch (767):  0.08016212\n",
      "Test loss at epoch (767):  0.042548772\n",
      "Train loss at epoch (768):  0.07970817\n",
      "Test loss at epoch (768):  0.041839585\n",
      "Train loss at epoch (769):  0.080080636\n",
      "Test loss at epoch (769):  0.04153969\n",
      "Train loss at epoch (770):  0.079404935\n",
      "Test loss at epoch (770):  0.041732915\n",
      "Train loss at epoch (771):  0.07928944\n",
      "Test loss at epoch (771):  0.041425645\n",
      "Train loss at epoch (772):  0.0799486\n",
      "Test loss at epoch (772):  0.042398047\n",
      "Train loss at epoch (773):  0.08013529\n",
      "Test loss at epoch (773):  0.042135146\n",
      "Train loss at epoch (774):  0.079387136\n",
      "Test loss at epoch (774):  0.04140086\n",
      "Train loss at epoch (775):  0.07997224\n",
      "Test loss at epoch (775):  0.042127613\n",
      "Train loss at epoch (776):  0.08000406\n",
      "Test loss at epoch (776):  0.041223325\n",
      "Train loss at epoch (777):  0.08044949\n",
      "Test loss at epoch (777):  0.04186302\n",
      "Train loss at epoch (778):  0.079524405\n",
      "Test loss at epoch (778):  0.04113058\n",
      "Train loss at epoch (779):  0.07977747\n",
      "Test loss at epoch (779):  0.041525926\n",
      "Train loss at epoch (780):  0.080151595\n",
      "Test loss at epoch (780):  0.041303605\n",
      "Train loss at epoch (781):  0.079920575\n",
      "Test loss at epoch (781):  0.04145816\n",
      "Train loss at epoch (782):  0.07891157\n",
      "Test loss at epoch (782):  0.041776117\n",
      "Train loss at epoch (783):  0.07968268\n",
      "Test loss at epoch (783):  0.04104794\n",
      "Train loss at epoch (784):  0.07938189\n",
      "Test loss at epoch (784):  0.041338153\n",
      "Train loss at epoch (785):  0.079796895\n",
      "Test loss at epoch (785):  0.041205194\n",
      "Train loss at epoch (786):  0.0799702\n",
      "Test loss at epoch (786):  0.04134759\n",
      "Train loss at epoch (787):  0.07919668\n",
      "Test loss at epoch (787):  0.04115352\n",
      "Train loss at epoch (788):  0.07923654\n",
      "Test loss at epoch (788):  0.041307233\n",
      "Train loss at epoch (789):  0.07912291\n",
      "Test loss at epoch (789):  0.041547902\n",
      "Train loss at epoch (790):  0.08008288\n",
      "Test loss at epoch (790):  0.04177963\n",
      "Train loss at epoch (791):  0.079184756\n",
      "Test loss at epoch (791):  0.040875975\n",
      "Train loss at epoch (792):  0.07968826\n",
      "Test loss at epoch (792):  0.041150644\n",
      "Train loss at epoch (793):  0.079099044\n",
      "Test loss at epoch (793):  0.041870162\n",
      "Train loss at epoch (794):  0.07948052\n",
      "Test loss at epoch (794):  0.04169356\n",
      "Train loss at epoch (795):  0.07911244\n",
      "Test loss at epoch (795):  0.040466834\n",
      "Train loss at epoch (796):  0.07905091\n",
      "Test loss at epoch (796):  0.041065544\n",
      "Train loss at epoch (797):  0.079021774\n",
      "Test loss at epoch (797):  0.041290328\n",
      "Train loss at epoch (798):  0.07894954\n",
      "Test loss at epoch (798):  0.040731885\n",
      "Train loss at epoch (799):  0.07916413\n",
      "Test loss at epoch (799):  0.041342925\n",
      "Train loss at epoch (800):  0.079074115\n",
      "Test loss at epoch (800):  0.041099593\n",
      "Train loss at epoch (801):  0.079168685\n",
      "Test loss at epoch (801):  0.041390575\n",
      "Train loss at epoch (802):  0.079347044\n",
      "Test loss at epoch (802):  0.041421752\n",
      "Train loss at epoch (803):  0.07883851\n",
      "Test loss at epoch (803):  0.04138112\n",
      "Train loss at epoch (804):  0.07891571\n",
      "Test loss at epoch (804):  0.0407548\n",
      "Train loss at epoch (805):  0.07879455\n",
      "Test loss at epoch (805):  0.041163206\n",
      "Train loss at epoch (806):  0.078876294\n",
      "Test loss at epoch (806):  0.041728325\n",
      "Train loss at epoch (807):  0.07932754\n",
      "Test loss at epoch (807):  0.041265734\n",
      "Train loss at epoch (808):  0.07838364\n",
      "Test loss at epoch (808):  0.041442044\n",
      "Train loss at epoch (809):  0.07919084\n",
      "Test loss at epoch (809):  0.04092563\n",
      "Train loss at epoch (810):  0.07889026\n",
      "Test loss at epoch (810):  0.04027854\n",
      "Train loss at epoch (811):  0.07929723\n",
      "Test loss at epoch (811):  0.04114461\n",
      "Train loss at epoch (812):  0.0797182\n",
      "Test loss at epoch (812):  0.040354915\n",
      "Train loss at epoch (813):  0.07889693\n",
      "Test loss at epoch (813):  0.041561395\n",
      "Train loss at epoch (814):  0.07887022\n",
      "Test loss at epoch (814):  0.04022819\n",
      "Train loss at epoch (815):  0.079056345\n",
      "Test loss at epoch (815):  0.04131346\n",
      "Train loss at epoch (816):  0.07885072\n",
      "Test loss at epoch (816):  0.04069653\n",
      "Train loss at epoch (817):  0.07880891\n",
      "Test loss at epoch (817):  0.04023084\n",
      "Train loss at epoch (818):  0.07919808\n",
      "Test loss at epoch (818):  0.040679757\n",
      "Train loss at epoch (819):  0.078973085\n",
      "Test loss at epoch (819):  0.040644504\n",
      "Train loss at epoch (820):  0.07922897\n",
      "Test loss at epoch (820):  0.041167874\n",
      "Train loss at epoch (821):  0.078668356\n",
      "Test loss at epoch (821):  0.040514596\n",
      "Train loss at epoch (822):  0.07871481\n",
      "Test loss at epoch (822):  0.04164586\n",
      "Train loss at epoch (823):  0.07919191\n",
      "Test loss at epoch (823):  0.040881865\n",
      "Train loss at epoch (824):  0.078991\n",
      "Test loss at epoch (824):  0.040820096\n",
      "Train loss at epoch (825):  0.07892353\n",
      "Test loss at epoch (825):  0.041509405\n",
      "Train loss at epoch (826):  0.07867678\n",
      "Test loss at epoch (826):  0.04016047\n",
      "Train loss at epoch (827):  0.07882556\n",
      "Test loss at epoch (827):  0.041033242\n",
      "Train loss at epoch (828):  0.078278296\n",
      "Test loss at epoch (828):  0.04053347\n",
      "Train loss at epoch (829):  0.078963764\n",
      "Test loss at epoch (829):  0.040488765\n",
      "Train loss at epoch (830):  0.078879066\n",
      "Test loss at epoch (830):  0.04026054\n",
      "Train loss at epoch (831):  0.0784872\n",
      "Test loss at epoch (831):  0.04090842\n",
      "Train loss at epoch (832):  0.07830302\n",
      "Test loss at epoch (832):  0.040170323\n",
      "Train loss at epoch (833):  0.078650825\n",
      "Test loss at epoch (833):  0.04069855\n",
      "Train loss at epoch (834):  0.07900543\n",
      "Test loss at epoch (834):  0.041205663\n",
      "Train loss at epoch (835):  0.0785013\n",
      "Test loss at epoch (835):  0.040396355\n",
      "Train loss at epoch (836):  0.078418896\n",
      "Test loss at epoch (836):  0.040815625\n",
      "Train loss at epoch (837):  0.07816239\n",
      "Test loss at epoch (837):  0.040848702\n",
      "Train loss at epoch (838):  0.07867961\n",
      "Test loss at epoch (838):  0.03990654\n",
      "Train loss at epoch (839):  0.07833604\n",
      "Test loss at epoch (839):  0.040571902\n",
      "Train loss at epoch (840):  0.078715816\n",
      "Test loss at epoch (840):  0.04012885\n",
      "Train loss at epoch (841):  0.07854208\n",
      "Test loss at epoch (841):  0.040323496\n",
      "Train loss at epoch (842):  0.07856426\n",
      "Test loss at epoch (842):  0.04102498\n",
      "Train loss at epoch (843):  0.077976465\n",
      "Test loss at epoch (843):  0.040895373\n",
      "Train loss at epoch (844):  0.07813724\n",
      "Test loss at epoch (844):  0.040960196\n",
      "Train loss at epoch (845):  0.07778915\n",
      "Test loss at epoch (845):  0.040423416\n",
      "Train loss at epoch (846):  0.078243904\n",
      "Test loss at epoch (846):  0.040262207\n",
      "Train loss at epoch (847):  0.07863485\n",
      "Test loss at epoch (847):  0.040578462\n",
      "Train loss at epoch (848):  0.078355834\n",
      "Test loss at epoch (848):  0.039740473\n",
      "Train loss at epoch (849):  0.077952474\n",
      "Test loss at epoch (849):  0.040588968\n",
      "Train loss at epoch (850):  0.07823719\n",
      "Test loss at epoch (850):  0.04038095\n",
      "Train loss at epoch (851):  0.07783814\n",
      "Test loss at epoch (851):  0.04047232\n",
      "Train loss at epoch (852):  0.07844262\n",
      "Test loss at epoch (852):  0.040096033\n",
      "Train loss at epoch (853):  0.078231946\n",
      "Test loss at epoch (853):  0.04053598\n",
      "Train loss at epoch (854):  0.0784165\n",
      "Test loss at epoch (854):  0.040505376\n",
      "Train loss at epoch (855):  0.077719316\n",
      "Test loss at epoch (855):  0.041323524\n",
      "Train loss at epoch (856):  0.078360565\n",
      "Test loss at epoch (856):  0.040625118\n",
      "Train loss at epoch (857):  0.07782356\n",
      "Test loss at epoch (857):  0.040066652\n",
      "Train loss at epoch (858):  0.0775061\n",
      "Test loss at epoch (858):  0.040553916\n",
      "Train loss at epoch (859):  0.077863164\n",
      "Test loss at epoch (859):  0.040012036\n",
      "Train loss at epoch (860):  0.07772892\n",
      "Test loss at epoch (860):  0.04140802\n",
      "Train loss at epoch (861):  0.077707276\n",
      "Test loss at epoch (861):  0.04027636\n",
      "Train loss at epoch (862):  0.077940784\n",
      "Test loss at epoch (862):  0.04009196\n",
      "Train loss at epoch (863):  0.07729547\n",
      "Test loss at epoch (863):  0.040523466\n",
      "Train loss at epoch (864):  0.07776827\n",
      "Test loss at epoch (864):  0.040677425\n",
      "Train loss at epoch (865):  0.0777903\n",
      "Test loss at epoch (865):  0.040530402\n",
      "Train loss at epoch (866):  0.07820117\n",
      "Test loss at epoch (866):  0.03953771\n",
      "Train loss at epoch (867):  0.077740826\n",
      "Test loss at epoch (867):  0.04018132\n",
      "Train loss at epoch (868):  0.07777444\n",
      "Test loss at epoch (868):  0.040389463\n",
      "Train loss at epoch (869):  0.07797292\n",
      "Test loss at epoch (869):  0.040293384\n",
      "Train loss at epoch (870):  0.07761535\n",
      "Test loss at epoch (870):  0.040004775\n",
      "Train loss at epoch (871):  0.07827346\n",
      "Test loss at epoch (871):  0.04040073\n",
      "Train loss at epoch (872):  0.07782074\n",
      "Test loss at epoch (872):  0.039467532\n",
      "Train loss at epoch (873):  0.07724138\n",
      "Test loss at epoch (873):  0.04047814\n",
      "Train loss at epoch (874):  0.07781394\n",
      "Test loss at epoch (874):  0.039890863\n",
      "Train loss at epoch (875):  0.077764824\n",
      "Test loss at epoch (875):  0.039940063\n",
      "Train loss at epoch (876):  0.07758459\n",
      "Test loss at epoch (876):  0.039935835\n",
      "Train loss at epoch (877):  0.078207664\n",
      "Test loss at epoch (877):  0.040336456\n",
      "Train loss at epoch (878):  0.07801799\n",
      "Test loss at epoch (878):  0.03937698\n",
      "Train loss at epoch (879):  0.07805718\n",
      "Test loss at epoch (879):  0.03982736\n",
      "Train loss at epoch (880):  0.07802185\n",
      "Test loss at epoch (880):  0.039706446\n",
      "Train loss at epoch (881):  0.077961735\n",
      "Test loss at epoch (881):  0.040724635\n",
      "Train loss at epoch (882):  0.07719521\n",
      "Test loss at epoch (882):  0.03893276\n",
      "Train loss at epoch (883):  0.07770917\n",
      "Test loss at epoch (883):  0.040668793\n",
      "Train loss at epoch (884):  0.0776664\n",
      "Test loss at epoch (884):  0.039900567\n",
      "Train loss at epoch (885):  0.07722734\n",
      "Test loss at epoch (885):  0.03944375\n",
      "Train loss at epoch (886):  0.07797338\n",
      "Test loss at epoch (886):  0.039346058\n",
      "Train loss at epoch (887):  0.07736807\n",
      "Test loss at epoch (887):  0.04043268\n",
      "Train loss at epoch (888):  0.077114455\n",
      "Test loss at epoch (888):  0.04045214\n",
      "Train loss at epoch (889):  0.077528134\n",
      "Test loss at epoch (889):  0.039267533\n",
      "Train loss at epoch (890):  0.077677555\n",
      "Test loss at epoch (890):  0.039630964\n",
      "Train loss at epoch (891):  0.077300996\n",
      "Test loss at epoch (891):  0.039757896\n",
      "Train loss at epoch (892):  0.07727981\n",
      "Test loss at epoch (892):  0.04018243\n",
      "Train loss at epoch (893):  0.07745235\n",
      "Test loss at epoch (893):  0.04035135\n",
      "Train loss at epoch (894):  0.077424936\n",
      "Test loss at epoch (894):  0.03971352\n",
      "Train loss at epoch (895):  0.07714467\n",
      "Test loss at epoch (895):  0.040000282\n",
      "Train loss at epoch (896):  0.077076316\n",
      "Test loss at epoch (896):  0.04034108\n",
      "Train loss at epoch (897):  0.07687699\n",
      "Test loss at epoch (897):  0.039411604\n",
      "Train loss at epoch (898):  0.07778773\n",
      "Test loss at epoch (898):  0.03938585\n",
      "Train loss at epoch (899):  0.07699414\n",
      "Test loss at epoch (899):  0.039063856\n",
      "Train loss at epoch (900):  0.0774825\n",
      "Test loss at epoch (900):  0.0396816\n",
      "Train loss at epoch (901):  0.077207156\n",
      "Test loss at epoch (901):  0.039451372\n",
      "Train loss at epoch (902):  0.07741674\n",
      "Test loss at epoch (902):  0.040272586\n",
      "Train loss at epoch (903):  0.07758184\n",
      "Test loss at epoch (903):  0.03899296\n",
      "Train loss at epoch (904):  0.077295676\n",
      "Test loss at epoch (904):  0.04038582\n",
      "Train loss at epoch (905):  0.07791551\n",
      "Test loss at epoch (905):  0.03963341\n",
      "Train loss at epoch (906):  0.07688136\n",
      "Test loss at epoch (906):  0.03950063\n",
      "Train loss at epoch (907):  0.07739047\n",
      "Test loss at epoch (907):  0.039861355\n",
      "Train loss at epoch (908):  0.077330284\n",
      "Test loss at epoch (908):  0.0397645\n",
      "Train loss at epoch (909):  0.07769783\n",
      "Test loss at epoch (909):  0.039897647\n",
      "Train loss at epoch (910):  0.077144176\n",
      "Test loss at epoch (910):  0.03995759\n",
      "Train loss at epoch (911):  0.07757818\n",
      "Test loss at epoch (911):  0.039444588\n",
      "Train loss at epoch (912):  0.07752001\n",
      "Test loss at epoch (912):  0.03969349\n",
      "Train loss at epoch (913):  0.077347636\n",
      "Test loss at epoch (913):  0.039450135\n",
      "Train loss at epoch (914):  0.07711014\n",
      "Test loss at epoch (914):  0.03925153\n",
      "Train loss at epoch (915):  0.07634683\n",
      "Test loss at epoch (915):  0.039489597\n",
      "Train loss at epoch (916):  0.07750031\n",
      "Test loss at epoch (916):  0.03928116\n",
      "Train loss at epoch (917):  0.07669379\n",
      "Test loss at epoch (917):  0.038628154\n",
      "Train loss at epoch (918):  0.07739948\n",
      "Test loss at epoch (918):  0.039202757\n",
      "Train loss at epoch (919):  0.07720034\n",
      "Test loss at epoch (919):  0.039498575\n",
      "Train loss at epoch (920):  0.07669562\n",
      "Test loss at epoch (920):  0.039611124\n",
      "Train loss at epoch (921):  0.077019185\n",
      "Test loss at epoch (921):  0.039065406\n",
      "Train loss at epoch (922):  0.077087566\n",
      "Test loss at epoch (922):  0.038998663\n",
      "Train loss at epoch (923):  0.07720448\n",
      "Test loss at epoch (923):  0.039232537\n",
      "Train loss at epoch (924):  0.07669164\n",
      "Test loss at epoch (924):  0.03913978\n",
      "Train loss at epoch (925):  0.077172786\n",
      "Test loss at epoch (925):  0.039503213\n",
      "Train loss at epoch (926):  0.07722329\n",
      "Test loss at epoch (926):  0.03864315\n",
      "Train loss at epoch (927):  0.07674802\n",
      "Test loss at epoch (927):  0.03935254\n",
      "Train loss at epoch (928):  0.07699993\n",
      "Test loss at epoch (928):  0.04013126\n",
      "Train loss at epoch (929):  0.07678035\n",
      "Test loss at epoch (929):  0.03882213\n",
      "Train loss at epoch (930):  0.07675426\n",
      "Test loss at epoch (930):  0.039614536\n",
      "Train loss at epoch (931):  0.076689355\n",
      "Test loss at epoch (931):  0.039171748\n",
      "Train loss at epoch (932):  0.076786295\n",
      "Test loss at epoch (932):  0.039275393\n",
      "Train loss at epoch (933):  0.076516874\n",
      "Test loss at epoch (933):  0.039485924\n",
      "Train loss at epoch (934):  0.07652207\n",
      "Test loss at epoch (934):  0.038886797\n",
      "Train loss at epoch (935):  0.07653743\n",
      "Test loss at epoch (935):  0.03940066\n",
      "Train loss at epoch (936):  0.07726069\n",
      "Test loss at epoch (936):  0.03916724\n",
      "Train loss at epoch (937):  0.07639856\n",
      "Test loss at epoch (937):  0.040042717\n",
      "Train loss at epoch (938):  0.07670401\n",
      "Test loss at epoch (938):  0.03980271\n",
      "Train loss at epoch (939):  0.076926984\n",
      "Test loss at epoch (939):  0.03929722\n",
      "Train loss at epoch (940):  0.076624915\n",
      "Test loss at epoch (940):  0.038920946\n",
      "Train loss at epoch (941):  0.07639507\n",
      "Test loss at epoch (941):  0.038945917\n",
      "Train loss at epoch (942):  0.07658273\n",
      "Test loss at epoch (942):  0.03967817\n",
      "Train loss at epoch (943):  0.076979026\n",
      "Test loss at epoch (943):  0.03933516\n",
      "Train loss at epoch (944):  0.076542445\n",
      "Test loss at epoch (944):  0.03874792\n",
      "Train loss at epoch (945):  0.076185025\n",
      "Test loss at epoch (945):  0.03912519\n",
      "Train loss at epoch (946):  0.076425806\n",
      "Test loss at epoch (946):  0.039003037\n",
      "Train loss at epoch (947):  0.0767435\n",
      "Test loss at epoch (947):  0.039265312\n",
      "Train loss at epoch (948):  0.07597755\n",
      "Test loss at epoch (948):  0.039494563\n",
      "Train loss at epoch (949):  0.07648819\n",
      "Test loss at epoch (949):  0.03954705\n",
      "Train loss at epoch (950):  0.07665426\n",
      "Test loss at epoch (950):  0.03922908\n",
      "Train loss at epoch (951):  0.07660655\n",
      "Test loss at epoch (951):  0.03929278\n",
      "Train loss at epoch (952):  0.076488726\n",
      "Test loss at epoch (952):  0.039096963\n",
      "Train loss at epoch (953):  0.07668176\n",
      "Test loss at epoch (953):  0.03900296\n",
      "Train loss at epoch (954):  0.076006\n",
      "Test loss at epoch (954):  0.039775573\n",
      "Train loss at epoch (955):  0.07709023\n",
      "Test loss at epoch (955):  0.03889899\n",
      "Train loss at epoch (956):  0.07634872\n",
      "Test loss at epoch (956):  0.03908275\n",
      "Train loss at epoch (957):  0.076712005\n",
      "Test loss at epoch (957):  0.038184557\n",
      "Train loss at epoch (958):  0.07628935\n",
      "Test loss at epoch (958):  0.038962543\n",
      "Train loss at epoch (959):  0.07654632\n",
      "Test loss at epoch (959):  0.039637193\n",
      "Train loss at epoch (960):  0.07685011\n",
      "Test loss at epoch (960):  0.038347352\n",
      "Train loss at epoch (961):  0.076490335\n",
      "Test loss at epoch (961):  0.03826527\n",
      "Train loss at epoch (962):  0.07676976\n",
      "Test loss at epoch (962):  0.039207928\n",
      "Train loss at epoch (963):  0.07672978\n",
      "Test loss at epoch (963):  0.039066695\n",
      "Train loss at epoch (964):  0.076394066\n",
      "Test loss at epoch (964):  0.039146207\n",
      "Train loss at epoch (965):  0.07635917\n",
      "Test loss at epoch (965):  0.0387076\n",
      "Train loss at epoch (966):  0.075957745\n",
      "Test loss at epoch (966):  0.039344557\n",
      "Train loss at epoch (967):  0.07639626\n",
      "Test loss at epoch (967):  0.039348707\n",
      "Train loss at epoch (968):  0.07622674\n",
      "Test loss at epoch (968):  0.038731992\n",
      "Train loss at epoch (969):  0.0761295\n",
      "Test loss at epoch (969):  0.038474407\n",
      "Train loss at epoch (970):  0.07677358\n",
      "Test loss at epoch (970):  0.039038498\n",
      "Train loss at epoch (971):  0.07616901\n",
      "Test loss at epoch (971):  0.039018944\n",
      "Train loss at epoch (972):  0.076421596\n",
      "Test loss at epoch (972):  0.038826328\n",
      "Train loss at epoch (973):  0.07614791\n",
      "Test loss at epoch (973):  0.038982473\n",
      "Train loss at epoch (974):  0.07619846\n",
      "Test loss at epoch (974):  0.03856043\n",
      "Train loss at epoch (975):  0.076373704\n",
      "Test loss at epoch (975):  0.039377168\n",
      "Train loss at epoch (976):  0.07596205\n",
      "Test loss at epoch (976):  0.039727937\n",
      "Train loss at epoch (977):  0.07619564\n",
      "Test loss at epoch (977):  0.039720815\n",
      "Train loss at epoch (978):  0.076223426\n",
      "Test loss at epoch (978):  0.03856631\n",
      "Train loss at epoch (979):  0.07607352\n",
      "Test loss at epoch (979):  0.039145052\n",
      "Train loss at epoch (980):  0.07594204\n",
      "Test loss at epoch (980):  0.0394285\n",
      "Train loss at epoch (981):  0.07581206\n",
      "Test loss at epoch (981):  0.0392174\n",
      "Train loss at epoch (982):  0.07625637\n",
      "Test loss at epoch (982):  0.039328054\n",
      "Train loss at epoch (983):  0.075971745\n",
      "Test loss at epoch (983):  0.03875561\n",
      "Train loss at epoch (984):  0.07609696\n",
      "Test loss at epoch (984):  0.038979404\n",
      "Train loss at epoch (985):  0.076184295\n",
      "Test loss at epoch (985):  0.03871168\n",
      "Train loss at epoch (986):  0.07598135\n",
      "Test loss at epoch (986):  0.03856212\n",
      "Train loss at epoch (987):  0.07644448\n",
      "Test loss at epoch (987):  0.037723593\n",
      "Train loss at epoch (988):  0.0766388\n",
      "Test loss at epoch (988):  0.03929381\n",
      "Train loss at epoch (989):  0.0763224\n",
      "Test loss at epoch (989):  0.038715117\n",
      "Train loss at epoch (990):  0.075552665\n",
      "Test loss at epoch (990):  0.038643915\n",
      "Train loss at epoch (991):  0.07604892\n",
      "Test loss at epoch (991):  0.038666364\n",
      "Train loss at epoch (992):  0.076198705\n",
      "Test loss at epoch (992):  0.038559984\n",
      "Train loss at epoch (993):  0.07634188\n",
      "Test loss at epoch (993):  0.039188284\n",
      "Train loss at epoch (994):  0.07531243\n",
      "Test loss at epoch (994):  0.039079502\n",
      "Train loss at epoch (995):  0.07587979\n",
      "Test loss at epoch (995):  0.038757667\n",
      "Train loss at epoch (996):  0.0755701\n",
      "Test loss at epoch (996):  0.039355032\n",
      "Train loss at epoch (997):  0.076089114\n",
      "Test loss at epoch (997):  0.039028086\n",
      "Train loss at epoch (998):  0.07569312\n",
      "Test loss at epoch (998):  0.0385418\n",
      "Train loss at epoch (999):  0.07596217\n",
      "Test loss at epoch (999):  0.03825771\n",
      "Train loss at epoch (1000):  0.075939886\n",
      "Test loss at epoch (1000):  0.0398434\n",
      "Train loss at epoch (1001):  0.07575258\n",
      "Test loss at epoch (1001):  0.03884604\n",
      "Train loss at epoch (1002):  0.075742275\n",
      "Test loss at epoch (1002):  0.039056405\n",
      "Train loss at epoch (1003):  0.075777985\n",
      "Test loss at epoch (1003):  0.03930762\n",
      "Train loss at epoch (1004):  0.07597213\n",
      "Test loss at epoch (1004):  0.038531426\n",
      "Train loss at epoch (1005):  0.075688735\n",
      "Test loss at epoch (1005):  0.03848499\n",
      "Train loss at epoch (1006):  0.07625147\n",
      "Test loss at epoch (1006):  0.039440617\n",
      "Train loss at epoch (1007):  0.07601815\n",
      "Test loss at epoch (1007):  0.03865281\n",
      "Train loss at epoch (1008):  0.0754438\n",
      "Test loss at epoch (1008):  0.03926086\n",
      "Train loss at epoch (1009):  0.07599445\n",
      "Test loss at epoch (1009):  0.03879166\n",
      "Train loss at epoch (1010):  0.07639694\n",
      "Test loss at epoch (1010):  0.0388742\n",
      "Train loss at epoch (1011):  0.07616559\n",
      "Test loss at epoch (1011):  0.037825026\n",
      "Train loss at epoch (1012):  0.075491846\n",
      "Test loss at epoch (1012):  0.038539886\n",
      "Train loss at epoch (1013):  0.0752527\n",
      "Test loss at epoch (1013):  0.038428836\n",
      "Train loss at epoch (1014):  0.076304264\n",
      "Test loss at epoch (1014):  0.03908541\n",
      "Train loss at epoch (1015):  0.07610346\n",
      "Test loss at epoch (1015):  0.03849466\n",
      "Train loss at epoch (1016):  0.0761247\n",
      "Test loss at epoch (1016):  0.038966022\n",
      "Train loss at epoch (1017):  0.075369366\n",
      "Test loss at epoch (1017):  0.038617197\n",
      "Train loss at epoch (1018):  0.07581384\n",
      "Test loss at epoch (1018):  0.03932462\n",
      "Train loss at epoch (1019):  0.07513278\n",
      "Test loss at epoch (1019):  0.038744263\n",
      "Train loss at epoch (1020):  0.07602347\n",
      "Test loss at epoch (1020):  0.03904436\n",
      "Train loss at epoch (1021):  0.0757578\n",
      "Test loss at epoch (1021):  0.03820627\n",
      "Train loss at epoch (1022):  0.07601252\n",
      "Test loss at epoch (1022):  0.03914502\n",
      "Train loss at epoch (1023):  0.07579665\n",
      "Test loss at epoch (1023):  0.038304612\n",
      "Train loss at epoch (1024):  0.07632895\n",
      "Test loss at epoch (1024):  0.03808753\n",
      "Train loss at epoch (1025):  0.076244205\n",
      "Test loss at epoch (1025):  0.038062654\n",
      "Train loss at epoch (1026):  0.07598743\n",
      "Test loss at epoch (1026):  0.03887802\n",
      "Train loss at epoch (1027):  0.0754701\n",
      "Test loss at epoch (1027):  0.03863096\n",
      "Train loss at epoch (1028):  0.07538966\n",
      "Test loss at epoch (1028):  0.03752169\n",
      "Train loss at epoch (1029):  0.07591843\n",
      "Test loss at epoch (1029):  0.03919175\n",
      "Train loss at epoch (1030):  0.075626336\n",
      "Test loss at epoch (1030):  0.038626097\n",
      "Train loss at epoch (1031):  0.075259976\n",
      "Test loss at epoch (1031):  0.038339254\n",
      "Train loss at epoch (1032):  0.0752636\n",
      "Test loss at epoch (1032):  0.03859155\n",
      "Train loss at epoch (1033):  0.07629189\n",
      "Test loss at epoch (1033):  0.038253773\n",
      "Train loss at epoch (1034):  0.075913414\n",
      "Test loss at epoch (1034):  0.03757128\n",
      "Train loss at epoch (1035):  0.075213745\n",
      "Test loss at epoch (1035):  0.038698547\n",
      "Train loss at epoch (1036):  0.07593937\n",
      "Test loss at epoch (1036):  0.039228536\n",
      "Train loss at epoch (1037):  0.075737745\n",
      "Test loss at epoch (1037):  0.038500927\n",
      "Train loss at epoch (1038):  0.07601097\n",
      "Test loss at epoch (1038):  0.039143287\n",
      "Train loss at epoch (1039):  0.07553139\n",
      "Test loss at epoch (1039):  0.03816417\n",
      "Train loss at epoch (1040):  0.075679176\n",
      "Test loss at epoch (1040):  0.038430467\n",
      "Train loss at epoch (1041):  0.075847164\n",
      "Test loss at epoch (1041):  0.03944579\n",
      "Train loss at epoch (1042):  0.075648144\n",
      "Test loss at epoch (1042):  0.03769441\n",
      "Train loss at epoch (1043):  0.07534269\n",
      "Test loss at epoch (1043):  0.038450204\n",
      "Train loss at epoch (1044):  0.075270124\n",
      "Test loss at epoch (1044):  0.03888995\n",
      "Train loss at epoch (1045):  0.07534442\n",
      "Test loss at epoch (1045):  0.038336493\n",
      "Train loss at epoch (1046):  0.075128764\n",
      "Test loss at epoch (1046):  0.03825927\n",
      "Train loss at epoch (1047):  0.07534618\n",
      "Test loss at epoch (1047):  0.0381526\n",
      "Train loss at epoch (1048):  0.07535735\n",
      "Test loss at epoch (1048):  0.03782149\n",
      "Train loss at epoch (1049):  0.075089954\n",
      "Test loss at epoch (1049):  0.038506735\n",
      "Train loss at epoch (1050):  0.075420074\n",
      "Test loss at epoch (1050):  0.03803893\n",
      "Train loss at epoch (1051):  0.0757053\n",
      "Test loss at epoch (1051):  0.0391814\n",
      "Train loss at epoch (1052):  0.0745058\n",
      "Test loss at epoch (1052):  0.038698047\n",
      "Train loss at epoch (1053):  0.0755371\n",
      "Test loss at epoch (1053):  0.039082646\n",
      "Train loss at epoch (1054):  0.07556628\n",
      "Test loss at epoch (1054):  0.038489584\n",
      "Train loss at epoch (1055):  0.075563386\n",
      "Test loss at epoch (1055):  0.03801057\n",
      "Train loss at epoch (1056):  0.075399004\n",
      "Test loss at epoch (1056):  0.038680162\n",
      "Train loss at epoch (1057):  0.07545735\n",
      "Test loss at epoch (1057):  0.038130622\n",
      "Train loss at epoch (1058):  0.075345226\n",
      "Test loss at epoch (1058):  0.03839893\n",
      "Train loss at epoch (1059):  0.07531424\n",
      "Test loss at epoch (1059):  0.038591944\n",
      "Train loss at epoch (1060):  0.0756073\n",
      "Test loss at epoch (1060):  0.038808037\n",
      "Train loss at epoch (1061):  0.075610235\n",
      "Test loss at epoch (1061):  0.038287487\n",
      "Train loss at epoch (1062):  0.07509719\n",
      "Test loss at epoch (1062):  0.037935965\n",
      "Train loss at epoch (1063):  0.07579858\n",
      "Test loss at epoch (1063):  0.037953667\n",
      "Train loss at epoch (1064):  0.07562621\n",
      "Test loss at epoch (1064):  0.037849966\n",
      "Train loss at epoch (1065):  0.07555086\n",
      "Test loss at epoch (1065):  0.038232606\n",
      "Train loss at epoch (1066):  0.07503237\n",
      "Test loss at epoch (1066):  0.037953846\n",
      "Train loss at epoch (1067):  0.075207815\n",
      "Test loss at epoch (1067):  0.038097605\n",
      "Train loss at epoch (1068):  0.07550624\n",
      "Test loss at epoch (1068):  0.03875571\n",
      "Train loss at epoch (1069):  0.07509932\n",
      "Test loss at epoch (1069):  0.038640603\n",
      "Train loss at epoch (1070):  0.0753134\n",
      "Test loss at epoch (1070):  0.03862388\n",
      "Train loss at epoch (1071):  0.075760394\n",
      "Test loss at epoch (1071):  0.038340688\n",
      "Train loss at epoch (1072):  0.0753483\n",
      "Test loss at epoch (1072):  0.038608477\n",
      "Train loss at epoch (1073):  0.075210385\n",
      "Test loss at epoch (1073):  0.03770135\n",
      "Train loss at epoch (1074):  0.07510546\n",
      "Test loss at epoch (1074):  0.03833901\n",
      "Train loss at epoch (1075):  0.07577725\n",
      "Test loss at epoch (1075):  0.038451284\n",
      "Train loss at epoch (1076):  0.07486263\n",
      "Test loss at epoch (1076):  0.038612098\n",
      "Train loss at epoch (1077):  0.075671345\n",
      "Test loss at epoch (1077):  0.03784555\n",
      "Train loss at epoch (1078):  0.07519234\n",
      "Test loss at epoch (1078):  0.03766989\n",
      "Train loss at epoch (1079):  0.075025335\n",
      "Test loss at epoch (1079):  0.037891753\n",
      "Train loss at epoch (1080):  0.075339116\n",
      "Test loss at epoch (1080):  0.038786806\n",
      "Train loss at epoch (1081):  0.07533825\n",
      "Test loss at epoch (1081):  0.037933312\n",
      "Train loss at epoch (1082):  0.07515882\n",
      "Test loss at epoch (1082):  0.038246445\n",
      "Train loss at epoch (1083):  0.07539392\n",
      "Test loss at epoch (1083):  0.038498476\n",
      "Train loss at epoch (1084):  0.07540519\n",
      "Test loss at epoch (1084):  0.038346775\n",
      "Train loss at epoch (1085):  0.07532312\n",
      "Test loss at epoch (1085):  0.0389432\n",
      "Train loss at epoch (1086):  0.07520492\n",
      "Test loss at epoch (1086):  0.038475562\n",
      "Train loss at epoch (1087):  0.07497453\n",
      "Test loss at epoch (1087):  0.038282488\n",
      "Train loss at epoch (1088):  0.07559936\n",
      "Test loss at epoch (1088):  0.039011072\n",
      "Train loss at epoch (1089):  0.07476104\n",
      "Test loss at epoch (1089):  0.03842237\n",
      "Train loss at epoch (1090):  0.07535561\n",
      "Test loss at epoch (1090):  0.038271874\n",
      "Train loss at epoch (1091):  0.07516807\n",
      "Test loss at epoch (1091):  0.03834837\n",
      "Train loss at epoch (1092):  0.07490247\n",
      "Test loss at epoch (1092):  0.03770467\n",
      "Train loss at epoch (1093):  0.075610474\n",
      "Test loss at epoch (1093):  0.038352262\n",
      "Train loss at epoch (1094):  0.075456545\n",
      "Test loss at epoch (1094):  0.0387249\n",
      "Train loss at epoch (1095):  0.07495938\n",
      "Test loss at epoch (1095):  0.03800558\n",
      "Train loss at epoch (1096):  0.07536594\n",
      "Test loss at epoch (1096):  0.03858256\n",
      "Train loss at epoch (1097):  0.07532469\n",
      "Test loss at epoch (1097):  0.038222983\n",
      "Train loss at epoch (1098):  0.074914224\n",
      "Test loss at epoch (1098):  0.0380549\n",
      "Train loss at epoch (1099):  0.07509882\n",
      "Test loss at epoch (1099):  0.038051356\n",
      "Train loss at epoch (1100):  0.07491554\n",
      "Test loss at epoch (1100):  0.03786759\n",
      "Train loss at epoch (1101):  0.07515585\n",
      "Test loss at epoch (1101):  0.038483057\n",
      "Train loss at epoch (1102):  0.07508164\n",
      "Test loss at epoch (1102):  0.03812611\n",
      "Train loss at epoch (1103):  0.07503941\n",
      "Test loss at epoch (1103):  0.03772755\n",
      "Train loss at epoch (1104):  0.07502481\n",
      "Test loss at epoch (1104):  0.03851432\n",
      "Train loss at epoch (1105):  0.07511494\n",
      "Test loss at epoch (1105):  0.038890615\n",
      "Train loss at epoch (1106):  0.074857004\n",
      "Test loss at epoch (1106):  0.037707396\n",
      "Train loss at epoch (1107):  0.075113416\n",
      "Test loss at epoch (1107):  0.03823589\n",
      "Train loss at epoch (1108):  0.07497627\n",
      "Test loss at epoch (1108):  0.037968256\n",
      "Train loss at epoch (1109):  0.07469089\n",
      "Test loss at epoch (1109):  0.03819232\n",
      "Train loss at epoch (1110):  0.07521929\n",
      "Test loss at epoch (1110):  0.037855726\n",
      "Train loss at epoch (1111):  0.07494418\n",
      "Test loss at epoch (1111):  0.03851292\n",
      "Train loss at epoch (1112):  0.07551424\n",
      "Test loss at epoch (1112):  0.03851551\n",
      "Train loss at epoch (1113):  0.07508512\n",
      "Test loss at epoch (1113):  0.03784207\n",
      "Train loss at epoch (1114):  0.0747861\n",
      "Test loss at epoch (1114):  0.038969103\n",
      "Train loss at epoch (1115):  0.07506997\n",
      "Test loss at epoch (1115):  0.037555374\n",
      "Train loss at epoch (1116):  0.075045004\n",
      "Test loss at epoch (1116):  0.037816316\n",
      "Train loss at epoch (1117):  0.07489158\n",
      "Test loss at epoch (1117):  0.038137853\n",
      "Train loss at epoch (1118):  0.074955836\n",
      "Test loss at epoch (1118):  0.038024828\n",
      "Train loss at epoch (1119):  0.07487093\n",
      "Test loss at epoch (1119):  0.037034135\n",
      "Train loss at epoch (1120):  0.07501411\n",
      "Test loss at epoch (1120):  0.038308807\n",
      "Train loss at epoch (1121):  0.074923374\n",
      "Test loss at epoch (1121):  0.03842984\n",
      "Train loss at epoch (1122):  0.075240985\n",
      "Test loss at epoch (1122):  0.037496254\n",
      "Train loss at epoch (1123):  0.074981205\n",
      "Test loss at epoch (1123):  0.037921935\n",
      "Train loss at epoch (1124):  0.07532746\n",
      "Test loss at epoch (1124):  0.038259808\n",
      "Train loss at epoch (1125):  0.07478028\n",
      "Test loss at epoch (1125):  0.03800232\n",
      "Train loss at epoch (1126):  0.07511814\n",
      "Test loss at epoch (1126):  0.03775179\n",
      "Train loss at epoch (1127):  0.07505541\n",
      "Test loss at epoch (1127):  0.0375453\n",
      "Train loss at epoch (1128):  0.074868955\n",
      "Test loss at epoch (1128):  0.038012244\n",
      "Train loss at epoch (1129):  0.0752033\n",
      "Test loss at epoch (1129):  0.038125\n",
      "Train loss at epoch (1130):  0.07535259\n",
      "Test loss at epoch (1130):  0.03779848\n",
      "Train loss at epoch (1131):  0.07488366\n",
      "Test loss at epoch (1131):  0.037255403\n",
      "Train loss at epoch (1132):  0.07523591\n",
      "Test loss at epoch (1132):  0.03797279\n",
      "Train loss at epoch (1133):  0.0748008\n",
      "Test loss at epoch (1133):  0.037707586\n",
      "Train loss at epoch (1134):  0.075220466\n",
      "Test loss at epoch (1134):  0.03838104\n",
      "Train loss at epoch (1135):  0.074466266\n",
      "Test loss at epoch (1135):  0.037918366\n",
      "Train loss at epoch (1136):  0.074834116\n",
      "Test loss at epoch (1136):  0.03783296\n",
      "Train loss at epoch (1137):  0.07532335\n",
      "Test loss at epoch (1137):  0.03842794\n",
      "Train loss at epoch (1138):  0.074588016\n",
      "Test loss at epoch (1138):  0.038215958\n",
      "Train loss at epoch (1139):  0.074799165\n",
      "Test loss at epoch (1139):  0.037638657\n",
      "Train loss at epoch (1140):  0.075081006\n",
      "Test loss at epoch (1140):  0.03838676\n",
      "Train loss at epoch (1141):  0.07498525\n",
      "Test loss at epoch (1141):  0.038076006\n",
      "Train loss at epoch (1142):  0.07554005\n",
      "Test loss at epoch (1142):  0.03787701\n",
      "Train loss at epoch (1143):  0.074332565\n",
      "Test loss at epoch (1143):  0.03800507\n",
      "Train loss at epoch (1144):  0.07472263\n",
      "Test loss at epoch (1144):  0.037403602\n",
      "Train loss at epoch (1145):  0.075074874\n",
      "Test loss at epoch (1145):  0.037492704\n",
      "Train loss at epoch (1146):  0.074892364\n",
      "Test loss at epoch (1146):  0.037674546\n",
      "Train loss at epoch (1147):  0.07522103\n",
      "Test loss at epoch (1147):  0.037547253\n",
      "Train loss at epoch (1148):  0.07448557\n",
      "Test loss at epoch (1148):  0.037758417\n",
      "Train loss at epoch (1149):  0.07491902\n",
      "Test loss at epoch (1149):  0.038129017\n",
      "Train loss at epoch (1150):  0.07529192\n",
      "Test loss at epoch (1150):  0.037946604\n",
      "Train loss at epoch (1151):  0.07430449\n",
      "Test loss at epoch (1151):  0.037598956\n",
      "Train loss at epoch (1152):  0.07476395\n",
      "Test loss at epoch (1152):  0.038322393\n",
      "Train loss at epoch (1153):  0.075045176\n",
      "Test loss at epoch (1153):  0.03753055\n",
      "Train loss at epoch (1154):  0.07470147\n",
      "Test loss at epoch (1154):  0.037711985\n",
      "Train loss at epoch (1155):  0.07492237\n",
      "Test loss at epoch (1155):  0.037477672\n",
      "Train loss at epoch (1156):  0.07487178\n",
      "Test loss at epoch (1156):  0.037499063\n",
      "Train loss at epoch (1157):  0.074932925\n",
      "Test loss at epoch (1157):  0.037760455\n",
      "Train loss at epoch (1158):  0.07462634\n",
      "Test loss at epoch (1158):  0.037841707\n",
      "Train loss at epoch (1159):  0.07513586\n",
      "Test loss at epoch (1159):  0.038220003\n",
      "Train loss at epoch (1160):  0.07504713\n",
      "Test loss at epoch (1160):  0.037427202\n",
      "Train loss at epoch (1161):  0.07542246\n",
      "Test loss at epoch (1161):  0.0385507\n",
      "Train loss at epoch (1162):  0.07489991\n",
      "Test loss at epoch (1162):  0.037239134\n",
      "Train loss at epoch (1163):  0.07477954\n",
      "Test loss at epoch (1163):  0.038128722\n",
      "Train loss at epoch (1164):  0.07463259\n",
      "Test loss at epoch (1164):  0.0378486\n",
      "Train loss at epoch (1165):  0.07448642\n",
      "Test loss at epoch (1165):  0.037709977\n",
      "Train loss at epoch (1166):  0.075182326\n",
      "Test loss at epoch (1166):  0.037508704\n",
      "Train loss at epoch (1167):  0.07461077\n",
      "Test loss at epoch (1167):  0.037675757\n",
      "Train loss at epoch (1168):  0.07507861\n",
      "Test loss at epoch (1168):  0.03819704\n",
      "Train loss at epoch (1169):  0.07502236\n",
      "Test loss at epoch (1169):  0.037808206\n",
      "Train loss at epoch (1170):  0.07484741\n",
      "Test loss at epoch (1170):  0.037371118\n",
      "Train loss at epoch (1171):  0.07560024\n",
      "Test loss at epoch (1171):  0.037331875\n",
      "Train loss at epoch (1172):  0.074729554\n",
      "Test loss at epoch (1172):  0.03748853\n",
      "Train loss at epoch (1173):  0.074755296\n",
      "Test loss at epoch (1173):  0.037282556\n",
      "Train loss at epoch (1174):  0.074527174\n",
      "Test loss at epoch (1174):  0.03682375\n",
      "Train loss at epoch (1175):  0.0740652\n",
      "Test loss at epoch (1175):  0.038211305\n",
      "Train loss at epoch (1176):  0.0749277\n",
      "Test loss at epoch (1176):  0.037696093\n",
      "Train loss at epoch (1177):  0.07461867\n",
      "Test loss at epoch (1177):  0.037803903\n",
      "Train loss at epoch (1178):  0.074756786\n",
      "Test loss at epoch (1178):  0.038108833\n",
      "Train loss at epoch (1179):  0.074410155\n",
      "Test loss at epoch (1179):  0.038718928\n",
      "Train loss at epoch (1180):  0.07442761\n",
      "Test loss at epoch (1180):  0.036782496\n",
      "Train loss at epoch (1181):  0.074504375\n",
      "Test loss at epoch (1181):  0.036944654\n",
      "Train loss at epoch (1182):  0.07486761\n",
      "Test loss at epoch (1182):  0.03745584\n",
      "Train loss at epoch (1183):  0.07484757\n",
      "Test loss at epoch (1183):  0.037373096\n",
      "Train loss at epoch (1184):  0.07409394\n",
      "Test loss at epoch (1184):  0.037894435\n",
      "Train loss at epoch (1185):  0.07480051\n",
      "Test loss at epoch (1185):  0.037805554\n",
      "Train loss at epoch (1186):  0.074847125\n",
      "Test loss at epoch (1186):  0.037686385\n",
      "Train loss at epoch (1187):  0.0748797\n",
      "Test loss at epoch (1187):  0.03792236\n",
      "Train loss at epoch (1188):  0.074121244\n",
      "Test loss at epoch (1188):  0.037142467\n",
      "Train loss at epoch (1189):  0.07481163\n",
      "Test loss at epoch (1189):  0.037893966\n",
      "Train loss at epoch (1190):  0.07486426\n",
      "Test loss at epoch (1190):  0.038881313\n",
      "Train loss at epoch (1191):  0.07454466\n",
      "Test loss at epoch (1191):  0.037260085\n",
      "Train loss at epoch (1192):  0.075030096\n",
      "Test loss at epoch (1192):  0.037618764\n",
      "Train loss at epoch (1193):  0.07461988\n",
      "Test loss at epoch (1193):  0.038422514\n",
      "Train loss at epoch (1194):  0.074727036\n",
      "Test loss at epoch (1194):  0.037486926\n",
      "Train loss at epoch (1195):  0.07423825\n",
      "Test loss at epoch (1195):  0.037296996\n",
      "Train loss at epoch (1196):  0.07477575\n",
      "Test loss at epoch (1196):  0.037764255\n",
      "Train loss at epoch (1197):  0.07451819\n",
      "Test loss at epoch (1197):  0.038230777\n",
      "Train loss at epoch (1198):  0.07419673\n",
      "Test loss at epoch (1198):  0.037709173\n",
      "Train loss at epoch (1199):  0.073939525\n",
      "Test loss at epoch (1199):  0.03795581\n",
      "Train loss at epoch (1200):  0.07466339\n",
      "Test loss at epoch (1200):  0.037834298\n",
      "Train loss at epoch (1201):  0.074426405\n",
      "Test loss at epoch (1201):  0.038306225\n",
      "Train loss at epoch (1202):  0.07480245\n",
      "Test loss at epoch (1202):  0.038043283\n",
      "Train loss at epoch (1203):  0.07471175\n",
      "Test loss at epoch (1203):  0.037986655\n",
      "Train loss at epoch (1204):  0.07490749\n",
      "Test loss at epoch (1204):  0.037750777\n",
      "Train loss at epoch (1205):  0.074418195\n",
      "Test loss at epoch (1205):  0.03781558\n",
      "Train loss at epoch (1206):  0.074062854\n",
      "Test loss at epoch (1206):  0.03722302\n",
      "Train loss at epoch (1207):  0.07430954\n",
      "Test loss at epoch (1207):  0.037787016\n",
      "Train loss at epoch (1208):  0.074710496\n",
      "Test loss at epoch (1208):  0.038093857\n",
      "Train loss at epoch (1209):  0.074508965\n",
      "Test loss at epoch (1209):  0.038329393\n",
      "Train loss at epoch (1210):  0.07409709\n",
      "Test loss at epoch (1210):  0.036794692\n",
      "Train loss at epoch (1211):  0.07453171\n",
      "Test loss at epoch (1211):  0.036965005\n",
      "Train loss at epoch (1212):  0.07460678\n",
      "Test loss at epoch (1212):  0.0373071\n",
      "Train loss at epoch (1213):  0.07485369\n",
      "Test loss at epoch (1213):  0.037294526\n",
      "Train loss at epoch (1214):  0.07494022\n",
      "Test loss at epoch (1214):  0.036989667\n",
      "Train loss at epoch (1215):  0.07433379\n",
      "Test loss at epoch (1215):  0.037503168\n",
      "Train loss at epoch (1216):  0.074814335\n",
      "Test loss at epoch (1216):  0.037621234\n",
      "Train loss at epoch (1217):  0.07483398\n",
      "Test loss at epoch (1217):  0.038398392\n",
      "Train loss at epoch (1218):  0.07461195\n",
      "Test loss at epoch (1218):  0.03814108\n",
      "Train loss at epoch (1219):  0.074727796\n",
      "Test loss at epoch (1219):  0.037769165\n",
      "Train loss at epoch (1220):  0.07454327\n",
      "Test loss at epoch (1220):  0.03705294\n",
      "Train loss at epoch (1221):  0.074719235\n",
      "Test loss at epoch (1221):  0.037772793\n",
      "Train loss at epoch (1222):  0.07513396\n",
      "Test loss at epoch (1222):  0.037041057\n",
      "Train loss at epoch (1223):  0.07462446\n",
      "Test loss at epoch (1223):  0.03741507\n",
      "Train loss at epoch (1224):  0.074252166\n",
      "Test loss at epoch (1224):  0.038020946\n",
      "Train loss at epoch (1225):  0.07464454\n",
      "Test loss at epoch (1225):  0.037936334\n",
      "Train loss at epoch (1226):  0.07408929\n",
      "Test loss at epoch (1226):  0.038386073\n",
      "Train loss at epoch (1227):  0.07430819\n",
      "Test loss at epoch (1227):  0.037869483\n",
      "Train loss at epoch (1228):  0.07429601\n",
      "Test loss at epoch (1228):  0.0377452\n",
      "Train loss at epoch (1229):  0.07487486\n",
      "Test loss at epoch (1229):  0.03785348\n",
      "Train loss at epoch (1230):  0.074230626\n",
      "Test loss at epoch (1230):  0.03769405\n",
      "Train loss at epoch (1231):  0.07440284\n",
      "Test loss at epoch (1231):  0.03794524\n",
      "Train loss at epoch (1232):  0.074767716\n",
      "Test loss at epoch (1232):  0.037701864\n",
      "Train loss at epoch (1233):  0.07490142\n",
      "Test loss at epoch (1233):  0.036918145\n",
      "Train loss at epoch (1234):  0.07403551\n",
      "Test loss at epoch (1234):  0.037012886\n",
      "Train loss at epoch (1235):  0.07444348\n",
      "Test loss at epoch (1235):  0.037714455\n",
      "Train loss at epoch (1236):  0.07421703\n",
      "Test loss at epoch (1236):  0.03747832\n",
      "Train loss at epoch (1237):  0.074414104\n",
      "Test loss at epoch (1237):  0.0375497\n",
      "Train loss at epoch (1238):  0.07501885\n",
      "Test loss at epoch (1238):  0.03796219\n",
      "Train loss at epoch (1239):  0.07478433\n",
      "Test loss at epoch (1239):  0.03735297\n",
      "Train loss at epoch (1240):  0.074399665\n",
      "Test loss at epoch (1240):  0.037684377\n",
      "Train loss at epoch (1241):  0.07460413\n",
      "Test loss at epoch (1241):  0.037129935\n",
      "Train loss at epoch (1242):  0.07404272\n",
      "Test loss at epoch (1242):  0.03707601\n",
      "Train loss at epoch (1243):  0.07403994\n",
      "Test loss at epoch (1243):  0.03746932\n",
      "Train loss at epoch (1244):  0.074326165\n",
      "Test loss at epoch (1244):  0.03825973\n",
      "Train loss at epoch (1245):  0.074179545\n",
      "Test loss at epoch (1245):  0.038213275\n",
      "Train loss at epoch (1246):  0.07442038\n",
      "Test loss at epoch (1246):  0.037267152\n",
      "Train loss at epoch (1247):  0.07433328\n",
      "Test loss at epoch (1247):  0.03773264\n",
      "Train loss at epoch (1248):  0.07379942\n",
      "Test loss at epoch (1248):  0.037345022\n",
      "Train loss at epoch (1249):  0.07470915\n",
      "Test loss at epoch (1249):  0.037152532\n",
      "Train loss at epoch (1250):  0.07446688\n",
      "Test loss at epoch (1250):  0.037459046\n",
      "Train loss at epoch (1251):  0.07456409\n",
      "Test loss at epoch (1251):  0.03813813\n",
      "Train loss at epoch (1252):  0.074268684\n",
      "Test loss at epoch (1252):  0.038289927\n",
      "Train loss at epoch (1253):  0.07462064\n",
      "Test loss at epoch (1253):  0.037527002\n",
      "Train loss at epoch (1254):  0.07455549\n",
      "Test loss at epoch (1254):  0.038007688\n",
      "Train loss at epoch (1255):  0.07522605\n",
      "Test loss at epoch (1255):  0.03747134\n",
      "Train loss at epoch (1256):  0.07449838\n",
      "Test loss at epoch (1256):  0.03759969\n",
      "Train loss at epoch (1257):  0.07433682\n",
      "Test loss at epoch (1257):  0.037635528\n",
      "Train loss at epoch (1258):  0.0750548\n",
      "Test loss at epoch (1258):  0.03722592\n",
      "Train loss at epoch (1259):  0.074522935\n",
      "Test loss at epoch (1259):  0.037434783\n",
      "Train loss at epoch (1260):  0.07450335\n",
      "Test loss at epoch (1260):  0.038291246\n",
      "Train loss at epoch (1261):  0.07473834\n",
      "Test loss at epoch (1261):  0.037101526\n",
      "Train loss at epoch (1262):  0.074538976\n",
      "Test loss at epoch (1262):  0.036978845\n",
      "Train loss at epoch (1263):  0.07419588\n",
      "Test loss at epoch (1263):  0.0375238\n",
      "Train loss at epoch (1264):  0.07485836\n",
      "Test loss at epoch (1264):  0.037494063\n",
      "Train loss at epoch (1265):  0.07445427\n",
      "Test loss at epoch (1265):  0.037777275\n",
      "Train loss at epoch (1266):  0.074494645\n",
      "Test loss at epoch (1266):  0.0379204\n",
      "Train loss at epoch (1267):  0.07422889\n",
      "Test loss at epoch (1267):  0.03828786\n",
      "Train loss at epoch (1268):  0.07487643\n",
      "Test loss at epoch (1268):  0.03754117\n",
      "Train loss at epoch (1269):  0.0745645\n",
      "Test loss at epoch (1269):  0.037113734\n",
      "Train loss at epoch (1270):  0.07423857\n",
      "Test loss at epoch (1270):  0.03736048\n",
      "Train loss at epoch (1271):  0.07439627\n",
      "Test loss at epoch (1271):  0.037700422\n",
      "Train loss at epoch (1272):  0.074778855\n",
      "Test loss at epoch (1272):  0.037467834\n",
      "Train loss at epoch (1273):  0.07462005\n",
      "Test loss at epoch (1273):  0.0378999\n",
      "Train loss at epoch (1274):  0.07442446\n",
      "Test loss at epoch (1274):  0.037613206\n",
      "Train loss at epoch (1275):  0.07438242\n",
      "Test loss at epoch (1275):  0.037809208\n",
      "Train loss at epoch (1276):  0.07421074\n",
      "Test loss at epoch (1276):  0.036905024\n",
      "Train loss at epoch (1277):  0.07426049\n",
      "Test loss at epoch (1277):  0.037405513\n",
      "Train loss at epoch (1278):  0.07492461\n",
      "Test loss at epoch (1278):  0.038368534\n",
      "Train loss at epoch (1279):  0.074213795\n",
      "Test loss at epoch (1279):  0.037477136\n",
      "Train loss at epoch (1280):  0.07408477\n",
      "Test loss at epoch (1280):  0.03725312\n",
      "Train loss at epoch (1281):  0.07452942\n",
      "Test loss at epoch (1281):  0.038771875\n",
      "Train loss at epoch (1282):  0.074261\n",
      "Test loss at epoch (1282):  0.037617475\n",
      "Train loss at epoch (1283):  0.07404289\n",
      "Test loss at epoch (1283):  0.038148936\n",
      "Train loss at epoch (1284):  0.074313715\n",
      "Test loss at epoch (1284):  0.037924223\n",
      "Train loss at epoch (1285):  0.07394104\n",
      "Test loss at epoch (1285):  0.037091628\n",
      "Train loss at epoch (1286):  0.074301764\n",
      "Test loss at epoch (1286):  0.03736499\n",
      "Train loss at epoch (1287):  0.0735677\n",
      "Test loss at epoch (1287):  0.037055973\n",
      "Train loss at epoch (1288):  0.073947586\n",
      "Test loss at epoch (1288):  0.03785412\n",
      "Train loss at epoch (1289):  0.07400242\n",
      "Test loss at epoch (1289):  0.038064122\n",
      "Train loss at epoch (1290):  0.0747253\n",
      "Test loss at epoch (1290):  0.03731808\n",
      "Train loss at epoch (1291):  0.07475922\n",
      "Test loss at epoch (1291):  0.03729154\n",
      "Train loss at epoch (1292):  0.074847825\n",
      "Test loss at epoch (1292):  0.037479326\n",
      "Train loss at epoch (1293):  0.074602015\n",
      "Test loss at epoch (1293):  0.037270527\n",
      "Train loss at epoch (1294):  0.07405092\n",
      "Test loss at epoch (1294):  0.036735356\n",
      "Train loss at epoch (1295):  0.0744983\n",
      "Test loss at epoch (1295):  0.03740148\n",
      "Train loss at epoch (1296):  0.074158534\n",
      "Test loss at epoch (1296):  0.03755615\n",
      "Train loss at epoch (1297):  0.0741813\n",
      "Test loss at epoch (1297):  0.036995936\n",
      "Train loss at epoch (1298):  0.074449316\n",
      "Test loss at epoch (1298):  0.037685223\n",
      "Train loss at epoch (1299):  0.07485011\n",
      "Test loss at epoch (1299):  0.037704263\n",
      "Train loss at epoch (1300):  0.0740263\n",
      "Test loss at epoch (1300):  0.03804046\n",
      "Train loss at epoch (1301):  0.07407041\n",
      "Test loss at epoch (1301):  0.03714337\n",
      "Train loss at epoch (1302):  0.074339986\n",
      "Test loss at epoch (1302):  0.038090967\n",
      "Train loss at epoch (1303):  0.07417411\n",
      "Test loss at epoch (1303):  0.037990008\n",
      "Train loss at epoch (1304):  0.07380469\n",
      "Test loss at epoch (1304):  0.03695602\n",
      "Train loss at epoch (1305):  0.07389318\n",
      "Test loss at epoch (1305):  0.037612062\n",
      "Train loss at epoch (1306):  0.07440322\n",
      "Test loss at epoch (1306):  0.03791417\n",
      "Train loss at epoch (1307):  0.07403754\n",
      "Test loss at epoch (1307):  0.03737937\n",
      "Train loss at epoch (1308):  0.0745174\n",
      "Test loss at epoch (1308):  0.03724371\n",
      "Train loss at epoch (1309):  0.07497581\n",
      "Test loss at epoch (1309):  0.037465066\n",
      "Train loss at epoch (1310):  0.0742385\n",
      "Test loss at epoch (1310):  0.037327807\n",
      "Train loss at epoch (1311):  0.07380347\n",
      "Test loss at epoch (1311):  0.03744199\n",
      "Train loss at epoch (1312):  0.074640095\n",
      "Test loss at epoch (1312):  0.037549205\n",
      "Train loss at epoch (1313):  0.07416061\n",
      "Test loss at epoch (1313):  0.037615284\n",
      "Train loss at epoch (1314):  0.073903605\n",
      "Test loss at epoch (1314):  0.037261188\n",
      "Train loss at epoch (1315):  0.07438481\n",
      "Test loss at epoch (1315):  0.036854785\n",
      "Train loss at epoch (1316):  0.07438521\n",
      "Test loss at epoch (1316):  0.037084606\n",
      "Train loss at epoch (1317):  0.07443009\n",
      "Test loss at epoch (1317):  0.03755396\n",
      "Train loss at epoch (1318):  0.07397529\n",
      "Test loss at epoch (1318):  0.037492964\n",
      "Train loss at epoch (1319):  0.074028894\n",
      "Test loss at epoch (1319):  0.03740618\n",
      "Train loss at epoch (1320):  0.07397248\n",
      "Test loss at epoch (1320):  0.036977224\n",
      "Train loss at epoch (1321):  0.07345002\n",
      "Test loss at epoch (1321):  0.037160765\n",
      "Train loss at epoch (1322):  0.07439312\n",
      "Test loss at epoch (1322):  0.03878889\n",
      "Train loss at epoch (1323):  0.0745769\n",
      "Test loss at epoch (1323):  0.0364607\n",
      "Train loss at epoch (1324):  0.07406531\n",
      "Test loss at epoch (1324):  0.0370779\n",
      "Train loss at epoch (1325):  0.07361338\n",
      "Test loss at epoch (1325):  0.037524417\n",
      "Train loss at epoch (1326):  0.074113294\n",
      "Test loss at epoch (1326):  0.037197147\n",
      "Train loss at epoch (1327):  0.074245\n",
      "Test loss at epoch (1327):  0.036961764\n",
      "Train loss at epoch (1328):  0.07410701\n",
      "Test loss at epoch (1328):  0.03700706\n",
      "Train loss at epoch (1329):  0.074148975\n",
      "Test loss at epoch (1329):  0.03739304\n",
      "Train loss at epoch (1330):  0.07395184\n",
      "Test loss at epoch (1330):  0.03770982\n",
      "Train loss at epoch (1331):  0.07472379\n",
      "Test loss at epoch (1331):  0.03757213\n",
      "Train loss at epoch (1332):  0.07432899\n",
      "Test loss at epoch (1332):  0.03808693\n",
      "Train loss at epoch (1333):  0.07401199\n",
      "Test loss at epoch (1333):  0.037388533\n",
      "Train loss at epoch (1334):  0.07425615\n",
      "Test loss at epoch (1334):  0.037391193\n",
      "Train loss at epoch (1335):  0.074083105\n",
      "Test loss at epoch (1335):  0.037414517\n",
      "Train loss at epoch (1336):  0.074439414\n",
      "Test loss at epoch (1336):  0.037317254\n",
      "Train loss at epoch (1337):  0.07410579\n",
      "Test loss at epoch (1337):  0.038161233\n",
      "Train loss at epoch (1338):  0.074296705\n",
      "Test loss at epoch (1338):  0.037273526\n",
      "Train loss at epoch (1339):  0.07417416\n",
      "Test loss at epoch (1339):  0.036789875\n",
      "Train loss at epoch (1340):  0.07407524\n",
      "Test loss at epoch (1340):  0.03718129\n",
      "Train loss at epoch (1341):  0.074110456\n",
      "Test loss at epoch (1341):  0.03749225\n",
      "Train loss at epoch (1342):  0.07417006\n",
      "Test loss at epoch (1342):  0.036976296\n",
      "Train loss at epoch (1343):  0.07394702\n",
      "Test loss at epoch (1343):  0.037824463\n",
      "Train loss at epoch (1344):  0.0743542\n",
      "Test loss at epoch (1344):  0.03697075\n",
      "Train loss at epoch (1345):  0.074572004\n",
      "Test loss at epoch (1345):  0.037991643\n",
      "Train loss at epoch (1346):  0.07430256\n",
      "Test loss at epoch (1346):  0.03736516\n",
      "Train loss at epoch (1347):  0.07423147\n",
      "Test loss at epoch (1347):  0.03736439\n",
      "Train loss at epoch (1348):  0.07451374\n",
      "Test loss at epoch (1348):  0.03777875\n",
      "Train loss at epoch (1349):  0.07415907\n",
      "Test loss at epoch (1349):  0.037644994\n",
      "Train loss at epoch (1350):  0.07427476\n",
      "Test loss at epoch (1350):  0.03692014\n",
      "Train loss at epoch (1351):  0.0739779\n",
      "Test loss at epoch (1351):  0.037741926\n",
      "Train loss at epoch (1352):  0.074443415\n",
      "Test loss at epoch (1352):  0.03736797\n",
      "Train loss at epoch (1353):  0.07480009\n",
      "Test loss at epoch (1353):  0.037444964\n",
      "Train loss at epoch (1354):  0.07461082\n",
      "Test loss at epoch (1354):  0.038155615\n",
      "Train loss at epoch (1355):  0.07419844\n",
      "Test loss at epoch (1355):  0.037991077\n",
      "Train loss at epoch (1356):  0.07481943\n",
      "Test loss at epoch (1356):  0.037556004\n",
      "Train loss at epoch (1357):  0.07434653\n",
      "Test loss at epoch (1357):  0.037750036\n",
      "Train loss at epoch (1358):  0.07371818\n",
      "Test loss at epoch (1358):  0.03773648\n",
      "Train loss at epoch (1359):  0.07400961\n",
      "Test loss at epoch (1359):  0.037440054\n",
      "Train loss at epoch (1360):  0.07448625\n",
      "Test loss at epoch (1360):  0.037750803\n",
      "Train loss at epoch (1361):  0.07416969\n",
      "Test loss at epoch (1361):  0.038082656\n",
      "Train loss at epoch (1362):  0.0741034\n",
      "Test loss at epoch (1362):  0.037715755\n",
      "Train loss at epoch (1363):  0.074267834\n",
      "Test loss at epoch (1363):  0.037890196\n",
      "Train loss at epoch (1364):  0.07480781\n",
      "Test loss at epoch (1364):  0.0370207\n",
      "Train loss at epoch (1365):  0.07391245\n",
      "Test loss at epoch (1365):  0.036907643\n",
      "Train loss at epoch (1366):  0.074163064\n",
      "Test loss at epoch (1366):  0.038102966\n",
      "Train loss at epoch (1367):  0.07368655\n",
      "Test loss at epoch (1367):  0.03761222\n",
      "Train loss at epoch (1368):  0.074362546\n",
      "Test loss at epoch (1368):  0.037646845\n",
      "Train loss at epoch (1369):  0.074042134\n",
      "Test loss at epoch (1369):  0.037286915\n",
      "Train loss at epoch (1370):  0.07468441\n",
      "Test loss at epoch (1370):  0.037609704\n",
      "Train loss at epoch (1371):  0.07421241\n",
      "Test loss at epoch (1371):  0.0375956\n",
      "Train loss at epoch (1372):  0.07444273\n",
      "Test loss at epoch (1372):  0.037293743\n",
      "Train loss at epoch (1373):  0.073935516\n",
      "Test loss at epoch (1373):  0.03714297\n",
      "Train loss at epoch (1374):  0.07421437\n",
      "Test loss at epoch (1374):  0.03726929\n",
      "Train loss at epoch (1375):  0.074047185\n",
      "Test loss at epoch (1375):  0.03708589\n",
      "Train loss at epoch (1376):  0.07393243\n",
      "Test loss at epoch (1376):  0.038287036\n",
      "Train loss at epoch (1377):  0.073827855\n",
      "Test loss at epoch (1377):  0.036963042\n",
      "Train loss at epoch (1378):  0.0744529\n",
      "Test loss at epoch (1378):  0.03767194\n",
      "Train loss at epoch (1379):  0.07402143\n",
      "Test loss at epoch (1379):  0.03799534\n",
      "Train loss at epoch (1380):  0.073950835\n",
      "Test loss at epoch (1380):  0.037849106\n",
      "Train loss at epoch (1381):  0.07433928\n",
      "Test loss at epoch (1381):  0.037229933\n",
      "Train loss at epoch (1382):  0.07429398\n",
      "Test loss at epoch (1382):  0.037186936\n",
      "Train loss at epoch (1383):  0.07419571\n",
      "Test loss at epoch (1383):  0.037836738\n",
      "Train loss at epoch (1384):  0.07404459\n",
      "Test loss at epoch (1384):  0.037971117\n",
      "Train loss at epoch (1385):  0.07464787\n",
      "Test loss at epoch (1385):  0.03777873\n",
      "Train loss at epoch (1386):  0.07387032\n",
      "Test loss at epoch (1386):  0.03743516\n",
      "Train loss at epoch (1387):  0.07404281\n",
      "Test loss at epoch (1387):  0.03802037\n",
      "Train loss at epoch (1388):  0.07401797\n",
      "Test loss at epoch (1388):  0.037324812\n",
      "Train loss at epoch (1389):  0.07450162\n",
      "Test loss at epoch (1389):  0.037270527\n",
      "Train loss at epoch (1390):  0.074321635\n",
      "Test loss at epoch (1390):  0.037568856\n",
      "Train loss at epoch (1391):  0.07440763\n",
      "Test loss at epoch (1391):  0.037590425\n",
      "Train loss at epoch (1392):  0.07434557\n",
      "Test loss at epoch (1392):  0.037408188\n",
      "Train loss at epoch (1393):  0.07360017\n",
      "Test loss at epoch (1393):  0.037310127\n",
      "Train loss at epoch (1394):  0.07414332\n",
      "Test loss at epoch (1394):  0.037541047\n",
      "Train loss at epoch (1395):  0.07384477\n",
      "Test loss at epoch (1395):  0.037179675\n",
      "Train loss at epoch (1396):  0.07394976\n",
      "Test loss at epoch (1396):  0.037765604\n",
      "Train loss at epoch (1397):  0.07447057\n",
      "Test loss at epoch (1397):  0.037059013\n",
      "Train loss at epoch (1398):  0.07475129\n",
      "Test loss at epoch (1398):  0.0378254\n",
      "Train loss at epoch (1399):  0.073688\n",
      "Test loss at epoch (1399):  0.038153965\n",
      "Train loss at epoch (1400):  0.07429057\n",
      "Test loss at epoch (1400):  0.037811182\n",
      "Train loss at epoch (1401):  0.074194364\n",
      "Test loss at epoch (1401):  0.03718535\n",
      "Train loss at epoch (1402):  0.07458708\n",
      "Test loss at epoch (1402):  0.03715028\n",
      "Train loss at epoch (1403):  0.07396508\n",
      "Test loss at epoch (1403):  0.037594207\n",
      "Train loss at epoch (1404):  0.074120626\n",
      "Test loss at epoch (1404):  0.0376078\n",
      "Train loss at epoch (1405):  0.07396085\n",
      "Test loss at epoch (1405):  0.037817586\n",
      "Train loss at epoch (1406):  0.07443173\n",
      "Test loss at epoch (1406):  0.037424073\n",
      "Train loss at epoch (1407):  0.07380433\n",
      "Test loss at epoch (1407):  0.037367932\n",
      "Train loss at epoch (1408):  0.074200965\n",
      "Test loss at epoch (1408):  0.037049998\n",
      "Train loss at epoch (1409):  0.07454139\n",
      "Test loss at epoch (1409):  0.037467256\n",
      "Train loss at epoch (1410):  0.074497\n",
      "Test loss at epoch (1410):  0.037738666\n",
      "Train loss at epoch (1411):  0.07406748\n",
      "Test loss at epoch (1411):  0.038070347\n",
      "Train loss at epoch (1412):  0.073627286\n",
      "Test loss at epoch (1412):  0.03704777\n",
      "Train loss at epoch (1413):  0.07399754\n",
      "Test loss at epoch (1413):  0.037811086\n",
      "Train loss at epoch (1414):  0.07468044\n",
      "Test loss at epoch (1414):  0.038081605\n",
      "Train loss at epoch (1415):  0.07416432\n",
      "Test loss at epoch (1415):  0.03717197\n",
      "Train loss at epoch (1416):  0.074075036\n",
      "Test loss at epoch (1416):  0.03708179\n",
      "Train loss at epoch (1417):  0.073889\n",
      "Test loss at epoch (1417):  0.03732323\n",
      "Train loss at epoch (1418):  0.07331299\n",
      "Test loss at epoch (1418):  0.0371949\n",
      "Train loss at epoch (1419):  0.07392897\n",
      "Test loss at epoch (1419):  0.037985153\n",
      "Train loss at epoch (1420):  0.07413172\n",
      "Test loss at epoch (1420):  0.037616424\n",
      "Train loss at epoch (1421):  0.074148186\n",
      "Test loss at epoch (1421):  0.0370964\n",
      "Train loss at epoch (1422):  0.07398272\n",
      "Test loss at epoch (1422):  0.037702248\n",
      "Train loss at epoch (1423):  0.07423823\n",
      "Test loss at epoch (1423):  0.037080064\n",
      "Train loss at epoch (1424):  0.07433624\n",
      "Test loss at epoch (1424):  0.0380659\n",
      "Train loss at epoch (1425):  0.07392143\n",
      "Test loss at epoch (1425):  0.037693523\n",
      "Train loss at epoch (1426):  0.0740611\n",
      "Test loss at epoch (1426):  0.03693842\n",
      "Train loss at epoch (1427):  0.07405768\n",
      "Test loss at epoch (1427):  0.037624527\n",
      "Train loss at epoch (1428):  0.0740614\n",
      "Test loss at epoch (1428):  0.037180107\n",
      "Train loss at epoch (1429):  0.074218534\n",
      "Test loss at epoch (1429):  0.037495784\n",
      "Train loss at epoch (1430):  0.074024856\n",
      "Test loss at epoch (1430):  0.03682099\n",
      "Train loss at epoch (1431):  0.07401942\n",
      "Test loss at epoch (1431):  0.03710206\n",
      "Train loss at epoch (1432):  0.07358959\n",
      "Test loss at epoch (1432):  0.03727489\n",
      "Train loss at epoch (1433):  0.07411225\n",
      "Test loss at epoch (1433):  0.03727249\n",
      "Train loss at epoch (1434):  0.07409399\n",
      "Test loss at epoch (1434):  0.036787122\n",
      "Train loss at epoch (1435):  0.0741658\n",
      "Test loss at epoch (1435):  0.03802287\n",
      "Train loss at epoch (1436):  0.07407331\n",
      "Test loss at epoch (1436):  0.037573803\n",
      "Train loss at epoch (1437):  0.07410596\n",
      "Test loss at epoch (1437):  0.037472706\n",
      "Train loss at epoch (1438):  0.07430565\n",
      "Test loss at epoch (1438):  0.036693458\n",
      "Train loss at epoch (1439):  0.07425795\n",
      "Test loss at epoch (1439):  0.03687802\n",
      "Train loss at epoch (1440):  0.074294224\n",
      "Test loss at epoch (1440):  0.038191486\n",
      "Train loss at epoch (1441):  0.074376956\n",
      "Test loss at epoch (1441):  0.0372734\n",
      "Train loss at epoch (1442):  0.07477326\n",
      "Test loss at epoch (1442):  0.03767124\n",
      "Train loss at epoch (1443):  0.07406599\n",
      "Test loss at epoch (1443):  0.038335953\n",
      "Train loss at epoch (1444):  0.07439269\n",
      "Test loss at epoch (1444):  0.037030384\n",
      "Train loss at epoch (1445):  0.073768094\n",
      "Test loss at epoch (1445):  0.037504006\n",
      "Train loss at epoch (1446):  0.07404795\n",
      "Test loss at epoch (1446):  0.037958976\n",
      "Train loss at epoch (1447):  0.07449971\n",
      "Test loss at epoch (1447):  0.037186284\n",
      "Train loss at epoch (1448):  0.073792264\n",
      "Test loss at epoch (1448):  0.037331913\n",
      "Train loss at epoch (1449):  0.07452417\n",
      "Test loss at epoch (1449):  0.038247835\n",
      "Train loss at epoch (1450):  0.07399454\n",
      "Test loss at epoch (1450):  0.03680417\n",
      "Train loss at epoch (1451):  0.07384673\n",
      "Test loss at epoch (1451):  0.038008325\n",
      "Train loss at epoch (1452):  0.07434219\n",
      "Test loss at epoch (1452):  0.037327297\n",
      "Train loss at epoch (1453):  0.0737883\n",
      "Test loss at epoch (1453):  0.038010955\n",
      "Train loss at epoch (1454):  0.07443009\n",
      "Test loss at epoch (1454):  0.036603633\n",
      "Train loss at epoch (1455):  0.07485561\n",
      "Test loss at epoch (1455):  0.0374792\n",
      "Train loss at epoch (1456):  0.07398318\n",
      "Test loss at epoch (1456):  0.037651468\n",
      "Train loss at epoch (1457):  0.07437536\n",
      "Test loss at epoch (1457):  0.037983093\n",
      "Train loss at epoch (1458):  0.074450985\n",
      "Test loss at epoch (1458):  0.037277162\n",
      "Train loss at epoch (1459):  0.07390731\n",
      "Test loss at epoch (1459):  0.03727035\n",
      "Train loss at epoch (1460):  0.07400534\n",
      "Test loss at epoch (1460):  0.03687657\n",
      "Train loss at epoch (1461):  0.074256405\n",
      "Test loss at epoch (1461):  0.037295762\n",
      "Train loss at epoch (1462):  0.074157506\n",
      "Test loss at epoch (1462):  0.037432976\n",
      "Train loss at epoch (1463):  0.0742566\n",
      "Test loss at epoch (1463):  0.03801018\n",
      "Train loss at epoch (1464):  0.074264295\n",
      "Test loss at epoch (1464):  0.037481904\n",
      "Train loss at epoch (1465):  0.07432163\n",
      "Test loss at epoch (1465):  0.037664257\n",
      "Train loss at epoch (1466):  0.07469824\n",
      "Test loss at epoch (1466):  0.03750012\n",
      "Train loss at epoch (1467):  0.07426602\n",
      "Test loss at epoch (1467):  0.037636463\n",
      "Train loss at epoch (1468):  0.074006096\n",
      "Test loss at epoch (1468):  0.037411176\n",
      "Train loss at epoch (1469):  0.07439715\n",
      "Test loss at epoch (1469):  0.03687417\n",
      "Train loss at epoch (1470):  0.07499172\n",
      "Test loss at epoch (1470):  0.037638705\n",
      "Train loss at epoch (1471):  0.0742392\n",
      "Test loss at epoch (1471):  0.037385643\n",
      "Train loss at epoch (1472):  0.07478027\n",
      "Test loss at epoch (1472):  0.03766813\n",
      "Train loss at epoch (1473):  0.07423699\n",
      "Test loss at epoch (1473):  0.03737341\n",
      "Train loss at epoch (1474):  0.07428599\n",
      "Test loss at epoch (1474):  0.037400357\n",
      "Train loss at epoch (1475):  0.07453349\n",
      "Test loss at epoch (1475):  0.038222685\n",
      "Train loss at epoch (1476):  0.07422661\n",
      "Test loss at epoch (1476):  0.037027277\n",
      "Train loss at epoch (1477):  0.0744783\n",
      "Test loss at epoch (1477):  0.036825296\n",
      "Train loss at epoch (1478):  0.0736393\n",
      "Test loss at epoch (1478):  0.03715366\n",
      "Train loss at epoch (1479):  0.07418442\n",
      "Test loss at epoch (1479):  0.037849285\n",
      "Train loss at epoch (1480):  0.074439906\n",
      "Test loss at epoch (1480):  0.036704864\n",
      "Train loss at epoch (1481):  0.074058495\n",
      "Test loss at epoch (1481):  0.03773694\n",
      "Train loss at epoch (1482):  0.07411489\n",
      "Test loss at epoch (1482):  0.03759457\n",
      "Train loss at epoch (1483):  0.07419235\n",
      "Test loss at epoch (1483):  0.03770895\n",
      "Train loss at epoch (1484):  0.07419957\n",
      "Test loss at epoch (1484):  0.03773187\n",
      "Train loss at epoch (1485):  0.07419581\n",
      "Test loss at epoch (1485):  0.03774375\n",
      "Train loss at epoch (1486):  0.07411127\n",
      "Test loss at epoch (1486):  0.037511766\n",
      "Train loss at epoch (1487):  0.074501745\n",
      "Test loss at epoch (1487):  0.03666982\n",
      "Train loss at epoch (1488):  0.07444937\n",
      "Test loss at epoch (1488):  0.036880586\n",
      "Train loss at epoch (1489):  0.07326029\n",
      "Test loss at epoch (1489):  0.037506882\n",
      "Train loss at epoch (1490):  0.07415399\n",
      "Test loss at epoch (1490):  0.036900647\n",
      "Train loss at epoch (1491):  0.07412873\n",
      "Test loss at epoch (1491):  0.03760505\n",
      "Train loss at epoch (1492):  0.074452415\n",
      "Test loss at epoch (1492):  0.038010996\n",
      "Train loss at epoch (1493):  0.07391617\n",
      "Test loss at epoch (1493):  0.037479527\n",
      "Train loss at epoch (1494):  0.07413291\n",
      "Test loss at epoch (1494):  0.03706993\n",
      "Train loss at epoch (1495):  0.07391231\n",
      "Test loss at epoch (1495):  0.03792052\n",
      "Train loss at epoch (1496):  0.07432412\n",
      "Test loss at epoch (1496):  0.038525295\n",
      "Train loss at epoch (1497):  0.0743781\n",
      "Test loss at epoch (1497):  0.037219036\n",
      "Train loss at epoch (1498):  0.07399769\n",
      "Test loss at epoch (1498):  0.03708052\n",
      "Train loss at epoch (1499):  0.073916554\n",
      "Test loss at epoch (1499):  0.037364356\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "train_dataset = TensorDataset(X_train.to(device), y_train.to(device))\n",
    "test_dataset = TensorDataset(X_test.to(device), y_test.to(device))\n",
    "\n",
    "EPOCHS = 1500\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=128, shuffle=True)\n",
    "transformer = Transformer(cfg)\n",
    "optimizer = SGD(transformer.parameters(), lr=1e-4, momentum=0.9)\n",
    "lr_scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "trainer = Trainer(\n",
    "    train_data_loader=train_data_loader,\n",
    "    test_data_loader=test_data_loader,\n",
    "    # optimizer=AdamW(decoder.parameters(), lr=1e-5),\n",
    "    optimizer=optimizer,\n",
    "    model=transformer,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    epochs=EPOCHS\n",
    ")\n",
    "\n",
    "trainer.train(loss_fn=nn.L1Loss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_transformer_enc_dec(x, model, output_len):\n",
    "  tgt_seq = x[:, -1:, :]\n",
    "  for i in range(output_len):\n",
    "    out = model(x, tgt_seq)\n",
    "    tgt_seq = torch.cat((tgt_seq, out[:, -1:, :]), dim=-2)\n",
    "\n",
    "  return tgt_seq[:, 1:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_sample, y_sample = test_dataset[np.random.randint(0, TEST_SIZE)]\n",
    "predictions = infer_transformer_enc_dec(x_sample.unsqueeze(0), transformer, output_len=TARGET_SEQ_LEN)\n",
    "labels = torch.cat((x_sample, y_sample[-1:]))\n",
    "\n",
    "plot_circle(\n",
    "    input=x_sample.detach().cpu().numpy(),\n",
    "    prediction=predictions[0].detach().cpu().numpy(),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
